{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib  # Better alternative for large objects\n",
    "# import numpy as np\n",
    "\n",
    "# granger_results_lstm = {}\n",
    "\n",
    "# for company, results in stationary_columns.items():\n",
    "#     print(f\"Processing company: {company}\")\n",
    "#     df_company = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company].copy()\n",
    "    \n",
    "#     # Ensure chronological ordering\n",
    "#     df_company = df_company.sort_index()  # Critical for time series\n",
    "    \n",
    "#     granger_results_lstm[company] = {}\n",
    "    \n",
    "#     for stock_var in results['STOCK']:\n",
    "#         granger_results_lstm[company][stock_var] = {}\n",
    "#         print(f\"  Stock variable: {stock_var}\")\n",
    "        \n",
    "#         for twitter_var in results['TWITTER']:\n",
    "#             print(f\"Twitter variable: {twitter_var}\")\n",
    "#             max_lags = granger_results.get(company, {}).get(stock_var, {}).get(twitter_var, {}).get('optimal_lags', 15)\n",
    "#             max_lags = int(max_lags)\n",
    "#             print(f\"Max lags: {max_lags}\")\n",
    "#             # Create dataset with correct column order [STOCK, TWEET]\n",
    "#             data = df_company[[stock_var, twitter_var]].copy()\n",
    "            \n",
    "            \n",
    "#             # Temporal split (preserve order)\n",
    "#             n = len(data)\n",
    "#             train_size = int(0.6 * n)\n",
    "#             valid_size = int(0.2 * n)\n",
    "            \n",
    "          \n",
    "            \n",
    "#             # Standardize using TRAIN stats only\n",
    "#             scaler = StandardScaler()\n",
    "#             data_train_scaled = scaler.fit_transform(data_train)\n",
    "#             data_valid_scaled = scaler.transform(data_valid)\n",
    "#             data_test_scaled = scaler.transform(data_test)\n",
    "            \n",
    "#             # Run causality test\n",
    "#             try:\n",
    "#                 results_cas = nonlincausality.nonlincausalityNN(\n",
    "#                 x=data_train_scaled,\n",
    "#                 maxlag=max_lags,  # Reduced maxlag for small datasets\n",
    "#                 NN_config=['d', 'dr', 'd'],  # Added extra Dense layer for better feature extraction\n",
    "#                 NN_neurons=[8, 0.3, 4],  # Reduced neurons to prevent overfitting\n",
    "#                 x_test=data_test_scaled,\n",
    "#                 run=3,  # Reduced runs for faster computation\n",
    "#                 epochs_num=[30, 20],  # Reduced epochs\n",
    "#                 learning_rate=[0.005, 0.0005],  # Adjusted learning rates\n",
    "#                 batch_size_num=16,  # Increased batch size for stability\n",
    "#                 x_val=data_valid_scaled,\n",
    "#                 regularization='l1_l2',  # Combined regularization\n",
    "#                 reg_alpha=[0.005, 0.005],  # Smaller regularization\n",
    "#                 callbacks=None,\n",
    "#                 verbose=False,\n",
    "#                 plot=False\n",
    "#                 )\n",
    "#                 # Store lightweight results summary\n",
    "#                 granger_results_lstm[company][stock_var][twitter_var] = {\n",
    "#                     'lags': {},\n",
    "#                     'metadata': {\n",
    "#                         'train_start': data_train.index[0],\n",
    "#                         'train_end': data_train.index[-1],\n",
    "#                         'test_start': data_test.index[0],\n",
    "#                         'test_end': data_test.index[-1],\n",
    "#                         'n_obs': {\n",
    "#                             'train': len(data_train),\n",
    "#                             'valid': len(data_valid),\n",
    "#                             'test': len(data_test)\n",
    "#                         }\n",
    "#                     }\n",
    "#                 }\n",
    "                \n",
    "#                 for lag, res_obj in results_cas.items():\n",
    "#                     granger_results_lstm[company][stock_var][twitter_var]['lags'][lag] = {\n",
    "#                         'p_value': res_obj.p_value,\n",
    "#                         'test_statistic': res_obj.test_statistic,\n",
    "#                         'RSS_X': res_obj.RSS_X_all,\n",
    "#                         'RSS_XY': res_obj.RSS_XY_all,\n",
    "#                         'best_error_X': res_obj.best_errors_X.tolist(),  # Convert arrays\n",
    "#                         'best_error_XY': res_obj.best_errors_XY.tolist()\n",
    "#                     }\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed for {company}-{stock_var}-{twitter_var}: {str(e)}\")\n",
    "#                 granger_results_lstm[company][stock_var][twitter_var] = {'error': str(e)}\n",
    "\n",
    "# # Save results with metadata preservation\n",
    "# joblib.dump(granger_results_lstm, 'granger_results_lstm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db29580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm_hyperparameters(X_train, y_train, input_shape):\n",
    "    \"\"\"Find optimal units and epochs using time-series cross-validation\"\"\"\n",
    "    best_units = 32  # Default\n",
    "    best_epochs = 100  # Default\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Hyperparameter search space\n",
    "    units_options = [16, 32, 48]\n",
    "    epochs_options = [10, 30, 60]\n",
    "    \n",
    "    # Reduced CV for efficiency\n",
    "    tscv = TimeSeriesSplit(n_splits=2)\n",
    "    \n",
    "    for units in units_options:\n",
    "        for max_epochs in epochs_options:\n",
    "            val_losses = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                # Split training/validation\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                # Build model\n",
    "                model = Sequential([\n",
    "                    LSTM(units, input_shape=input_shape),\n",
    "                    Dense(1)\n",
    "                ])\n",
    "                model.compile(optimizer='adam', loss='mse')\n",
    "                \n",
    "                # Train with early stopping\n",
    "                early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                          patience=5, \n",
    "                                          restore_best_weights=True,\n",
    "                                          verbose=0)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    X_fold_train, y_fold_train,\n",
    "                    validation_data=(X_fold_val, y_fold_val),\n",
    "                    epochs=max_epochs,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Track best validation loss\n",
    "                val_losses.append(min(history.history['val_loss']))\n",
    "            \n",
    "            # Average validation loss across folds\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_units = units\n",
    "                best_epochs = max_epochs\n",
    "    \n",
    "    return best_units, best_epochs\n",
    "\n",
    "def train_final_model(X_train, y_train, input_shape, units, epochs):\n",
    "    \"\"\"Train final model with optimal hyperparameters\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Early stopping on validation split\n",
    "    early_stop = EarlyStopping(monitor='val_loss', \n",
    "                              patience=5, \n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def test_granger_causality(Y, X_base, df, n_lags=5, test_size=0.2, \n",
    "                          n_bootstrap=200, block_size=5):\n",
    "    \"\"\"\n",
    "    Main Granger causality test function with:\n",
    "    - Hyperparameter tuning\n",
    "    - Block bootstrapping\n",
    "    \"\"\"\n",
    "    # 1. Prepare data\n",
    "    temp_df = pd.DataFrame(index=df.index)\n",
    "    temp_df[Y] = df[Y]\n",
    "    # Create Y lags\n",
    "    for i in range(1, n_lags+1):\n",
    "        temp_df[f'{Y}_lag_{i}'] = temp_df[Y].shift(i)\n",
    "    \n",
    "    # Identify X lags\n",
    "    X_lags = [f'{X_base}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    missing = [col for col in X_lags if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Missing columns for {X_base}: {missing}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Combine and clean\n",
    "    data = pd.concat([temp_df, df[X_lags]], axis=1).dropna()\n",
    "    y_data = data[Y].values\n",
    "    Y_lag_cols = [f'{Y}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    full_index = data.index\n",
    "    \n",
    "    # 2. Train-test split (time-based)\n",
    "    n_test = int(len(data) * test_size)\n",
    "    n_train = len(data) - n_test\n",
    "    splits = {\n",
    "        'train': slice(None, n_train),\n",
    "        'test': slice(n_train, None)\n",
    "    }\n",
    "    \n",
    "    # 3. Prepare feature sets\n",
    "    # Restricted model (only Y lags)\n",
    "    X_restricted = data[Y_lag_cols].values\n",
    "    \n",
    "    # Unrestricted model (Y and X lags)\n",
    "    X_unrestricted = np.zeros((len(data), n_lags, 2))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(n_lags):\n",
    "            X_unrestricted[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "            X_unrestricted[i, j, 1] = data[X_lags[j]].iloc[i]\n",
    "    \n",
    "    # Split data\n",
    "    X_res_train, X_res_test = X_restricted[splits['train']], X_restricted[splits['test']]\n",
    "    X_unres_train, X_unres_test = X_unrestricted[splits['train']], X_unrestricted[splits['test']]\n",
    "    y_train, y_test = y_data[splits['train']], y_data[splits['test']]\n",
    "    \n",
    "    # 4. Scaling\n",
    "    scaler_res = StandardScaler()\n",
    "    X_res_train_scaled = scaler_res.fit_transform(X_res_train)\n",
    "    X_res_test_scaled = scaler_res.transform(X_res_test)\n",
    "    X_res_train_3d = X_res_train_scaled.reshape((-1, n_lags, 1))\n",
    "    X_res_test_3d = X_res_test_scaled.reshape((-1, n_lags, 1))\n",
    "    \n",
    "    scaler_unres = StandardScaler()\n",
    "    X_unres_train_2d = X_unres_train.reshape((-1, n_lags*2))\n",
    "    X_unres_test_2d = X_unres_test.reshape((-1, n_lags*2))\n",
    "    X_unres_train_scaled = scaler_unres.fit_transform(X_unres_train_2d)\n",
    "    X_unres_test_scaled = scaler_unres.transform(X_unres_test_2d)\n",
    "    X_unres_train_3d = X_unres_train_scaled.reshape((-1, n_lags, 2))\n",
    "    X_unres_test_3d = X_unres_test_scaled.reshape((-1, n_lags, 2))\n",
    "    \n",
    "    # 5. Hyperparameter tuning\n",
    "    print(f\"Tuning hyperparameters for {Y} <- {X_base}...\")\n",
    "    units_res, epochs_res = tune_lstm_hyperparameters(\n",
    "        X_res_train_3d, y_train, (n_lags, 1))\n",
    "    units_unres, epochs_unres = tune_lstm_hyperparameters(\n",
    "        X_unres_train_3d, y_train, (n_lags, 2))\n",
    "    \n",
    "    print(f\"Best params - Restricted: units={units_res}, epochs={epochs_res}\")\n",
    "    print(f\"Best params - Unrestricted: units={units_unres}, epochs={epochs_unres}\")\n",
    "    \n",
    "    # 6. Train final models\n",
    "    model_res = train_final_model(\n",
    "        X_res_train_3d, y_train, (n_lags, 1), units_res, epochs_res)\n",
    "    model_unres = train_final_model(\n",
    "        X_unres_train_3d, y_train, (n_lags, 2), units_unres, epochs_unres)\n",
    "    \n",
    "    # 7. Original predictions\n",
    "    pred_res = model_res.predict(X_res_test_3d).flatten()\n",
    "    pred_unres = model_unres.predict(X_unres_test_3d).flatten()\n",
    "    e_res = y_test - pred_res\n",
    "    e_unres = y_test - pred_unres\n",
    "    d_original = e_res**2 - e_unres**2\n",
    "    d_bar_original = np.mean(d_original)\n",
    "    \n",
    "    # 8. Bootstrapping\n",
    "    base_series = df.loc[full_index, X_base].copy()\n",
    "    bootstrap_stats = []\n",
    "    \n",
    "    print(f\"Running {n_bootstrap} bootstraps...\")\n",
    "    for b in range(n_bootstrap):\n",
    "        if (b+1) % 50 == 0:\n",
    "            print(f\"  Bootstrap {b+1}/{n_bootstrap}\")\n",
    "        \n",
    "        # Create shuffled X series\n",
    "        shuffled_base = block_shuffle(base_series, block_size)\n",
    "        \n",
    "        # Create new lag features from shuffled series\n",
    "        temp = pd.DataFrame(index=full_index)\n",
    "        temp[X_base] = shuffled_base\n",
    "        for i in range(1, n_lags+1):\n",
    "            temp[f'{X_base}_lag_{i}'] = temp[X_base].shift(i)\n",
    "        \n",
    "        # Build new unrestricted dataset\n",
    "        X_unrestricted_shuffled = np.zeros((len(data), n_lags, 2))\n",
    "        for i in range(len(data)):\n",
    "            for j in range(n_lags):\n",
    "                X_unrestricted_shuffled[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "                X_unrestricted_shuffled[i, j, 1] = temp[X_lags[j]].iloc[i]\n",
    "        \n",
    "        # Extract test portion\n",
    "        X_unres_test_shuffled = X_unrestricted_shuffled[splits['test']]\n",
    "        X_unres_test_shuffled_2d = X_unres_test_shuffled.reshape((-1, n_lags*2))\n",
    "        X_unres_test_shuffled_scaled = scaler_unres.transform(X_unres_test_shuffled_2d)\n",
    "        X_unres_test_shuffled_3d = X_unres_test_shuffled_scaled.reshape((-1, n_lags, 2))\n",
    "        \n",
    "        # Predict with shuffled X\n",
    "        pred_unres_shuffled = model_unres.predict(X_unres_test_shuffled_3d).flatten()\n",
    "        e_unres_shuffled = y_test - pred_unres_shuffled\n",
    "        \n",
    "        # Compute loss difference\n",
    "        d_shuffled = e_res**2 - e_unres_shuffled**2\n",
    "        d_bar_shuffled = np.mean(d_shuffled)\n",
    "        bootstrap_stats.append(d_bar_shuffled)\n",
    "    \n",
    "    # 9. Calculate p-value\n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    p_value = (np.sum(bootstrap_stats >= d_bar_original) + 1) / (n_bootstrap + 1)\n",
    "    causal = (d_bar_original > 0) and (p_value < 0.05)\n",
    "    \n",
    "    return {\n",
    "        'Y': Y,\n",
    "        'X': X_base,\n",
    "        'd_bar': d_bar_original,\n",
    "        'p_value': p_value,\n",
    "        'causal': causal,\n",
    "        'units_res': units_res,\n",
    "        'epochs_res': epochs_res,\n",
    "        'units_unres': units_unres,\n",
    "        'epochs_unres': epochs_unres,\n",
    "        'bootstrap_mean': np.mean(bootstrap_stats),\n",
    "        'boostrap_stats':bootstrap_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb63274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_predictions(df, tweet_cols, stock_cols, test_days=100, lookback=7):\n",
    "   # Split into train and test\n",
    "   train_df = df[:-test_days]\n",
    "   test_df = df[-test_days:]\n",
    "   \n",
    "   predictions = []\n",
    "   \n",
    "   for i in range(len(test_df)):\n",
    "       # Get updated training data including latest actual values\n",
    "       current_train = pd.concat([train_df, test_df[:i]])\n",
    "       \n",
    "       # Prepare data\n",
    "       X, y, scaler_y = prepare_data(current_train, tweet_cols, stock_cols, lookback)\n",
    "       \n",
    "       # Train model\n",
    "       model = create_model((lookback, len(tweet_cols)), len(stock_cols))\n",
    "       model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
    "       \n",
    "       # Predict next day\n",
    "       last_sequence = scaler_X.transform(current_train[tweet_cols].tail(lookback))\n",
    "       pred = model.predict(last_sequence.reshape(1, lookback, -1))\n",
    "       pred = scaler_y.inverse_transform(pred)[0]\n",
    "       predictions.append(pred)\n",
    "   \n",
    "   return np.array(predictions)\n",
    "\n",
    "predictions = create_rolling_predictions(df, tweet_columns, stock_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lookbacks(df, tweet_cols, stock_cols, lookbacks=[1]):\n",
    "    results = {}\n",
    "    \n",
    "    for lookback in lookbacks:\n",
    "        X, y, scaler_y = prepare_data(df, tweet_cols, stock_cols, lookback)\n",
    "        \n",
    "        # Train/test split\n",
    "        split = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model((lookback, len(tweet_cols)), len(stock_cols))\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                          validation_split=0.1, verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions = scaler_y.inverse_transform(predictions)\n",
    "        true_values = scaler_y.inverse_transform(y_test)\n",
    "        \n",
    "        mse = np.mean((predictions - true_values) ** 2)\n",
    "        results[lookback] = {\n",
    "            'mse': mse,\n",
    "            'val_loss': min(history.history['val_loss'])\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44911710",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_lookbacks(merged_daily, tweet_columns, stock_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def block_shuffle(series, block_size):\n",
    "    \"\"\"Shuffle blocks of a time series while preserving internal structure.\"\"\"\n",
    "    n = len(series)\n",
    "    n_blocks = n // block_size\n",
    "    blocks = [series[i*block_size:(i+1)*block_size].values for i in range(n_blocks)]\n",
    "    \n",
    "    # Handle remainder if exists\n",
    "    remainder = n % block_size\n",
    "    if remainder > 0:\n",
    "        blocks.append(series[n_blocks*block_size:].values)\n",
    "    \n",
    "    np.random.shuffle(blocks)\n",
    "    # Maintain original index structure\n",
    "    shuffled_values = np.concatenate(blocks)\n",
    "    return pd.Series(shuffled_values, index=series.index[:len(shuffled_values)])\n",
    "\n",
    "def test_granger_causality(Y, X_base, df, n_lags=5, test_size=0.2, \n",
    "                           epochs=30, units=30, n_bootstrap=200, block_size=30):\n",
    "    \"\"\"\n",
    "    Test Granger causality using LSTM neural networks with block bootstrapping.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: Target variable (string)\n",
    "    - X_base: Base predictor variable (string) \n",
    "    - df: DataFrame containing all variables and their lags\n",
    "    - n_lags: Number of lags to use\n",
    "    - test_size: Proportion of data for testing\n",
    "    - epochs: Number of training epochs for LSTM\n",
    "    - units: Number of LSTM units\n",
    "    - n_bootstrap: Number of bootstrap iterations\n",
    "    - block_size: Size of blocks for block bootstrapping\n",
    "    \n",
    "    Returns:\n",
    "    - d_bar_original: Original test statistic\n",
    "    - p_value: Bootstrap p-value\n",
    "    - causal: Boolean indicating significant causality\n",
    "    - bootstrap_stats: Array of bootstrap statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if Y not in df.columns:\n",
    "        raise ValueError(f\"Target variable '{Y}' not found in DataFrame\")\n",
    "    if X_base not in df.columns:\n",
    "        raise ValueError(f\"Base predictor '{X_base}' not found in DataFrame\")\n",
    "    \n",
    "    # Create lags for Y if they don't exist\n",
    "    temp_df = pd.DataFrame(index=df.index)\n",
    "    temp_df[Y] = df[Y]\n",
    "    for i in range(1, n_lags+1):\n",
    "        temp_df[f'{Y}_lag_{i}'] = temp_df[Y].shift(i)\n",
    "    \n",
    "    # Identify lag columns for X_base\n",
    "    X_lags = [f'{X_base}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    missing = [col for col in X_lags if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Missing lag columns for {X_base}: {missing}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Combine data and drop NA\n",
    "    data = pd.concat([temp_df, df[X_lags]], axis=1).dropna()\n",
    "    \n",
    "    if len(data) < 50:  # Minimum data requirement\n",
    "        print(f\"Insufficient data after removing NAs: {len(data)} rows\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    y_data = data[Y].values\n",
    "    Y_lag_cols = [f'{Y}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    full_index = data.index\n",
    "    \n",
    "    # Time-based train-test split\n",
    "    n_test = max(1, int(len(data) * test_size))  # Ensure at least 1 test sample\n",
    "    n_train = len(data) - n_test\n",
    "    \n",
    "    if n_train < 10:  # Minimum training requirement\n",
    "        print(f\"Insufficient training data: {n_train} samples\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Build datasets\n",
    "    X_restricted = data[Y_lag_cols].values\n",
    "    X_unrestricted = np.zeros((len(data), n_lags, 2))\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(n_lags):\n",
    "            X_unrestricted[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "            X_unrestricted[i, j, 1] = data[X_lags[j]].iloc[i]\n",
    "    \n",
    "    # Split data\n",
    "    X_res_train, X_res_test = X_restricted[:n_train], X_restricted[n_train:]\n",
    "    X_unres_train, X_unres_test = X_unrestricted[:n_train], X_unrestricted[n_train:]\n",
    "    y_train, y_test = y_data[:n_train], y_data[n_train:]\n",
    "    \n",
    "    # Scaling\n",
    "    scaler_res = StandardScaler()\n",
    "    X_res_train_scaled = scaler_res.fit_transform(X_res_train)\n",
    "    X_res_test_scaled = scaler_res.transform(X_res_test)\n",
    "    X_res_train_3d = X_res_train_scaled.reshape((-1, n_lags, 1))\n",
    "    X_res_test_3d = X_res_test_scaled.reshape((-1, n_lags, 1))\n",
    "    \n",
    "    scaler_unres = StandardScaler()\n",
    "    X_unres_train_2d = X_unres_train.reshape((-1, n_lags*2))\n",
    "    X_unres_test_2d = X_unres_test.reshape((-1, n_lags*2))\n",
    "    X_unres_train_scaled = scaler_unres.fit_transform(X_unres_train_2d)\n",
    "    X_unres_test_scaled = scaler_unres.transform(X_unres_test_2d)\n",
    "    X_unres_train_3d = X_unres_train_scaled.reshape((-1, n_lags, 2))\n",
    "    X_unres_test_3d = X_unres_test_scaled.reshape((-1, n_lags, 2))\n",
    "    \n",
    "    # Model building function\n",
    "    def build_model(input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(units, input_shape=input_shape, return_sequences=False),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    # Train original models with error handling\n",
    "    try:\n",
    "        model_res = build_model((n_lags, 1))\n",
    "        model_unres = build_model((n_lags, 2))\n",
    "        \n",
    "        # Suppress verbose output\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model_res.fit(X_res_train_3d, y_train, epochs=epochs, batch_size=min(32, len(y_train)//2), verbose=0)\n",
    "            model_unres.fit(X_unres_train_3d, y_train, epochs=epochs, batch_size=min(32, len(y_train)//2), verbose=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training models: {str(e)}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Original predictions and errors\n",
    "    try:\n",
    "        pred_res = model_res.predict(X_res_test_3d, verbose=0).flatten()\n",
    "        pred_unres = model_unres.predict(X_unres_test_3d, verbose=0).flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {str(e)}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    e_res = y_test - pred_res\n",
    "    e_unres = y_test - pred_unres\n",
    "    d_original = e_res**2 - e_unres**2\n",
    "    d_bar_original = np.mean(d_original)\n",
    "    \n",
    "    # Prepare for bootstrapping\n",
    "    base_series = df.loc[full_index, X_base].copy()\n",
    "    bootstrap_stats = []\n",
    "    \n",
    "    print(f\"Running {n_bootstrap} bootstrap iterations...\")\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Bootstrap iteration {i + 1}/{n_bootstrap}\")\n",
    "        \n",
    "        try:\n",
    "            # Create shuffled X series with block bootstrapping\n",
    "            shuffled_base = block_shuffle(base_series, block_size)\n",
    "            \n",
    "            # Create new lag features from shuffled series\n",
    "            temp = pd.DataFrame(index=full_index)\n",
    "            temp[X_base] = shuffled_base\n",
    "            for j in range(1, n_lags+1):\n",
    "                temp[f'{X_base}_lag_{j}'] = temp[X_base].shift(j)\n",
    "            \n",
    "            # Build new unrestricted dataset\n",
    "            X_unrestricted_shuffled = np.zeros((len(data), n_lags, 2))\n",
    "            for k in range(len(data)):\n",
    "                for j in range(n_lags):\n",
    "                    X_unrestricted_shuffled[k, j, 0] = data[Y_lag_cols[j]].iloc[k]\n",
    "                    # Handle potential NaN values from shifted data\n",
    "                    lag_val = temp[X_lags[j]].iloc[k] if k < len(temp) else 0\n",
    "                    X_unrestricted_shuffled[k, j, 1] = lag_val if not pd.isna(lag_val) else 0\n",
    "            \n",
    "            # Extract test portion and scale\n",
    "            X_unres_test_shuffled = X_unrestricted_shuffled[n_train:]\n",
    "            X_unres_test_shuffled_2d = X_unres_test_shuffled.reshape((-1, n_lags*2))\n",
    "            X_unres_test_shuffled_scaled = scaler_unres.transform(X_unres_test_shuffled_2d)\n",
    "            X_unres_test_shuffled_3d = X_unres_test_shuffled_scaled.reshape((-1, n_lags, 2))\n",
    "            \n",
    "            # Predict with shuffled X\n",
    "            pred_unres_shuffled = model_unres.predict(X_unres_test_shuffled_3d, verbose=0).flatten()\n",
    "            e_unres_shuffled = y_test - pred_unres_shuffled\n",
    "            \n",
    "            # Compute loss difference\n",
    "            d_shuffled = e_res**2 - e_unres_shuffled**2\n",
    "            d_bar_shuffled = np.mean(d_shuffled)\n",
    "            bootstrap_stats.append(d_bar_shuffled)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bootstrap iteration {i}: {str(e)}\")\n",
    "            # Use a neutral value for failed iterations\n",
    "            bootstrap_stats.append(0.0)\n",
    "    \n",
    "    # Calculate bootstrap p-value\n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    valid_stats = bootstrap_stats[~np.isnan(bootstrap_stats)]\n",
    "    \n",
    "    if len(valid_stats) < n_bootstrap * 0.5:  # If more than half failed\n",
    "        print(f\"Too many bootstrap failures: {n_bootstrap - len(valid_stats)}/{n_bootstrap}\")\n",
    "        return d_bar_original, None, False, bootstrap_stats\n",
    "    \n",
    "    # Conservative p-value calculation\n",
    "    p_value = (np.sum(valid_stats >= d_bar_original) + 1) / (len(valid_stats) + 1)\n",
    "    causal = (d_bar_original > 0) and (p_value < 0.05)\n",
    "    \n",
    "    return d_bar_original, p_value, causal, bootstrap_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = 'CCC'\n",
    "df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "stationary_columns_with_lags = [x for x in df.columns if x.startswith(tuple(stationary_columns))]\n",
    "\n",
    "\n",
    "lstm_result = {}\n",
    "for company in companies:\n",
    "    lstm_result[company] = {}\n",
    "    stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "    stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "    df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "    \n",
    "    for stock_col in stationary_columns_STOCK:\n",
    "        lstm_result[company][stock_col] = {}\n",
    "        \n",
    "        for twitter_var in stationary_columns_with_lags:\n",
    "           \n",
    "            model_results = test_model_metrics(df=df,tweet_cols=[twitter_var],stock_col=[stock_col],company=company)\n",
    "            lstm_result[company][stock_col][twitter_var] = model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for col in stationary_columns:\n",
    "    d_bar_original, p_value, causal, bootstrap_stats = test_granger_causality(\n",
    "        Y='Wolumen',\n",
    "        X_base='Positive',\n",
    "        df=companies_data_daily_final_full[companies_data_daily_final_full['company'] == 'CCC'],\n",
    "        n_lags=1,\n",
    "        test_size=0.2,\n",
    "        epochs=60,\n",
    "        units=48,\n",
    "        block_size= autocorrelation_results_ord['CCC']['TWITTER'][col],\n",
    "        n_bootstrap=300\n",
    "    )\n",
    "    results.append({\n",
    "        'Variable': col,\n",
    "        'd_bar_original': d_bar_original,\n",
    "        'p_value': p_value,\n",
    "        'Causal': causal,\n",
    "        'Bootstrap Stats': bootstrap_stats\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Permutation helpers ---\n",
    "def _pick_block_len(T, p, base=None):\n",
    "    rot = int(np.ceil(1.5 * T**(1/3)))  # rule-of-thumb\n",
    "    b = max(3, p + 1, rot, base or 0)\n",
    "    return min(b, max(3, T // 5))  # keep your original cap for simplicity\n",
    "\n",
    "# Stationary bootstrap (Politis–Romano)\n",
    "def _stationary_bootstrap(s: pd.Series, p_start: float, rng=None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Bootstrap series with random-length circular blocks.\n",
    "    p_start = 1 / expected_block_length.\n",
    "    \"\"\"\n",
    "    rng = rng or np.random.default_rng()\n",
    "    x = s.to_numpy()\n",
    "    n = len(x)\n",
    "    out = np.empty(n, dtype=x.dtype)\n",
    "    pos = rng.integers(0, n)\n",
    "    out[0] = x[pos]\n",
    "    for t in range(1, n):\n",
    "        pos = rng.integers(0, n) if rng.random() < p_start else (pos + 1) % n\n",
    "        out[t] = x[pos]\n",
    "    return pd.Series(out, index=s.index)\n",
    "\n",
    "def granger_permutation(df, y, x, p, B=1000, base_block=None, exog=None):\n",
    "    \"\"\"\n",
    "    Permutation-based Granger test:\n",
    "      - Restricted model: y ~ lags(y) + exog\n",
    "      - Unrestricted:     y ~ lags(y) + lags(x) + exog\n",
    "    Returns dict with F, p_perm, B_valid, etc.\n",
    "    \"\"\"\n",
    "    exog = exog or []\n",
    "\n",
    "    # 1) Base frame: only the needed columns, no NaNs, sorted\n",
    "    df = df[[y, x] + exog].dropna().copy().sort_index()\n",
    "\n",
    "    # 2) Build lag columns\n",
    "    for L in range(1, p + 1):\n",
    "        df[f'{y}_L{L}'] = df[y].shift(L)\n",
    "        df[f'{x}_L{L}'] = df[x].shift(L)\n",
    "\n",
    "    cols_y = [f'{y}_L{L}' for L in range(1, p + 1)]\n",
    "    cols_x = [f'{x}_L{L}' for L in range(1, p + 1)]\n",
    "\n",
    "    # 3) Final modeling data (drops first p rows due to lags)\n",
    "    data = df[[y] + cols_y + cols_x + exog].dropna()\n",
    "    n = len(data)\n",
    "    if n < 2 * p + 5:\n",
    "        return {\"ok\": False, \"reason\": \"too_few_obs\"}\n",
    "\n",
    "    Y = data[y].to_numpy()\n",
    "    Xr = sm.add_constant(data[cols_y + exog], has_constant='add')\n",
    "    Xu = sm.add_constant(data[cols_y + cols_x + exog], has_constant='add')\n",
    "\n",
    "    m_r = sm.OLS(Y, Xr).fit()\n",
    "    m_u = sm.OLS(Y, Xu).fit()\n",
    "\n",
    "    rss_r, rss_u = m_r.ssr, m_u.ssr\n",
    "    k = p  # number of added x-lag regressors\n",
    "    dfd = m_u.df_resid  # robust degrees of freedom\n",
    "    if dfd <= 0:\n",
    "        return {\"ok\": False, \"reason\": \"nonpos_df\"}\n",
    "\n",
    "    F_act = ((rss_r - rss_u) / k) / (rss_u / dfd)\n",
    "\n",
    "    # 4) Bootstrap settings\n",
    "    block = _pick_block_len(len(data), p, base_block)\n",
    "    p_start = 1.0 / block\n",
    "    idx = data.index  # rows kept after lagging\n",
    "\n",
    "    # 5) Permutation loop (bootstrap x on full df index, then subset to idx)\n",
    "    cnt = 0\n",
    "    val = 0\n",
    "    for _ in range(B):\n",
    "        x_perm = _stationary_bootstrap(df[x], p_start)\n",
    "\n",
    "        # Build x-lags on full index, then subset to idx\n",
    "        xlags_full = pd.DataFrame(\n",
    "            {f'{x}_L{L}': x_perm.shift(L) for L in range(1, p + 1)},\n",
    "            index=df.index\n",
    "        )\n",
    "\n",
    "        Xb = pd.concat(\n",
    "            [\n",
    "                data[cols_y].reset_index(drop=True),\n",
    "                xlags_full.loc[idx, cols_x].reset_index(drop=True),\n",
    "                data[exog].reset_index(drop=True)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        Xb = sm.add_constant(Xb, has_constant='add')\n",
    "\n",
    "        # Should be NaN-free now; keep a light guard\n",
    "        if Xb.isnull().any().any():\n",
    "            continue\n",
    "\n",
    "        rss_u_b = sm.OLS(Y, Xb).fit().ssr\n",
    "        F_b = ((rss_r - rss_u_b) / k) / (rss_u_b / dfd)\n",
    "\n",
    "        cnt += (F_b >= F_act)\n",
    "        val += 1\n",
    "\n",
    "    p_perm = (cnt + 1) / (val + 1) if val else np.nan\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"F\": float(F_act),\n",
    "        \"p_perm\": float(p_perm) if np.isfinite(p_perm) else np.nan,\n",
    "        \"B_valid\": int(val),\n",
    "        \"block\": int(block),\n",
    "        \"p\": int(p),\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# ====== MAIN RUN ======\n",
    "# ======================\n",
    "filename = \"granger_permutation_results.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing file...\")\n",
    "    df_perm = pd.read_csv(filename)\n",
    "else:\n",
    "    B = 1000\n",
    "    BASE_BLOCK = 5\n",
    "    rows = []\n",
    "\n",
    "    # Expecting these to be defined in your environment:\n",
    "    # - stationary_columns: {company: {'STOCK': [...], 'TWITTER': [...]}, ...}\n",
    "    # - companies_data_daily_final_full: DataFrame with at least ['company','Date', ...]\n",
    "    for company, cols in stationary_columns.items():\n",
    "        dfc = companies_data_daily_final_full.loc[\n",
    "            companies_data_daily_final_full['company'] == company\n",
    "        ].copy()\n",
    "        dfc.index = pd.to_datetime(dfc['Date'])\n",
    "        dfc = dfc.sort_index()\n",
    "\n",
    "        for y in cols['STOCK']:\n",
    "            for x in cols['TWITTER']:\n",
    "                pair = dfc[[y, x]].dropna()\n",
    "                T = len(pair)\n",
    "                if T < 50:\n",
    "                    continue\n",
    "\n",
    "                p_cap = min(20, max(1, T // 8))\n",
    "                try:\n",
    "                    sel = VAR(pair).select_order(maxlags=p_cap)\n",
    "                    p = int(max(1, sel.bic))\n",
    "                except Exception:\n",
    "                    p = 1\n",
    "\n",
    "                if T < 8 * p:\n",
    "                    continue\n",
    "\n",
    "                res = granger_permutation(dfc, y=y, x=x, p=p, B=B, base_block=BASE_BLOCK)\n",
    "                res.update({\"T\": int(T), \"Company\": company, \"Target Variable\": y, \"Source Feature\": x})\n",
    "                if res.get(\"ok\"):\n",
    "                    rows.append({\n",
    "                        \"Company\": company,\n",
    "                        \"Target Variable\": y,\n",
    "                        \"Source Feature\": x,\n",
    "                        \"Lag\": res[\"p\"],\n",
    "                        \"F_stat\": res[\"F\"],\n",
    "                        \"p_value_perm\": res[\"p_perm\"],\n",
    "                        \"B_valid\": res[\"B_valid\"],\n",
    "                        \"block\": res[\"block\"],\n",
    "                        \"T\": res[\"T\"],\n",
    "                    })\n",
    "\n",
    "    df_perm = pd.DataFrame(rows)\n",
    "    df_perm.to_csv(filename, index=False)\n",
    "\n",
    "# --- (Re)compute BH–FDR within (Company, Target Variable), safely handling NaNs ---\n",
    "if not df_perm.empty and 'p_value_perm' in df_perm.columns:\n",
    "    df_perm['p_value_perm_bh'] = np.nan\n",
    "    df_perm['significant_perm_bh'] = False\n",
    "\n",
    "    for (c, t), idx_grp in df_perm.groupby(['Company', 'Target Variable']).groups.items():\n",
    "        mask = df_perm.index.isin(idx_grp)\n",
    "        valid = mask & np.isfinite(df_perm['p_value_perm'])\n",
    "        if valid.any():\n",
    "            rej, p_adj, *_ = multipletests(\n",
    "                df_perm.loc[valid, 'p_value_perm'], alpha=0.05, method='fdr_bh'\n",
    "            )\n",
    "            df_perm.loc[valid, 'p_value_perm_bh'] = p_adj\n",
    "            df_perm.loc[valid, 'significant_perm_bh'] = rej\n",
    "\n",
    "    df_perm['Flag'] = df_perm['significant_perm_bh'].fillna(False).astype(bool)\n",
    "\n",
    "    # persist the updated BH/Flag columns\n",
    "    df_perm.to_csv(filename, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_granger_causality(\n",
    "        Y='Wolumen',\n",
    "        X_base='sentiment_divergence',\n",
    "        df=companies_data_daily_final_full[companies_data_daily_final_full['company'] == 'CCC'],\n",
    "        n_lags=1,\n",
    "        test_size=0.2,\n",
    "        block_size= autocorrelation_results_ord['CCC']['TWITTER']['sentiment_divergence'],\n",
    "        n_bootstrap=100\n",
    "    )\n",
    "\n",
    "sns.histplot(res['boostrap_stats'], kde=True)  # kde=True adds smooth curve\n",
    "plt.title(\"Distribution of Data\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adca484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company = 'CCC'\n",
    "# df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "# stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "# stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "# stationary_columns_with_lags = [x for x in df.columns if x.startswith(tuple(stationary_columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import norm, t\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling precomputed lags turned out to be too complex, so we will not use it in the final analysis.\n",
    "# companies_data_daily_final = {}\n",
    "# for company, df in companies_data_daily.items():\n",
    "# #     companies_data_daily_final[company] = create_prediction_lags(df, tweet_col=tweet_columns, max_lags=5)\n",
    "# # companies_data_daily_final_full = pd.DataFrame()\n",
    "# tweet_columns_with_lagged = [x for x in companies_data_daily_final['ALLEGRO'] if x.startswith(tuple(tweet_columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c038fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionaries\n",
    "autocorrelation_results = {}\n",
    "# Autocorrelation threshold\n",
    "autocorr_p_threshold = 0.05  # p-value threshold for autocorrelation significance\n",
    "max_lags = 10  # Maximum number of lags to test\n",
    "\n",
    "# Check autocorrelation for each variable\n",
    "for company in companies:\n",
    "    STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "    TWITTER = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "    autocorrelation_results[company] = {'STOCK': {}, 'TWITTER': {}}\n",
    "    \n",
    "    for variable in STOCK + TWITTER:\n",
    "        # Extract data for the specific variable and company\n",
    "        series = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company][variable].dropna()\n",
    "        \n",
    "        # Skip if series is too short\n",
    "        if len(series) <= max_lags:\n",
    "            category = 'STOCK' if variable in STOCK else 'TWITTER'\n",
    "            autocorrelation_results[company][category][variable] = False\n",
    "            continue\n",
    "        \n",
    "        # Perform the Ljung-Box test for autocorrelation\n",
    "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "        result = acorr_ljungbox(series, lags=max_lags, return_df=True)\n",
    "        \n",
    "        # Check if any lag shows significant autocorrelation\n",
    "        p_values = result['lb_pvalue']\n",
    "        has_autocorrelation = any(p_values < autocorr_p_threshold)\n",
    "        \n",
    "        # Store whether the variable has significant autocorrelation\n",
    "        category = 'STOCK' if variable in STOCK else 'TWITTER'\n",
    "        autocorrelation_results[company][category][variable] = has_autocorrelation\n",
    "\n",
    "pprint.pprint(autocorrelation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "IC = \"bic\"\n",
    "ALPHA = 0.05\n",
    "MIN_T = 50\n",
    "\n",
    "granger_results = {}\n",
    "\n",
    "for company, _ in stationarity_results.items():\n",
    "    granger_results[company] = {}\n",
    "\n",
    "    df_company = companies_data_daily_final_full.loc[\n",
    "        companies_data_daily_final_full[\"company\"] == company\n",
    "    ].copy()\n",
    "    df_company.index = pd.to_datetime(df_company[\"Date\"])\n",
    "    df_company = df_company.sort_index()\n",
    "\n",
    "    stationary_stock   = stationary_columns[company][\"STOCK\"]\n",
    "    stationary_twitter = stationary_columns[company][\"TWITTER\"]\n",
    "\n",
    "    for stock_var in stationary_stock:\n",
    "        granger_results[company][stock_var] = {}\n",
    "\n",
    "        for twitter_var in stationary_twitter:\n",
    "            df_pair = df_company[[stock_var, twitter_var]].dropna().copy()\n",
    "            T = len(df_pair)\n",
    "            if T < MIN_T:\n",
    "                continue\n",
    "\n",
    "            model = VAR(df_pair)\n",
    "            p_cap = min(20, max(1, T // 8))\n",
    "            sel = model.select_order(maxlags=p_cap)\n",
    "            p_opt = int(max(1, getattr(sel, IC)))   # AIC or BIC\n",
    "\n",
    "            if T < 8 * p_opt:\n",
    "                continue\n",
    "\n",
    "            # --- FIT (avoid name 'var_results') ---\n",
    "            res = VAR(df_pair).fit(maxlags=int(p_opt), ic=None, trend=\"c\")\n",
    "            assert res.k_ar == int(p_opt)  # sanity check\n",
    "\n",
    "            # Granger: twitter_var -> stock_var\n",
    "            gr = res.test_causality(caused=stock_var, causing=twitter_var, kind=\"f\")\n",
    "\n",
    "            # Stability & whiteness\n",
    "            is_stable = bool(res.is_stable())\n",
    "            try:\n",
    "                wh = res.test_whiteness(nlags=min(10, max(5, p_opt + 2)))\n",
    "                white_p = float(wh.pvalue)\n",
    "                resid_white = bool(white_p >= ALPHA)\n",
    "            except Exception:\n",
    "                white_p, resid_white = np.nan, True\n",
    "\n",
    "            # ARCH per equation\n",
    "            diagnostics = {}\n",
    "            resid = res.resid\n",
    "            for col in df_pair.columns:\n",
    "                try:\n",
    "                    lm_stat, lm_p, f_stat, f_p = het_arch(\n",
    "                        resid[col].to_numpy(), nlags=min(10, T // 10)\n",
    "                    )\n",
    "                    diagnostics[col] = {\"arch_lm_p\": float(lm_p), \"arch_flag\": bool(lm_p < ALPHA)}\n",
    "                except Exception:\n",
    "                    diagnostics[col] = {\"arch_lm_p\": np.nan, \"arch_flag\": None}\n",
    "\n",
    "            granger_results[company][stock_var][twitter_var] = {\n",
    "                \"p_opt\": int(p_opt),\n",
    "                \"T\": int(T),\n",
    "                \"ic_used\": IC,\n",
    "                \"F_stat\": float(gr.test_statistic),\n",
    "                \"p_value\": float(gr.pvalue),\n",
    "                \"significant\": bool(gr.pvalue < ALPHA),\n",
    "                \"stable\": is_stable,\n",
    "                \"resid_white\": resid_white,\n",
    "                \"resid_white_p\": white_p,\n",
    "                \"diagnostics\": diagnostics,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33685af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # global picture\n",
    "# p = df_perm['p_value_perm'].dropna().values\n",
    "# print(\"m=\", p.size, \"min p=\", p.min() if p.size else None, \"median=\", np.median(p) if p.size else None)\n",
    "\n",
    "# # BH per (Company, Target), q=0.10 for exploratory\n",
    "# for (c,t), idx in df_perm.groupby(['Company','Target Variable']).groups.items():\n",
    "#     mask = df_perm.index.isin(idx)\n",
    "#     rej, p_adj, *_ = multipletests(df_perm.loc[mask,'p_value_perm'], alpha=0.05, method='fdr_bh')\n",
    "#     df_perm.loc[mask,'p_value_perm_bh'] = p_adj\n",
    "#     df_perm.loc[mask,'significant_perm_bh'] = rej.astype(bool)\n",
    "\n",
    "# # plot-ready flag (exploratory)\n",
    "# df_perm['Flag'] = df_perm['significant_perm_bh'].fillna(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary of variables to drop per company\n",
    "# columns_to_drop = {\n",
    "#     '11BIT': ['view_count_Neutral','engagement_impact'],\n",
    "#     'ALLEGRO': ['Neutral',''],\n",
    "#     'CCC': ['view_weighted_polarity'],\n",
    "#     'CDR': ['tweet_volume', 'view_weighted_polarity'],\n",
    "#     'INPOST': ['tweet_volume', 'view_weighted_polarity'],\n",
    "#     'XTB': ['tweet_volume', 'retweet_count_Positive', 'view_weighted_polarity'],\n",
    "#     'MENTZEN': ['view_count_Positive', 'retweet_count_Positive', 'sentiment_score_views',\n",
    "#                 'retweet_count_Neutral', 'total_retweet_count', 'sentiment_intensity', 'view_weighted_polarity'],\n",
    "#     'ŻABKA': ['tweet_volume']\n",
    "# }\n",
    "# columns_to_drop\n",
    "\n",
    "# corr_per_company = {}\n",
    "# for company in companies:\n",
    "#     print(company)\n",
    "#     df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "\n",
    "#     stationary_columns_curr = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "#     cols_to_drop = columns_to_drop.get(company, [])\n",
    "#     df_corr = df[stationary_columns_curr].corr()\n",
    "#     df_corr.reset_index(inplace=True)\n",
    "#     df_corr_melted = pd.melt(df_corr,'index')\n",
    "\n",
    "#     df_corr_melted = df_corr_melted[~((df_corr_melted['index'].isin(cols_to_drop)) | (df_corr_melted['variable'].isin(cols_to_drop)))]\n",
    "#     corr_per_company[company] = df_corr_melted[(df_corr_melted['value']>=0.9) & (df_corr_melted['value']<1) ]\n",
    "#     pprint.pprint(corr_per_company[company])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
