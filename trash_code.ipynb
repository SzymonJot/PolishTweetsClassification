{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib  # Better alternative for large objects\n",
    "# import numpy as np\n",
    "\n",
    "# granger_results_lstm = {}\n",
    "\n",
    "# for company, results in stationary_columns.items():\n",
    "#     print(f\"Processing company: {company}\")\n",
    "#     df_company = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company].copy()\n",
    "    \n",
    "#     # Ensure chronological ordering\n",
    "#     df_company = df_company.sort_index()  # Critical for time series\n",
    "    \n",
    "#     granger_results_lstm[company] = {}\n",
    "    \n",
    "#     for stock_var in results['STOCK']:\n",
    "#         granger_results_lstm[company][stock_var] = {}\n",
    "#         print(f\"  Stock variable: {stock_var}\")\n",
    "        \n",
    "#         for twitter_var in results['TWITTER']:\n",
    "#             print(f\"Twitter variable: {twitter_var}\")\n",
    "#             max_lags = granger_results.get(company, {}).get(stock_var, {}).get(twitter_var, {}).get('optimal_lags', 15)\n",
    "#             max_lags = int(max_lags)\n",
    "#             print(f\"Max lags: {max_lags}\")\n",
    "#             # Create dataset with correct column order [STOCK, TWEET]\n",
    "#             data = df_company[[stock_var, twitter_var]].copy()\n",
    "            \n",
    "            \n",
    "#             # Temporal split (preserve order)\n",
    "#             n = len(data)\n",
    "#             train_size = int(0.6 * n)\n",
    "#             valid_size = int(0.2 * n)\n",
    "            \n",
    "          \n",
    "            \n",
    "#             # Standardize using TRAIN stats only\n",
    "#             scaler = StandardScaler()\n",
    "#             data_train_scaled = scaler.fit_transform(data_train)\n",
    "#             data_valid_scaled = scaler.transform(data_valid)\n",
    "#             data_test_scaled = scaler.transform(data_test)\n",
    "            \n",
    "#             # Run causality test\n",
    "#             try:\n",
    "#                 results_cas = nonlincausality.nonlincausalityNN(\n",
    "#                 x=data_train_scaled,\n",
    "#                 maxlag=max_lags,  # Reduced maxlag for small datasets\n",
    "#                 NN_config=['d', 'dr', 'd'],  # Added extra Dense layer for better feature extraction\n",
    "#                 NN_neurons=[8, 0.3, 4],  # Reduced neurons to prevent overfitting\n",
    "#                 x_test=data_test_scaled,\n",
    "#                 run=3,  # Reduced runs for faster computation\n",
    "#                 epochs_num=[30, 20],  # Reduced epochs\n",
    "#                 learning_rate=[0.005, 0.0005],  # Adjusted learning rates\n",
    "#                 batch_size_num=16,  # Increased batch size for stability\n",
    "#                 x_val=data_valid_scaled,\n",
    "#                 regularization='l1_l2',  # Combined regularization\n",
    "#                 reg_alpha=[0.005, 0.005],  # Smaller regularization\n",
    "#                 callbacks=None,\n",
    "#                 verbose=False,\n",
    "#                 plot=False\n",
    "#                 )\n",
    "#                 # Store lightweight results summary\n",
    "#                 granger_results_lstm[company][stock_var][twitter_var] = {\n",
    "#                     'lags': {},\n",
    "#                     'metadata': {\n",
    "#                         'train_start': data_train.index[0],\n",
    "#                         'train_end': data_train.index[-1],\n",
    "#                         'test_start': data_test.index[0],\n",
    "#                         'test_end': data_test.index[-1],\n",
    "#                         'n_obs': {\n",
    "#                             'train': len(data_train),\n",
    "#                             'valid': len(data_valid),\n",
    "#                             'test': len(data_test)\n",
    "#                         }\n",
    "#                     }\n",
    "#                 }\n",
    "                \n",
    "#                 for lag, res_obj in results_cas.items():\n",
    "#                     granger_results_lstm[company][stock_var][twitter_var]['lags'][lag] = {\n",
    "#                         'p_value': res_obj.p_value,\n",
    "#                         'test_statistic': res_obj.test_statistic,\n",
    "#                         'RSS_X': res_obj.RSS_X_all,\n",
    "#                         'RSS_XY': res_obj.RSS_XY_all,\n",
    "#                         'best_error_X': res_obj.best_errors_X.tolist(),  # Convert arrays\n",
    "#                         'best_error_XY': res_obj.best_errors_XY.tolist()\n",
    "#                     }\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed for {company}-{stock_var}-{twitter_var}: {str(e)}\")\n",
    "#                 granger_results_lstm[company][stock_var][twitter_var] = {'error': str(e)}\n",
    "\n",
    "# # Save results with metadata preservation\n",
    "# joblib.dump(granger_results_lstm, 'granger_results_lstm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db29580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm_hyperparameters(X_train, y_train, input_shape):\n",
    "    \"\"\"Find optimal units and epochs using time-series cross-validation\"\"\"\n",
    "    best_units = 32  # Default\n",
    "    best_epochs = 100  # Default\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Hyperparameter search space\n",
    "    units_options = [16, 32, 48]\n",
    "    epochs_options = [10, 30, 60]\n",
    "    \n",
    "    # Reduced CV for efficiency\n",
    "    tscv = TimeSeriesSplit(n_splits=2)\n",
    "    \n",
    "    for units in units_options:\n",
    "        for max_epochs in epochs_options:\n",
    "            val_losses = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                # Split training/validation\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                # Build model\n",
    "                model = Sequential([\n",
    "                    LSTM(units, input_shape=input_shape),\n",
    "                    Dense(1)\n",
    "                ])\n",
    "                model.compile(optimizer='adam', loss='mse')\n",
    "                \n",
    "                # Train with early stopping\n",
    "                early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                          patience=5, \n",
    "                                          restore_best_weights=True,\n",
    "                                          verbose=0)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    X_fold_train, y_fold_train,\n",
    "                    validation_data=(X_fold_val, y_fold_val),\n",
    "                    epochs=max_epochs,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Track best validation loss\n",
    "                val_losses.append(min(history.history['val_loss']))\n",
    "            \n",
    "            # Average validation loss across folds\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_units = units\n",
    "                best_epochs = max_epochs\n",
    "    \n",
    "    return best_units, best_epochs\n",
    "\n",
    "def train_final_model(X_train, y_train, input_shape, units, epochs):\n",
    "    \"\"\"Train final model with optimal hyperparameters\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Early stopping on validation split\n",
    "    early_stop = EarlyStopping(monitor='val_loss', \n",
    "                              patience=5, \n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def test_granger_causality(Y, X_base, df, n_lags=5, test_size=0.2, \n",
    "                          n_bootstrap=200, block_size=5):\n",
    "    \"\"\"\n",
    "    Main Granger causality test function with:\n",
    "    - Hyperparameter tuning\n",
    "    - Block bootstrapping\n",
    "    \"\"\"\n",
    "    # 1. Prepare data\n",
    "    temp_df = pd.DataFrame(index=df.index)\n",
    "    temp_df[Y] = df[Y]\n",
    "    # Create Y lags\n",
    "    for i in range(1, n_lags+1):\n",
    "        temp_df[f'{Y}_lag_{i}'] = temp_df[Y].shift(i)\n",
    "    \n",
    "    # Identify X lags\n",
    "    X_lags = [f'{X_base}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    missing = [col for col in X_lags if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Missing columns for {X_base}: {missing}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Combine and clean\n",
    "    data = pd.concat([temp_df, df[X_lags]], axis=1).dropna()\n",
    "    y_data = data[Y].values\n",
    "    Y_lag_cols = [f'{Y}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    full_index = data.index\n",
    "    \n",
    "    # 2. Train-test split (time-based)\n",
    "    n_test = int(len(data) * test_size)\n",
    "    n_train = len(data) - n_test\n",
    "    splits = {\n",
    "        'train': slice(None, n_train),\n",
    "        'test': slice(n_train, None)\n",
    "    }\n",
    "    \n",
    "    # 3. Prepare feature sets\n",
    "    # Restricted model (only Y lags)\n",
    "    X_restricted = data[Y_lag_cols].values\n",
    "    \n",
    "    # Unrestricted model (Y and X lags)\n",
    "    X_unrestricted = np.zeros((len(data), n_lags, 2))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(n_lags):\n",
    "            X_unrestricted[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "            X_unrestricted[i, j, 1] = data[X_lags[j]].iloc[i]\n",
    "    \n",
    "    # Split data\n",
    "    X_res_train, X_res_test = X_restricted[splits['train']], X_restricted[splits['test']]\n",
    "    X_unres_train, X_unres_test = X_unrestricted[splits['train']], X_unrestricted[splits['test']]\n",
    "    y_train, y_test = y_data[splits['train']], y_data[splits['test']]\n",
    "    \n",
    "    # 4. Scaling\n",
    "    scaler_res = StandardScaler()\n",
    "    X_res_train_scaled = scaler_res.fit_transform(X_res_train)\n",
    "    X_res_test_scaled = scaler_res.transform(X_res_test)\n",
    "    X_res_train_3d = X_res_train_scaled.reshape((-1, n_lags, 1))\n",
    "    X_res_test_3d = X_res_test_scaled.reshape((-1, n_lags, 1))\n",
    "    \n",
    "    scaler_unres = StandardScaler()\n",
    "    X_unres_train_2d = X_unres_train.reshape((-1, n_lags*2))\n",
    "    X_unres_test_2d = X_unres_test.reshape((-1, n_lags*2))\n",
    "    X_unres_train_scaled = scaler_unres.fit_transform(X_unres_train_2d)\n",
    "    X_unres_test_scaled = scaler_unres.transform(X_unres_test_2d)\n",
    "    X_unres_train_3d = X_unres_train_scaled.reshape((-1, n_lags, 2))\n",
    "    X_unres_test_3d = X_unres_test_scaled.reshape((-1, n_lags, 2))\n",
    "    \n",
    "    # 5. Hyperparameter tuning\n",
    "    print(f\"Tuning hyperparameters for {Y} <- {X_base}...\")\n",
    "    units_res, epochs_res = tune_lstm_hyperparameters(\n",
    "        X_res_train_3d, y_train, (n_lags, 1))\n",
    "    units_unres, epochs_unres = tune_lstm_hyperparameters(\n",
    "        X_unres_train_3d, y_train, (n_lags, 2))\n",
    "    \n",
    "    print(f\"Best params - Restricted: units={units_res}, epochs={epochs_res}\")\n",
    "    print(f\"Best params - Unrestricted: units={units_unres}, epochs={epochs_unres}\")\n",
    "    \n",
    "    # 6. Train final models\n",
    "    model_res = train_final_model(\n",
    "        X_res_train_3d, y_train, (n_lags, 1), units_res, epochs_res)\n",
    "    model_unres = train_final_model(\n",
    "        X_unres_train_3d, y_train, (n_lags, 2), units_unres, epochs_unres)\n",
    "    \n",
    "    # 7. Original predictions\n",
    "    pred_res = model_res.predict(X_res_test_3d).flatten()\n",
    "    pred_unres = model_unres.predict(X_unres_test_3d).flatten()\n",
    "    e_res = y_test - pred_res\n",
    "    e_unres = y_test - pred_unres\n",
    "    d_original = e_res**2 - e_unres**2\n",
    "    d_bar_original = np.mean(d_original)\n",
    "    \n",
    "    # 8. Bootstrapping\n",
    "    base_series = df.loc[full_index, X_base].copy()\n",
    "    bootstrap_stats = []\n",
    "    \n",
    "    print(f\"Running {n_bootstrap} bootstraps...\")\n",
    "    for b in range(n_bootstrap):\n",
    "        if (b+1) % 50 == 0:\n",
    "            print(f\"  Bootstrap {b+1}/{n_bootstrap}\")\n",
    "        \n",
    "        # Create shuffled X series\n",
    "        shuffled_base = block_shuffle(base_series, block_size)\n",
    "        \n",
    "        # Create new lag features from shuffled series\n",
    "        temp = pd.DataFrame(index=full_index)\n",
    "        temp[X_base] = shuffled_base\n",
    "        for i in range(1, n_lags+1):\n",
    "            temp[f'{X_base}_lag_{i}'] = temp[X_base].shift(i)\n",
    "        \n",
    "        # Build new unrestricted dataset\n",
    "        X_unrestricted_shuffled = np.zeros((len(data), n_lags, 2))\n",
    "        for i in range(len(data)):\n",
    "            for j in range(n_lags):\n",
    "                X_unrestricted_shuffled[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "                X_unrestricted_shuffled[i, j, 1] = temp[X_lags[j]].iloc[i]\n",
    "        \n",
    "        # Extract test portion\n",
    "        X_unres_test_shuffled = X_unrestricted_shuffled[splits['test']]\n",
    "        X_unres_test_shuffled_2d = X_unres_test_shuffled.reshape((-1, n_lags*2))\n",
    "        X_unres_test_shuffled_scaled = scaler_unres.transform(X_unres_test_shuffled_2d)\n",
    "        X_unres_test_shuffled_3d = X_unres_test_shuffled_scaled.reshape((-1, n_lags, 2))\n",
    "        \n",
    "        # Predict with shuffled X\n",
    "        pred_unres_shuffled = model_unres.predict(X_unres_test_shuffled_3d).flatten()\n",
    "        e_unres_shuffled = y_test - pred_unres_shuffled\n",
    "        \n",
    "        # Compute loss difference\n",
    "        d_shuffled = e_res**2 - e_unres_shuffled**2\n",
    "        d_bar_shuffled = np.mean(d_shuffled)\n",
    "        bootstrap_stats.append(d_bar_shuffled)\n",
    "    \n",
    "    # 9. Calculate p-value\n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    p_value = (np.sum(bootstrap_stats >= d_bar_original) + 1) / (n_bootstrap + 1)\n",
    "    causal = (d_bar_original > 0) and (p_value < 0.05)\n",
    "    \n",
    "    return {\n",
    "        'Y': Y,\n",
    "        'X': X_base,\n",
    "        'd_bar': d_bar_original,\n",
    "        'p_value': p_value,\n",
    "        'causal': causal,\n",
    "        'units_res': units_res,\n",
    "        'epochs_res': epochs_res,\n",
    "        'units_unres': units_unres,\n",
    "        'epochs_unres': epochs_unres,\n",
    "        'bootstrap_mean': np.mean(bootstrap_stats),\n",
    "        'boostrap_stats':bootstrap_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb63274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_predictions(df, tweet_cols, stock_cols, test_days=100, lookback=7):\n",
    "   # Split into train and test\n",
    "   train_df = df[:-test_days]\n",
    "   test_df = df[-test_days:]\n",
    "   \n",
    "   predictions = []\n",
    "   \n",
    "   for i in range(len(test_df)):\n",
    "       # Get updated training data including latest actual values\n",
    "       current_train = pd.concat([train_df, test_df[:i]])\n",
    "       \n",
    "       # Prepare data\n",
    "       X, y, scaler_y = prepare_data(current_train, tweet_cols, stock_cols, lookback)\n",
    "       \n",
    "       # Train model\n",
    "       model = create_model((lookback, len(tweet_cols)), len(stock_cols))\n",
    "       model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
    "       \n",
    "       # Predict next day\n",
    "       last_sequence = scaler_X.transform(current_train[tweet_cols].tail(lookback))\n",
    "       pred = model.predict(last_sequence.reshape(1, lookback, -1))\n",
    "       pred = scaler_y.inverse_transform(pred)[0]\n",
    "       predictions.append(pred)\n",
    "   \n",
    "   return np.array(predictions)\n",
    "\n",
    "predictions = create_rolling_predictions(df, tweet_columns, stock_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lookbacks(df, tweet_cols, stock_cols, lookbacks=[1]):\n",
    "    results = {}\n",
    "    \n",
    "    for lookback in lookbacks:\n",
    "        X, y, scaler_y = prepare_data(df, tweet_cols, stock_cols, lookback)\n",
    "        \n",
    "        # Train/test split\n",
    "        split = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model((lookback, len(tweet_cols)), len(stock_cols))\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                          validation_split=0.1, verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions = scaler_y.inverse_transform(predictions)\n",
    "        true_values = scaler_y.inverse_transform(y_test)\n",
    "        \n",
    "        mse = np.mean((predictions - true_values) ** 2)\n",
    "        results[lookback] = {\n",
    "            'mse': mse,\n",
    "            'val_loss': min(history.history['val_loss'])\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44911710",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_lookbacks(merged_daily, tweet_columns, stock_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def block_shuffle(series, block_size):\n",
    "    \"\"\"Shuffle blocks of a time series while preserving internal structure.\"\"\"\n",
    "    n = len(series)\n",
    "    n_blocks = n // block_size\n",
    "    blocks = [series[i*block_size:(i+1)*block_size].values for i in range(n_blocks)]\n",
    "    \n",
    "    # Handle remainder if exists\n",
    "    remainder = n % block_size\n",
    "    if remainder > 0:\n",
    "        blocks.append(series[n_blocks*block_size:].values)\n",
    "    \n",
    "    np.random.shuffle(blocks)\n",
    "    # Maintain original index structure\n",
    "    shuffled_values = np.concatenate(blocks)\n",
    "    return pd.Series(shuffled_values, index=series.index[:len(shuffled_values)])\n",
    "\n",
    "def test_granger_causality(Y, X_base, df, n_lags=5, test_size=0.2, \n",
    "                           epochs=30, units=30, n_bootstrap=200, block_size=30):\n",
    "    \"\"\"\n",
    "    Test Granger causality using LSTM neural networks with block bootstrapping.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: Target variable (string)\n",
    "    - X_base: Base predictor variable (string) \n",
    "    - df: DataFrame containing all variables and their lags\n",
    "    - n_lags: Number of lags to use\n",
    "    - test_size: Proportion of data for testing\n",
    "    - epochs: Number of training epochs for LSTM\n",
    "    - units: Number of LSTM units\n",
    "    - n_bootstrap: Number of bootstrap iterations\n",
    "    - block_size: Size of blocks for block bootstrapping\n",
    "    \n",
    "    Returns:\n",
    "    - d_bar_original: Original test statistic\n",
    "    - p_value: Bootstrap p-value\n",
    "    - causal: Boolean indicating significant causality\n",
    "    - bootstrap_stats: Array of bootstrap statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if Y not in df.columns:\n",
    "        raise ValueError(f\"Target variable '{Y}' not found in DataFrame\")\n",
    "    if X_base not in df.columns:\n",
    "        raise ValueError(f\"Base predictor '{X_base}' not found in DataFrame\")\n",
    "    \n",
    "    # Create lags for Y if they don't exist\n",
    "    temp_df = pd.DataFrame(index=df.index)\n",
    "    temp_df[Y] = df[Y]\n",
    "    for i in range(1, n_lags+1):\n",
    "        temp_df[f'{Y}_lag_{i}'] = temp_df[Y].shift(i)\n",
    "    \n",
    "    # Identify lag columns for X_base\n",
    "    X_lags = [f'{X_base}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    missing = [col for col in X_lags if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Missing lag columns for {X_base}: {missing}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Combine data and drop NA\n",
    "    data = pd.concat([temp_df, df[X_lags]], axis=1).dropna()\n",
    "    \n",
    "    if len(data) < 50:  # Minimum data requirement\n",
    "        print(f\"Insufficient data after removing NAs: {len(data)} rows\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    y_data = data[Y].values\n",
    "    Y_lag_cols = [f'{Y}_lag_{i}' for i in range(1, n_lags+1)]\n",
    "    full_index = data.index\n",
    "    \n",
    "    # Time-based train-test split\n",
    "    n_test = max(1, int(len(data) * test_size))  # Ensure at least 1 test sample\n",
    "    n_train = len(data) - n_test\n",
    "    \n",
    "    if n_train < 10:  # Minimum training requirement\n",
    "        print(f\"Insufficient training data: {n_train} samples\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Build datasets\n",
    "    X_restricted = data[Y_lag_cols].values\n",
    "    X_unrestricted = np.zeros((len(data), n_lags, 2))\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(n_lags):\n",
    "            X_unrestricted[i, j, 0] = data[Y_lag_cols[j]].iloc[i]\n",
    "            X_unrestricted[i, j, 1] = data[X_lags[j]].iloc[i]\n",
    "    \n",
    "    # Split data\n",
    "    X_res_train, X_res_test = X_restricted[:n_train], X_restricted[n_train:]\n",
    "    X_unres_train, X_unres_test = X_unrestricted[:n_train], X_unrestricted[n_train:]\n",
    "    y_train, y_test = y_data[:n_train], y_data[n_train:]\n",
    "    \n",
    "    # Scaling\n",
    "    scaler_res = StandardScaler()\n",
    "    X_res_train_scaled = scaler_res.fit_transform(X_res_train)\n",
    "    X_res_test_scaled = scaler_res.transform(X_res_test)\n",
    "    X_res_train_3d = X_res_train_scaled.reshape((-1, n_lags, 1))\n",
    "    X_res_test_3d = X_res_test_scaled.reshape((-1, n_lags, 1))\n",
    "    \n",
    "    scaler_unres = StandardScaler()\n",
    "    X_unres_train_2d = X_unres_train.reshape((-1, n_lags*2))\n",
    "    X_unres_test_2d = X_unres_test.reshape((-1, n_lags*2))\n",
    "    X_unres_train_scaled = scaler_unres.fit_transform(X_unres_train_2d)\n",
    "    X_unres_test_scaled = scaler_unres.transform(X_unres_test_2d)\n",
    "    X_unres_train_3d = X_unres_train_scaled.reshape((-1, n_lags, 2))\n",
    "    X_unres_test_3d = X_unres_test_scaled.reshape((-1, n_lags, 2))\n",
    "    \n",
    "    # Model building function\n",
    "    def build_model(input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(units, input_shape=input_shape, return_sequences=False),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    # Train original models with error handling\n",
    "    try:\n",
    "        model_res = build_model((n_lags, 1))\n",
    "        model_unres = build_model((n_lags, 2))\n",
    "        \n",
    "        # Suppress verbose output\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model_res.fit(X_res_train_3d, y_train, epochs=epochs, batch_size=min(32, len(y_train)//2), verbose=0)\n",
    "            model_unres.fit(X_unres_train_3d, y_train, epochs=epochs, batch_size=min(32, len(y_train)//2), verbose=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training models: {str(e)}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    # Original predictions and errors\n",
    "    try:\n",
    "        pred_res = model_res.predict(X_res_test_3d, verbose=0).flatten()\n",
    "        pred_unres = model_unres.predict(X_unres_test_3d, verbose=0).flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {str(e)}\")\n",
    "        return None, None, False, None\n",
    "    \n",
    "    e_res = y_test - pred_res\n",
    "    e_unres = y_test - pred_unres\n",
    "    d_original = e_res**2 - e_unres**2\n",
    "    d_bar_original = np.mean(d_original)\n",
    "    \n",
    "    # Prepare for bootstrapping\n",
    "    base_series = df.loc[full_index, X_base].copy()\n",
    "    bootstrap_stats = []\n",
    "    \n",
    "    print(f\"Running {n_bootstrap} bootstrap iterations...\")\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Bootstrap iteration {i + 1}/{n_bootstrap}\")\n",
    "        \n",
    "        try:\n",
    "            # Create shuffled X series with block bootstrapping\n",
    "            shuffled_base = block_shuffle(base_series, block_size)\n",
    "            \n",
    "            # Create new lag features from shuffled series\n",
    "            temp = pd.DataFrame(index=full_index)\n",
    "            temp[X_base] = shuffled_base\n",
    "            for j in range(1, n_lags+1):\n",
    "                temp[f'{X_base}_lag_{j}'] = temp[X_base].shift(j)\n",
    "            \n",
    "            # Build new unrestricted dataset\n",
    "            X_unrestricted_shuffled = np.zeros((len(data), n_lags, 2))\n",
    "            for k in range(len(data)):\n",
    "                for j in range(n_lags):\n",
    "                    X_unrestricted_shuffled[k, j, 0] = data[Y_lag_cols[j]].iloc[k]\n",
    "                    # Handle potential NaN values from shifted data\n",
    "                    lag_val = temp[X_lags[j]].iloc[k] if k < len(temp) else 0\n",
    "                    X_unrestricted_shuffled[k, j, 1] = lag_val if not pd.isna(lag_val) else 0\n",
    "            \n",
    "            # Extract test portion and scale\n",
    "            X_unres_test_shuffled = X_unrestricted_shuffled[n_train:]\n",
    "            X_unres_test_shuffled_2d = X_unres_test_shuffled.reshape((-1, n_lags*2))\n",
    "            X_unres_test_shuffled_scaled = scaler_unres.transform(X_unres_test_shuffled_2d)\n",
    "            X_unres_test_shuffled_3d = X_unres_test_shuffled_scaled.reshape((-1, n_lags, 2))\n",
    "            \n",
    "            # Predict with shuffled X\n",
    "            pred_unres_shuffled = model_unres.predict(X_unres_test_shuffled_3d, verbose=0).flatten()\n",
    "            e_unres_shuffled = y_test - pred_unres_shuffled\n",
    "            \n",
    "            # Compute loss difference\n",
    "            d_shuffled = e_res**2 - e_unres_shuffled**2\n",
    "            d_bar_shuffled = np.mean(d_shuffled)\n",
    "            bootstrap_stats.append(d_bar_shuffled)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bootstrap iteration {i}: {str(e)}\")\n",
    "            # Use a neutral value for failed iterations\n",
    "            bootstrap_stats.append(0.0)\n",
    "    \n",
    "    # Calculate bootstrap p-value\n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    valid_stats = bootstrap_stats[~np.isnan(bootstrap_stats)]\n",
    "    \n",
    "    if len(valid_stats) < n_bootstrap * 0.5:  # If more than half failed\n",
    "        print(f\"Too many bootstrap failures: {n_bootstrap - len(valid_stats)}/{n_bootstrap}\")\n",
    "        return d_bar_original, None, False, bootstrap_stats\n",
    "    \n",
    "    # Conservative p-value calculation\n",
    "    p_value = (np.sum(valid_stats >= d_bar_original) + 1) / (len(valid_stats) + 1)\n",
    "    causal = (d_bar_original > 0) and (p_value < 0.05)\n",
    "    \n",
    "    return d_bar_original, p_value, causal, bootstrap_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = 'CCC'\n",
    "df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "stationary_columns_with_lags = [x for x in df.columns if x.startswith(tuple(stationary_columns))]\n",
    "\n",
    "\n",
    "lstm_result = {}\n",
    "for company in companies:\n",
    "    lstm_result[company] = {}\n",
    "    stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "    stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "    df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "    \n",
    "    for stock_col in stationary_columns_STOCK:\n",
    "        lstm_result[company][stock_col] = {}\n",
    "        \n",
    "        for twitter_var in stationary_columns_with_lags:\n",
    "           \n",
    "            model_results = test_model_metrics(df=df,tweet_cols=[twitter_var],stock_col=[stock_col],company=company)\n",
    "            lstm_result[company][stock_col][twitter_var] = model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for col in stationary_columns:\n",
    "    d_bar_original, p_value, causal, bootstrap_stats = test_granger_causality(\n",
    "        Y='Wolumen',\n",
    "        X_base='Positive',\n",
    "        df=companies_data_daily_final_full[companies_data_daily_final_full['company'] == 'CCC'],\n",
    "        n_lags=1,\n",
    "        test_size=0.2,\n",
    "        epochs=60,\n",
    "        units=48,\n",
    "        block_size= autocorrelation_results_ord['CCC']['TWITTER'][col],\n",
    "        n_bootstrap=300\n",
    "    )\n",
    "    results.append({\n",
    "        'Variable': col,\n",
    "        'd_bar_original': d_bar_original,\n",
    "        'p_value': p_value,\n",
    "        'Causal': causal,\n",
    "        'Bootstrap Stats': bootstrap_stats\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_granger_causality(\n",
    "        Y='Wolumen',\n",
    "        X_base='sentiment_divergence',\n",
    "        df=companies_data_daily_final_full[companies_data_daily_final_full['company'] == 'CCC'],\n",
    "        n_lags=1,\n",
    "        test_size=0.2,\n",
    "        block_size= autocorrelation_results_ord['CCC']['TWITTER']['sentiment_divergence'],\n",
    "        n_bootstrap=100\n",
    "    )\n",
    "\n",
    "sns.histplot(res['boostrap_stats'], kde=True)  # kde=True adds smooth curve\n",
    "plt.title(\"Distribution of Data\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adca484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company = 'CCC'\n",
    "# df = companies_data_daily_final_full[companies_data_daily_final_full['company'] == company]\n",
    "# stationary_columns = [k for k,v in stationarity_results[company]['TWITTER'].items() if v]\n",
    "# stationary_columns_STOCK = [k for k,v in stationarity_results[company]['STOCK'].items() if v]\n",
    "# stationary_columns_with_lags = [x for x in df.columns if x.startswith(tuple(stationary_columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import norm, t\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling precomputed lags turned out to be too complex, so we will not use it in the final analysis.\n",
    "# companies_data_daily_final = {}\n",
    "# for company, df in companies_data_daily.items():\n",
    "# #     companies_data_daily_final[company] = create_prediction_lags(df, tweet_col=tweet_columns, max_lags=5)\n",
    "# # companies_data_daily_final_full = pd.DataFrame()\n",
    "# tweet_columns_with_lagged = [x for x in companies_data_daily_final['ALLEGRO'] if x.startswith(tuple(tweet_columns))]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
