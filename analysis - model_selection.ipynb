{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pl_core_news_lg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "from functions import *\n",
    "from evaluation_functions import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import product\n",
    "tool = language_tool_python.LanguageTool('pl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.read_csv(r'TrainingData\\dataset_labeled.csv', nrows=0).columns.tolist()\n",
    "\n",
    "columns_to_exclude = ['Unnamed: 0','Unnamed: 0.1' , 'Unnamed: 0.2']  # example columns to skip\n",
    "wanted_columns = [col for col in all_columns if col not in columns_to_exclude]\n",
    "\n",
    "dataset_labeled = pd.read_csv(r'TrainingData\\dataset_labeled.csv', usecols=wanted_columns)\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e754e58e43834fc78e1f45507e128979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset  = tokenize_datasets(dataset_labeled,'sdadas/polish-roberta-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "ax = count.plot(kind='bar')\n",
    "ax.set_xlabel('Klasy')\n",
    "ax.set_ylabel('Obserwacje')\n",
    "ax.set_xticklabels(['Neutralne', 'Negatywne', 'Pozytywne'])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 77.0\n",
      "Max tokens: 221\n"
     ]
    }
   ],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "The first part was training the model with different basic preprocessing strategies.\n",
    "Then we evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing http\n",
    "def preprocess_tweet_https(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    return tweet\n",
    "\n",
    "#Removing hashtags\n",
    "def preprocess_tweet_hashtag(tweet):\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing mentions\n",
    "def preprocess_tweet_mention(tweet):\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing cashtag\n",
    "def preprocess_tweet_cashtag(tweet):\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing all charatcters except polish letter and ? !\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing repeated letters\n",
    "def preprocess_tweet_rep(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing white spaces\n",
    "def preprocess_tweet_norm(tweet):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "#Normalizing caps\n",
    "def preprocess_caps(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(^|[.!?]\\s+)(\\w)', lambda m: m.group(1) + m.group(2).upper(), tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_tco(tweet):\n",
    "    return re.sub(r\"https?://t\\.co/\\S+\", \"\", tweet).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipelines\n",
    "basic_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_emoji': [\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'No_processing_http': [\n",
    "        preprocess_tweet_https\n",
    "    ],\n",
    "    'No_processing_hashtag': [\n",
    "        preprocess_tweet_hashtag\n",
    "    ],\n",
    "    'No_processing_mention': [\n",
    "        preprocess_tweet_mention\n",
    "    ],\n",
    "    'No_processing_cashtag': [\n",
    "        preprocess_tweet_cashtag\n",
    "    ],\n",
    "    'No_processing__text': [\n",
    "        preprocess_tweet_text\n",
    "    ],\n",
    "    'No_processing__rep': [\n",
    "        preprocess_tweet_rep\n",
    "    ],\n",
    "    'No_processing_norm': [\n",
    "        preprocess_tweet_norm\n",
    "    ],\n",
    "    'No_processing_caps': [\n",
    "        preprocess_caps\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_models = {}\n",
    "tested_models['ROBERT'] = \"sdadas/polish-roberta-base-v2\"\n",
    "tested_models['HERBERT']  = \"allegro/herbert-base-cased\"\n",
    "tested_models['POLBERT']  = \"dkleczek/bert-base-polish-cased-v1\"\n",
    "tested_models['MBERT'] = 'google-bert/bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = {}\n",
    "strat['No_processing'] = basic_processing_pipelines['No_processing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData/processed_data_No_processing.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4199f0626b4eb8b0770a4031466401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = transform_data(processing_pipelines = strat,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "tokenize_datasets = tokenize_datasets(datasets['No_processing'],model_name='allegro/herbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'allegro/herbert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best basic data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(r'./preprocessing_comparison') or os.makedirs(r'./preprocessing_comparison')\n",
    "os.path.exists(r'./results') or os.makedirs(r'./results')\n",
    "\n",
    "datasets = transform_data(processing_pipelines = basic_processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "\n",
    "params={'train_seed': [12], 'model_seed': [12,53,42]}\n",
    "all_params = [dict(zip(params.keys(), values)) \n",
    "             for values in itertools.product(*params.values())]\n",
    "\n",
    "all_combinations = list(product(all_params, tested_models.values(), datasets.keys()))\n",
    "\n",
    "for params_set, model, dataset_name in tqdm.tqdm(all_combinations, desc=\"Grid Search\", total=len(all_combinations)):\n",
    "    try:\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=datasets[dataset_name], params = params_set, model_name=model, strategy_name=dataset_name)\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%''H%M%S')\n",
    "        results_df.to_csv(rf\"./preprocessing_comparison/preprocessing_comparison_model_{model[:4]}_{params_set['train_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best data preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the preprocessing steps\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = preprocess_tweet_mention(tweet)\n",
    "    tweet = preprocess_tweet_https(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_processing_pipelines = {\n",
    "    'Basic_processing': [\n",
    "        preprocess_tweet\n",
    "    ],\n",
    "    'Basic_processing_spelling': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'Basic_processing_spelling_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        remove_stops\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = transform_data(processing_pipelines = strategies_processing_pipelines, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "os.path.exists(r'./strategy_comparison') or os.makedirs(r'./strategy_comparison')\n",
    "\n",
    "datasets = transform_data(processing_pipelines = strategies_processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "\n",
    "params={'train_seed': [12], 'model_seed': [12,53,42]}\n",
    "all_params = [dict(zip(params.keys(), values)) \n",
    "             for values in itertools.product(*params.values())]\n",
    "\n",
    "all_combinations = list(product(all_params, tested_models.values(), datasets.keys()))\n",
    "\n",
    "for params_set, model, dataset_name in tqdm.tqdm(all_combinations, desc=\"Grid Search\", total=len(all_combinations)):\n",
    "    try:\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=datasets[dataset_name], params = params_set, model_name=model, strategy_name=dataset_name)\n",
    "        # Save results  \n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%' \\\n",
    "        'H%M%S')\n",
    "        results_df.to_csv(rf\"./strategy_comparison/preprocessing_comparison_model_{model[:4]}_{params['train_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with best preprocessing strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipelines = {}\n",
    "processing_pipelines['Basic_processing'] = strategies_processing_pipelines['Basic_processing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 150\n",
    "dataset = transform_data(processing_pipelines = processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset_name = list(dataset.keys())[0]\n",
    "os.path.exists(r'./grid_search_comparison') or os.makedirs(r'./grid_search_comparison')\n",
    "\n",
    "model = tested_models['ROBERT']\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5,1.5e-5, 2e-5],     \n",
    "    \"batch_size\": [4,8, 16],               \n",
    "    \"epochs\": [3,4,5],                    \n",
    "    \"weight_decay\": [0.05,0.01, 0.001],       \n",
    "    \"class_weight_floor\": [0.5, 1.0],\n",
    "    \"train_seed\": [42],  \n",
    "    \"model_seed\": [42, 53, 12]\n",
    "}\n",
    "all_params = [dict(zip(param_grid.keys(), values)) \n",
    "             for values in itertools.product(*param_grid.values())]\n",
    "\n",
    "sampled_params = random.sample(all_params, k=min(N_SAMPLES, len(all_params)))\n",
    "\n",
    "for params_set in tqdm.tqdm(sampled_params, desc=\"Grid Search\", total=len(sampled_params)):\n",
    "    try:\n",
    "        \n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=dataset['Basic_processing'], params = params_set, model_name=model)\n",
    "        # Save results  \n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        results_df.to_csv(rf\"./grid_search_comparison/preprocessing_comparison_model_{model[:4]}_{params_set['model_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData/processed_data_Basic_processing.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228dfea7241248d692a8e97ba2fe1a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345005f444b045b8b85411e5f029ae99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Basic_processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/polish-roberta-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='808' max='808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [808/808 30:07, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.894115</td>\n",
       "      <td>0.659259</td>\n",
       "      <td>0.596269</td>\n",
       "      <td>0.657580</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.737991</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.783200</td>\n",
       "      <td>0.861295</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.667966</td>\n",
       "      <td>0.720573</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.798337</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.894779</td>\n",
       "      <td>0.735802</td>\n",
       "      <td>0.694064</td>\n",
       "      <td>0.737606</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.597222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params= {\n",
    "    \"learning_rate\": 1.5e-5,     \n",
    "    \"batch_size\": 4,               \n",
    "    \"epochs\": 4,                    \n",
    "    \"weight_decay\": 0.01,       \n",
    "    \"class_weight_floor\": 1,\n",
    "    \"train_seed\": 42,  \n",
    "    \"model_seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = 'Basic_processing'\n",
    "model_name = 'sdadas/polish-roberta-base-v2'\n",
    "dataset = transform_data(processing_pipelines = {'Basic_processing': strategies_processing_pipelines['Basic_processing']}, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset = dataset['Basic_processing']\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=params[\"train_seed\"], stratify=dataset['labels'])\n",
    "train_df_tokenized = tokenize_datasets(df = train_df, model_name=model_name, max_length=128, column='text')\n",
    "val_df_tokenized = tokenize_datasets(df = test_df, model_name=model_name, max_length=128, column='text')\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_df_tokenized,\n",
    "        'test': val_df_tokenized\n",
    "    })\n",
    "try:\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    trainer, eval = train_evaluate(\n",
    "        params=params, \n",
    "        tokenized_dataset=dataset_dict,\n",
    "        model_name=model_name, \n",
    "    \n",
    "    )\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Skipping failed params {params}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_evaluate(params, tokenized_dataset, model_name):\n",
    "    # Get and validate labels\n",
    "    train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
    "    \n",
    "    # Class weight calculation with floor\n",
    "    class_weights = get_class_weights(train_labels, floor=params.get(\"class_weight_floor\", 1.0))\n",
    "\n",
    "    # Model initialization\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=3\n",
    "    )\n",
    "   \n",
    "    # Training arguments with imbalance optimizations\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./trained/{hash(str(params))}\",\n",
    "        num_train_epochs=params[\"epochs\"],\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        warmup_ratio=0.1,  \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1_weighted\",\n",
    "        greater_is_better=True,\n",
    "        seed=params['model_seed'],\n",
    "        optim=\"adamw_torch\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=params.get(\"grad_accum_steps\", 2),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "         \n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[MasterCSVLoggerCallback(run_id=f'{model_name}_{\"_\".join(map(str, params.values()))}')], \n",
    "    )\n",
    "\n",
    "    # Train with validation checks\n",
    "    try:\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return trainer, eval_results\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    "    \"learning_rate\": 1.5e-5,     \n",
    "    \"batch_size\": 4,               \n",
    "    \"epochs\": 5,                    \n",
    "    \"weight_decay\": 0.01,       \n",
    "    \"class_weight_floor\": 1,\n",
    "    \"train_seed\": 42,  \n",
    "    \"model_seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = 'Basic_processing'\n",
    "model_name = 'sdadas/polish-roberta-base-v2'\n",
    "dataset = transform_data(processing_pipelines = {'Basic_processing': strategies_processing_pipelines['Basic_processing']}, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset = dataset['Basic_processing']\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=params[\"train_seed\"], stratify=dataset['labels'])\n",
    "train_df_tokenized = tokenize_datasets(df = train_df, model_name=model_name, max_length=128, column='text')\n",
    "val_df_tokenized = tokenize_datasets(df = test_df, model_name=model_name, max_length=128, column='text')\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_df_tokenized,\n",
    "        'test': val_df_tokenized\n",
    "    })\n",
    "try:\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    trainer, eval = train_evaluate(\n",
    "        params=params, \n",
    "        tokenized_dataset=dataset_dict,\n",
    "        model_name=model_name, \n",
    "    \n",
    "    )\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Skipping failed params {params}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    "    \"learning_rate\": 1.5e-5,     \n",
    "    \"batch_size\": 4,               \n",
    "    \"epochs\": 5,                    \n",
    "    \"weight_decay\": 0.01,       \n",
    "    \"class_weight_floor\": 1.5,\n",
    "    \"train_seed\": 42,  \n",
    "    \"model_seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = 'Basic_processing'\n",
    "model_name = 'sdadas/polish-roberta-base-v2'\n",
    "dataset = transform_data(processing_pipelines = {'Basic_processing': strategies_processing_pipelines['Basic_processing']}, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset = dataset['Basic_processing']\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=params[\"train_seed\"], stratify=dataset['labels'])\n",
    "train_df_tokenized = tokenize_datasets(df = train_df, model_name=model_name, max_length=128, column='text')\n",
    "val_df_tokenized = tokenize_datasets(df = test_df, model_name=model_name, max_length=128, column='text')\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_df_tokenized,\n",
    "        'test': val_df_tokenized\n",
    "    })\n",
    "try:\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    trainer, eval = train_evaluate(\n",
    "        params=params, \n",
    "        tokenized_dataset=dataset_dict,\n",
    "        model_name=model_name, \n",
    "    \n",
    "    )\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Skipping failed params {params}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\n",
    "    \"learning_rate\": 1.5e-5,     \n",
    "    \"batch_size\": 4,               \n",
    "    \"epochs\": 4,                    \n",
    "    \"weight_decay\": 0.01,       \n",
    "    \"class_weight_floor\": 1.5,\n",
    "    \"train_seed\": 42,  \n",
    "    \"model_seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = 'Basic_processing'\n",
    "model_name = 'sdadas/polish-roberta-base-v2'\n",
    "dataset = transform_data(processing_pipelines = {'Basic_processing': strategies_processing_pipelines['Basic_processing']}, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset = dataset['Basic_processing']\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=params[\"train_seed\"], stratify=dataset['labels'])\n",
    "train_df_tokenized = tokenize_datasets(df = train_df, model_name=model_name, max_length=128, column='text')\n",
    "val_df_tokenized = tokenize_datasets(df = test_df, model_name=model_name, max_length=128, column='text')\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "        'train': train_df_tokenized,\n",
    "        'test': val_df_tokenized\n",
    "    })\n",
    "try:\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    trainer, eval = train_evaluate(\n",
    "        params=params, \n",
    "        tokenized_dataset=dataset_dict,\n",
    "        model_name=model_name, \n",
    "    \n",
    "    )\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Skipping failed params {params}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
