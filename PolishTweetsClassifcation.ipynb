{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import spacy\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction\n",
    ")\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from functions import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "tool = language_tool_python.LanguageTool('pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv('TrainingData/annotation_dataset - general_label.csv',index_col=0)\n",
    "dataset_2 = pd.read_csv('TrainingData/annotation_dataset - annotation_second_round_labeled.csv',index_col=0)\n",
    "\n",
    "dataset_1 = dataset_1[['text','Overall']]\n",
    "dataset_1 =  dataset_1.rename(columns={'Overall':'labels'})\n",
    "\n",
    "dataset_2 = dataset_2[['text','label']]\n",
    "dataset_2 =  dataset_2.rename(columns={'label':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled = pd.concat([dataset_1,dataset_2],axis=0)\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='labels'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 86.84999999999991\n",
      "Max tokens: 358\n"
     ]
    }
   ],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "The first part was training the model with different basic preprocessing strategies.\n",
    "Then we evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_models = {}\n",
    "tested_models['ROBERT'] = \"sdadas/polish-roberta-base-v2\"\n",
    "tested_models['HERBERT']  = \"allegro/herbert-base-cased\"\n",
    "tested_models['POLBERT']  = \"dkleczek/bert-base-polish-cased-v1\"\n",
    "tested_models['MBERT'] = 'google-bert/bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub(r'@\\w+|#\\w+', ' ', tweet)\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    # Replace underscores with space\n",
    "    tweet = re.sub(r'_', ' ', tweet)\n",
    "    # Changed \\w to use explicit character ranges instead\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing http\n",
    "def preprocess_tweet_https(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    return tweet\n",
    "\n",
    "#Removing hashtags\n",
    "def preprocess_tweet_hashtag(tweet):\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing mentions\n",
    "def preprocess_tweet_mention(tweet):\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing cashtag\n",
    "def preprocess_tweet_cashtag(tweet):\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing all charatcters except polish letter and ? !\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing repeated letters\n",
    "def preprocess_tweet_rep(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing white spaces\n",
    "def preprocess_tweet_norm(tweet):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "#Normalizing caps\n",
    "def preprocess_caps(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(^|[.!?]\\s+)(\\w)', lambda m: m.group(1) + m.group(2).upper(), tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_tco(tweet):\n",
    "    return re.sub(r\"https?://t\\.co/\\S+\", \"\", tweet).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(df, transformations, name):\n",
    "    \"\"\"Apply a sequence of transformations to the dataframe's text column\"\"\"\n",
    "    processed_df = reduce(\n",
    "        lambda acc, func: acc.assign(text=acc['text'].apply(func)).drop_duplicates(subset='text'),\n",
    "        transformations,\n",
    "        copy.deepcopy(df)\n",
    "    )\n",
    "    processed_df.to_csv(fr'TrainingData/processed_data_{name}.csv')\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipelines\n",
    "strategies_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_emoji': [\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'No_processing_spelling': [\n",
    "        tool.correct\n",
    "    ],\n",
    "    'No_processing_spelling_lem': [\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'No_processing_spelling_lem_sp': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ],\n",
    "    'Basic_processing': [\n",
    "        preprocess_tweet\n",
    "    ],\n",
    "    'Basic_processing_emoji': [\n",
    "        preprocess_tweet,\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'Basic_processing_spelling': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ]\n",
    "}\n",
    "\n",
    "basic_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_http': [\n",
    "        preprocess_tweet_https\n",
    "    ],\n",
    "    'No_processing_hashtag': [\n",
    "        preprocess_tweet_hashtag\n",
    "    ],\n",
    "    'No_processing_mention': [\n",
    "        preprocess_tweet_mention\n",
    "    ],\n",
    "    'No_processing_cashtag': [\n",
    "        preprocess_tweet_cashtag\n",
    "    ],\n",
    "    'No_processing__text': [\n",
    "        preprocess_tweet_text\n",
    "    ],\n",
    "    'No_processing__rep': [\n",
    "        preprocess_tweet_rep\n",
    "    ],\n",
    "    'No_processing_norm': [\n",
    "        preprocess_tweet_norm\n",
    "    ],\n",
    "    'No_processing_caps': [\n",
    "        preprocess_caps\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all variants\n",
    "datasets_basic = {\n",
    "    name: apply_transformations(dataset_labeled, pipeline,name = name)\n",
    "    for name, pipeline in basic_processing_pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_strategies = {\n",
    "#    name: apply_transformations(dataset_labeled, pipeline, name = name)\n",
    "#    for name, pipeline in strategies_processing_pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_basic\n",
    "model = tested_models['HERBERT']\n",
    "train_test_seed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 9301.88 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 3972.61 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2082/2082 [00:00<00:00, 14017.62 examples/s]\n",
      "Map: 100%|██████████| 893/893 [00:00<00:00, 14175.20 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 14127.77 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14563.89 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2087/2087 [00:00<00:00, 13958.18 examples/s]\n",
      "Map: 100%|██████████| 895/895 [00:00<00:00, 14436.20 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13644.28 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 13886.83 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13599.39 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14451.33 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13886.90 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14430.30 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13916.47 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14332.01 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13871.81 examples/s]\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14332.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = {}\n",
    "for key, df in datasets.items():\n",
    "    # First split the data before balancing\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.remove_columns('__index_level_0__')\n",
    "    train_test = dataset.train_test_split(test_size=0.3, seed=train_test_seed)\n",
    "    \n",
    "    # Convert to pandas to balance only training data\n",
    "    train_df = train_test['train'].to_pandas()\n",
    "    test_df = train_test['test'].to_pandas()\n",
    "    \n",
    "    # Balance only training data\n",
    "    #balanced_train_df = balance_text_data(train_df)\n",
    "    #balanced_train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    # Tokenize both datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    \n",
    "    # Tokenize training data\n",
    "    tokenized_train = train_dataset.map(\n",
    "        lambda examples: tokenizer(examples[\"text\"], \n",
    "                                 truncation=True, \n",
    "                                 padding=\"max_length\",\n",
    "                                 max_length=128),\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Tokenize test data\n",
    "    tokenized_test = test_dataset.map(\n",
    "        lambda examples: tokenizer(examples[\"text\"], \n",
    "                                 truncation=True, \n",
    "                                 padding=\"max_length\",\n",
    "                                 max_length=128),\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\", \n",
    "                              columns=[\"input_ids\", \n",
    "                                      \"attention_mask\", \n",
    "                                      \"labels\"])\n",
    "    tokenized_test.set_format(\"torch\", \n",
    "                             columns=[\"input_ids\", \n",
    "                                     \"attention_mask\", \n",
    "                                     \"labels\"])\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    tokenized_datasets[key] = DatasetDict({\n",
    "        'train': tokenized_train,\n",
    "        'test': tokenized_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(key, tokenized_dataset, base_model_name,train_test_seed):\n",
    "\n",
    "    # Reload model for each run to reset weights\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=3)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{key}_{base_model_name}_{train_test_seed}\",\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=4,  \n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\", \n",
    "        greater_is_better=True,\n",
    "        seed=45,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=25,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_grad_norm=15.0,\n",
    "        save_total_limit=2,  # Keep only last 2 checkpoints\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    # Initialize custom trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "  2%|▏         | 25/1044 [00:46<24:09,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0199, 'grad_norm': 6.282765865325928, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 50/1044 [01:22<24:47,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0029, 'grad_norm': 7.6540350914001465, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 75/1044 [02:00<24:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9635, 'grad_norm': 5.721133708953857, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 100/1044 [02:37<23:05,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9043, 'grad_norm': 6.687543869018555, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 125/1044 [03:15<23:21,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9131, 'grad_norm': 4.711859226226807, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 150/1044 [03:52<21:13,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9506, 'grad_norm': 9.803505897521973, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 175/1044 [04:26<20:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9048, 'grad_norm': 5.755303382873535, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 200/1044 [05:01<20:08,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8514, 'grad_norm': 6.7344183921813965, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 225/1044 [05:37<20:20,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8162, 'grad_norm': 8.015406608581543, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 250/1044 [06:11<18:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7287, 'grad_norm': 9.721258163452148, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 261/1044 [06:27<18:14,  1.40s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "                                                  \n",
      " 25%|██▌       | 261/1044 [07:00<18:14,  1.40s/it]Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7359331846237183, 'eval_accuracy': 0.6964285714285714, 'eval_f1_macro': 0.620968768267722, 'eval_f1_weighted': 0.6868305955234931, 'eval_f1_0': 0.5565749235474006, 'eval_f1_1': 0.7880386983289358, 'eval_f1_2': 0.5182926829268293, 'eval_runtime': 33.5908, 'eval_samples_per_second': 26.674, 'eval_steps_per_second': 0.834, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-261\\model.safetensors\n",
      " 26%|██▋       | 275/1044 [07:31<19:45,  1.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6792, 'grad_norm': 23.60762596130371, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 300/1044 [08:07<17:12,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.637, 'grad_norm': 15.861551284790039, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 325/1044 [08:42<16:53,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7014, 'grad_norm': 16.926652908325195, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 350/1044 [09:17<16:08,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6352, 'grad_norm': 19.106857299804688, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 375/1044 [09:52<15:47,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6328, 'grad_norm': 19.566364288330078, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 400/1044 [10:27<15:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7092, 'grad_norm': 14.072370529174805, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 425/1044 [11:03<14:27,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6991, 'grad_norm': 10.693466186523438, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 450/1044 [11:38<13:44,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6101, 'grad_norm': 17.276012420654297, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 475/1044 [12:16<14:58,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6093, 'grad_norm': 14.378206253051758, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 500/1044 [12:53<14:20,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6457, 'grad_norm': 20.051076889038086, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 522/1044 [13:26<12:28,  1.43s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "                                                  \n",
      " 50%|█████     | 522/1044 [14:01<12:28,  1.43s/it]Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6468263864517212, 'eval_accuracy': 0.7299107142857143, 'eval_f1_macro': 0.6690888397338918, 'eval_f1_weighted': 0.7235204337644959, 'eval_f1_0': 0.5909090909090909, 'eval_f1_1': 0.808206958073149, 'eval_f1_2': 0.6081504702194357, 'eval_runtime': 35.0895, 'eval_samples_per_second': 25.535, 'eval_steps_per_second': 0.798, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-522\\model.safetensors\n",
      " 50%|█████     | 525/1044 [14:17<1:11:41,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5596, 'grad_norm': 12.767314910888672, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 550/1044 [14:52<11:56,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3971, 'grad_norm': 22.254589080810547, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 575/1044 [15:30<12:20,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4523, 'grad_norm': 19.56853485107422, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 600/1044 [16:06<10:25,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4649, 'grad_norm': 21.884523391723633, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 625/1044 [16:43<11:17,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3434, 'grad_norm': 42.929725646972656, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 650/1044 [17:22<10:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.296, 'grad_norm': 7.410569667816162, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 675/1044 [18:01<10:28,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3725, 'grad_norm': 15.799896240234375, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 700/1044 [18:41<09:14,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4767, 'grad_norm': 17.63815689086914, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 725/1044 [19:18<07:51,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2924, 'grad_norm': 17.492277145385742, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 750/1044 [19:54<07:04,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3045, 'grad_norm': 27.873435974121094, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 775/1044 [20:30<06:25,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3387, 'grad_norm': 3.084507942199707, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 783/1044 [20:42<06:23,  1.47s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "                                                  \n",
      " 75%|███████▌  | 783/1044 [21:17<06:23,  1.47s/it]Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7467493414878845, 'eval_accuracy': 0.7310267857142857, 'eval_f1_macro': 0.6913156922508543, 'eval_f1_weighted': 0.7332893745061568, 'eval_f1_0': 0.6433915211970075, 'eval_f1_1': 0.7980769230769231, 'eval_f1_2': 0.6324786324786325, 'eval_runtime': 34.7624, 'eval_samples_per_second': 25.775, 'eval_steps_per_second': 0.805, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-261] due to args.save_total_limit\n",
      " 77%|███████▋  | 800/1044 [21:51<06:06,  1.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.248, 'grad_norm': 24.956268310546875, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 825/1044 [22:27<05:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2503, 'grad_norm': 11.114324569702148, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 850/1044 [23:03<04:36,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1927, 'grad_norm': 42.22187805175781, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 875/1044 [23:40<04:02,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.304, 'grad_norm': 23.874753952026367, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 900/1044 [24:16<03:27,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1713, 'grad_norm': 19.512584686279297, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 925/1044 [24:53<02:54,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2167, 'grad_norm': 27.499536514282227, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 950/1044 [25:28<02:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1928, 'grad_norm': 9.684943199157715, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 975/1044 [26:03<01:34,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1701, 'grad_norm': 4.503691673278809, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1000/1044 [26:38<01:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2185, 'grad_norm': 32.55777359008789, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1025/1044 [27:12<00:26,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2254, 'grad_norm': 32.448944091796875, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [27:38<00:00,  1.37s/it]Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "                                                   \n",
      "100%|██████████| 1044/1044 [28:25<00:00,  1.37s/it]Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8151745200157166, 'eval_accuracy': 0.7310267857142857, 'eval_f1_macro': 0.6933679405088733, 'eval_f1_weighted': 0.7343442820653088, 'eval_f1_0': 0.6448362720403022, 'eval_f1_1': 0.7976653696498055, 'eval_f1_2': 0.6376021798365122, 'eval_runtime': 35.7704, 'eval_samples_per_second': 25.049, 'eval_steps_per_second': 0.783, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_allegro/herbert-base-cased_20\\checkpoint-1044 (score: 0.6933679405088733).\n",
      "100%|██████████| 1044/1044 [28:39<00:00,  1.65s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1719.35, 'train_samples_per_second': 4.858, 'train_steps_per_second': 0.607, 'train_loss': 0.5320252725904472, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:34<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7310267857142857, 'macro_f1': 0.6933679405088733, 'weighted_f1': 0.7343442820653088, 'neutral_f1': 0.6448362720403022, 'positive_f1': 0.7976653696498055, 'negative_f1': 0.6376021798365122, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_http\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_test_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:model,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m })\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[1;32mIn[17], line 40\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(key, tokenized_dataset, base_model_name, train_test_seed)\u001b[0m\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2062\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_training_loop\u001b[39m(\n\u001b[0;32m   2060\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys_for_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2061\u001b[0m ):\n\u001b[1;32m-> 2062\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfree_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m   2064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mauto_find_batch_size:\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:3240\u001b[0m, in \u001b[0;36mAccelerator.free_memory\u001b[1;34m(self, *objects)\u001b[0m\n\u001b[0;32m   3238\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mdestroy()\n\u001b[0;32m   3239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3240\u001b[0m objects \u001b[38;5;241m=\u001b[39m \u001b[43mrelease_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedulers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\accelerate\\utils\\memory.py:84\u001b[0m, in \u001b[0;36mrelease_memory\u001b[1;34m(*objects)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(objects)):\n\u001b[0;32m     83\u001b[0m     objects[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgarbage_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\accelerate\\utils\\memory.py:42\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[1;34m(garbage_collection)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mClears the device cache by calling `torch.{backend}.empty_cache`. Can also run `gc.collect()`, but do note that\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03mthis is a *considerable* slowdown and should be used sparingly.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m garbage_collection:\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_xpu_available():\n\u001b[0;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main comparison loop\n",
    "results = []\n",
    "\n",
    "for key, tokenized_dataset in tokenized_datasets.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing preprocessing variant: {key}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f'Training model: {model}')\n",
    "        metrics = train_and_evaluate(key, tokenized_dataset,model,train_test_seed)\n",
    "        \n",
    "        results.append({\n",
    "            \"model\":model,\n",
    "            \"preprocessing\": key,\n",
    "            \"accuracy\": metrics[\"eval_accuracy\"],\n",
    "            \"macro_f1\": metrics[\"eval_f1_macro\"],\n",
    "            \"weighted_f1\": metrics[\"eval_f1_weighted\"],\n",
    "            \"neutral_f1\": metrics[\"eval_f1_0\"],\n",
    "            \"positive_f1\": metrics[\"eval_f1_1\"],\n",
    "            \"negative_f1\": metrics[\"eval_f1_2\"],\n",
    "            \"epochs\": metrics[\"epoch\"]\n",
    "        })\n",
    "\n",
    "        print(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with variant {key}: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(rf\"preprocessing_comparison_general_label_preprocessing_{model}.csv\"), exist_ok=True)\n",
    "\n",
    "# Now save the CSV\n",
    "results_df.to_csv(rf\"preprocessing_comparison_general_label_preprocessing_{model}.csv\", index=False)\n",
    "print(\"\\nComparison saved to preprocessing_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"preprocessing_comparison_general_label_sdas_parts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing</td>\n",
       "      <td>0.727679</td>\n",
       "      <td>0.684198</td>\n",
       "      <td>0.731442</td>\n",
       "      <td>0.678851</td>\n",
       "      <td>0.800770</td>\n",
       "      <td>0.572973</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_http</td>\n",
       "      <td>0.693169</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.696373</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>0.757482</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>3.992322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_hashtag</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.673528</td>\n",
       "      <td>0.723492</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_mention</td>\n",
       "      <td>0.716201</td>\n",
       "      <td>0.666518</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.789229</td>\n",
       "      <td>0.596923</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_cashtag</td>\n",
       "      <td>0.738839</td>\n",
       "      <td>0.690386</td>\n",
       "      <td>0.737947</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.807116</td>\n",
       "      <td>0.567073</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing__text</td>\n",
       "      <td>0.716518</td>\n",
       "      <td>0.658608</td>\n",
       "      <td>0.712187</td>\n",
       "      <td>0.657963</td>\n",
       "      <td>0.790528</td>\n",
       "      <td>0.527331</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing__rep</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.674425</td>\n",
       "      <td>0.720917</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.789272</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_norm</td>\n",
       "      <td>0.722098</td>\n",
       "      <td>0.685312</td>\n",
       "      <td>0.726706</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>0.601093</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model          preprocessing  accuracy  macro_f1  \\\n",
       "0  allegro/herbert-base-cased          No_processing  0.727679  0.684198   \n",
       "1  allegro/herbert-base-cased     No_processing_http  0.693169  0.663911   \n",
       "2  allegro/herbert-base-cased  No_processing_hashtag  0.726562  0.673528   \n",
       "3  allegro/herbert-base-cased  No_processing_mention  0.716201  0.666518   \n",
       "4  allegro/herbert-base-cased  No_processing_cashtag  0.738839  0.690386   \n",
       "5  allegro/herbert-base-cased    No_processing__text  0.716518  0.658608   \n",
       "6  allegro/herbert-base-cased     No_processing__rep  0.718750  0.674425   \n",
       "7  allegro/herbert-base-cased     No_processing_norm  0.722098  0.685312   \n",
       "\n",
       "   weighted_f1  neutral_f1  positive_f1  negative_f1    epochs  \n",
       "0     0.731442    0.678851     0.800770     0.572973  4.000000  \n",
       "1     0.696373    0.631841     0.757482     0.602410  3.992322  \n",
       "2     0.723492    0.677083     0.796330     0.547170  4.000000  \n",
       "3     0.713497    0.613402     0.789229     0.596923  4.000000  \n",
       "4     0.737947    0.696970     0.807116     0.567073  4.000000  \n",
       "5     0.712187    0.657963     0.790528     0.527331  4.000000  \n",
       "6     0.720917    0.666667     0.789272     0.567335  4.000000  \n",
       "7     0.726706    0.666667     0.788177     0.601093  4.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search using best preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['No_processing', 'No_processing_http', 'No_processing_hashtag', 'No_processing_mention', 'No_processing_cashtag', 'No_processing__text', 'No_processing__rep', 'No_processing_norm', 'No_processing_caps'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results_robust.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file E:\\Project_clean\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-1044\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file E:\\Project_clean\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-1044\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at E:\\Project_clean\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-1044.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:50<00:00,  2.21it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.68      0.64       189\n",
      "     Neutral       0.83      0.77      0.80       533\n",
      "    Positive       0.61      0.67      0.64       174\n",
      "\n",
      "    accuracy                           0.73       896\n",
      "   macro avg       0.68      0.71      0.69       896\n",
      "weighted avg       0.74      0.73      0.73       896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = r'E:\\Project_clean\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased_20\\checkpoint-1044'\n",
    "\n",
    "tokenized_dataset = tokenized_datasets['No_processing']\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Create and display confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:35<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions with probabilities:\n",
      "Example 0:\n",
      "Negative prob: 0.0585\n",
      "Neutral prob: 0.9265\n",
      "Positive prob: 0.0149\n",
      "Predicted class: 1\n",
      "\n",
      "Example 1:\n",
      "Negative prob: 0.0070\n",
      "Neutral prob: 0.9882\n",
      "Positive prob: 0.0047\n",
      "Predicted class: 1\n",
      "\n",
      "Example 2:\n",
      "Negative prob: 0.6083\n",
      "Neutral prob: 0.3718\n",
      "Positive prob: 0.0200\n",
      "Predicted class: 0\n",
      "\n",
      "Example 3:\n",
      "Negative prob: 0.0054\n",
      "Neutral prob: 0.9887\n",
      "Positive prob: 0.0059\n",
      "Predicted class: 1\n",
      "\n",
      "Example 4:\n",
      "Negative prob: 0.9544\n",
      "Neutral prob: 0.0265\n",
      "Positive prob: 0.0191\n",
      "Predicted class: 0\n",
      "\n",
      "Average confidence score: 0.9236\n",
      "\n",
      "Number of uncertain predictions: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions (these are logits)\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "raw_logits = predictions.predictions\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "softmax_probs = F.softmax(torch.tensor(raw_logits), dim=1).numpy()\n",
    "\n",
    "# Now you can:\n",
    "# 1. Print probabilities for individual predictions\n",
    "print(\"First 5 predictions with probabilities:\")\n",
    "for i in range(5):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Negative prob: {softmax_probs[i][0]:.4f}\")\n",
    "    print(f\"Neutral prob: {softmax_probs[i][1]:.4f}\")\n",
    "    print(f\"Positive prob: {softmax_probs[i][2]:.4f}\")\n",
    "    print(f\"Predicted class: {np.argmax(softmax_probs[i])}\")\n",
    "    print()\n",
    "\n",
    "# 2. Get average confidence scores\n",
    "avg_confidence = np.mean(np.max(softmax_probs, axis=1))\n",
    "print(f\"Average confidence score: {avg_confidence:.4f}\")\n",
    "\n",
    "# 3. Find highly uncertain predictions (where max probability is below a threshold)\n",
    "uncertainty_threshold = 0.7\n",
    "uncertain_preds = np.where(np.max(softmax_probs, axis=1) < uncertainty_threshold)[0]\n",
    "print(f\"\\nNumber of uncertain predictions: {len(uncertain_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot distribution of maximum probabilities\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(np\u001b[38;5;241m.\u001b[39mmax(\u001b[43msoftmax_probs\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of Prediction Confidence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaximum Probability\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'softmax_probs' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of maximum probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.max(softmax_probs, axis=1), bins=50)\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:34<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Highly confident misclassifications:\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MarcinBanas3 @Felipe___666 @AdamAdam4949 Dla tego Allegro mogło dostać po dupie, a Orlenu nie wolno dotknąć. Bo pięknie rżnie takich jak ja. Niemiec 5,40, ja 6,78/l. By udziałowcy byli zadowoleni, szczególnie większościowi. To jest zorganizowana mafia. https://t.co/BTNYu8gwh4\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9161\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Suilerua_ Pisałem, żeby trzymać to ccc\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9885\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Hello InPost @InPostPL od rana czekam na przesyłkę a tu jeszcze lepszy zong teraz się pokazał... Gratuluję organizacji pracy.... https://t.co/47mtmV00IC\n",
      "True label: Negative\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9675\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @m_gdula @__Lewica Pan w ogóle nie rozumie na czym polega problem. CI którzy nie płacą i tak nie zapłacą a obciąży Pan dodatkowo taki Inpost. Ma Pan chociaż kawałem mózgu ? Bo ta propozycja sugeruje, że nie bardzo.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9495\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MakrelG Kurwa ludzie sprzedają CDR bo jakaś gówna gra , wogole niekojarzonej firmy globalnie - nie wypaliła. Takie rzeczy tylko na Bananie xD jedna z najlepszych firm na świecie traci 5% czyli ponad połowę kapitalizacji 11bit bo tamtym gra nie wypaliła. Ludzie są debilami z pieluchami\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9745\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @RoyalNavvy inpost&gt;&gt;&gt; wszystko inne\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9871\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Jeszcze w tym roku znana platforma zakupowa ma uruchomić czeskojęzyczny serwis https://t.co/exyDHXXqUf. Marcin Półchłopek z Allegro opowiada Business Insiderowi, jakie firma ma plany – i co z tego wynika dla polskiego użytkownika. https://t.co/Op4cVmx3rY\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9532\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Wilk_z_GPW a jak tu okazja na xtb? to tylko moze spadac w dol\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9722\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Konsolowe wydania Sonic Superstars taniej na Allegro. Ceny zaczynają się od 176,66 zł https://t.co/bqtItp5VVk\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9664\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: #CCC Ultro obejmie w ofercie conajmniej 5 mln akcji, co daje utrzymanie proporcji w głosach i efektywnie objęcie akcji za conajmniej 194 mln PLN. Czyli dla innych inwestorów zostaje mniej\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9523\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @amthreehu ale w inpost tez nie trzeba drukować :( ludzie to chujki\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9083\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @bocianovvsky Jak nie trzymasz w pudełkach to spoko są te gablotki z Allegro. Ja narazie trzymam w pudełkach. https://t.co/pNVO8fGAiJ\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9146\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Dostałem dzisiaj buty z CCC. Zastanawiam się czy oni nie pomylili adresu. Może miały być dostarczone na Nowogejowską ? Od lat kupuję przez Internet, czegoś takiego jeszcze nie miałem. 😡🤬😡🤬 https://t.co/DKQuPEqQGq\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9390\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @InPostPL prosze inpost tak bardzo Was lubie\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9210\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @bednarow_t @MarcinBrixen Ja słyszałem, że te wszystkie paczki inpostu to Kaczyński i Morawiecki roznoszą nocami i to dzięki nim inpost osiągnął taki sukces\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9061\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Mysterybuzu Nie dość, że luksemburscy właściciele Heket Topco S.à r.l. oraz PG Investment Company 1113B S.à r.l. zachowali kontrolę nad żabką to jeszcze zgarnęli z polskiego rynku ponad 7 miliardów zł. https://t.co/DlrGDUhu3W\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9244\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @InPostPL @JOlkiewicz widziales, najpierw STS teraz inpost ;(\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9445\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @buniabuba Jeśli ktoś chce się tak pobawić, to polecam kupno zestawu do szycia chirurgicznego na allegro (igła, nici, pęsetą, te \"dzikie\" nożyczki) oraz świńskiej lub drobiowej nogi, ewentualnie kurzy cycek\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9851\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @moraw_m Kupuję materac za 700 zł u producenta ..przesyłka 300 zł !!!. Więc kupuje ten sam materac u producenta, na Allegro...przesyłka gratis... i jak ich wspierać?\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9787\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Jandor4119 @ewale21 Dla pocieszenia UNITRA została reaktywowaną inwestor osoba CD Projekt będą polskie wierzę dla audiofili\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9134\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @marcinwarchol Warchoł, robisz z siebie mało rozgarniętą osobę, a może nią jesteś? Doktorat zakupiłeś na allegro.lokalnie?\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9763\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @doktor_90_fx @ZofiaGambetti 11% straty to żaden wstyd. Zdarza się. A grillowac nie zamierzam, nie mój styl, czasami się wygłupiam i tyle. Twoje życie, twoje decyzje. A CDR i tak odjeżdża ;))))\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9440\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Dzisiaj na otwarciu sprzedane #GRX z zyskiem 37% co pokryło stratę z 11BIT A do prywatnego portfela za GRX trafiło #SNT po 171zł 😉 https://t.co/Ie68ZrEnj9\n",
      "True label: Negative\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9806\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @ewa_szczesniak Wiemy o tym. Ceny ze wszystkim szaleją. Nawozy na allegro. Ale też gnojówki robię. Opryski robię z jodyny, sody. Na aksamitkach w tym roku zaoszczędzilam, bo zrobiłam ponad 300 sadzonek 😁.\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9092\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @KIRIMASU_MMA @PiotrWiel @Allegro_Group @zrzutkapl @GrzegorzBraun_ No i co chciałeś przez to powiedzieć? Że jesteś jebaćpisem? Po pierwsze nie ty jeden, po drugie nikogo to nie interesuje.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9544\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @natnwyd Gll i ccc mam nadzieję bo rozjebiecie\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9381\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MakrelG Całe szczęście że zacząłem sprzedaż polskich spółek na USA przy dolarze za 4 PLN. Zostały mi tylko te które zarabiają zagranicą. Livechat, Toya, Autopartner. Dobrałem CCC. Nieprędko wrócę na GPW kapitałem. Powodzenia wszystkim\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9765\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @ilar1958 W sumie dobrze, nic mnie tak nie wkurwia jak PP. Jak Sasin się za to zabiera to znaczy, że rychło upadnie. Może inpost z rządowym kontraktem na usługi pocztowe posprząta ten burdel.\n",
      "True label: Positive\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9406\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @EwuniaXYZ Poszło😁a z allegro cisza w sprawie puszek.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9704\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Garść wiadomości z obozu Sony i Microsoftu oraz zwolnienia w CD Projekt RED. Te i wiele innych newsów w tym wydaniu Niecodziennika Okiem Deva. Zapraszam! https://t.co/R7gkKWIB2G https://t.co/Lg6fT0rLja #okiemdeva #gamedev #gamedevelopment #newsy #niecodziennik\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9849\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Wiecie dlaczego #Żabka cofnęła się do IPO price? Otóż, “Kurs przebił 23.6% zniesienia Fibonaciego” 👀🙃 Wiara w 🪄 magiczne poziomy technicznej analizy wciąż potrafi mnie zaskakiwać ;)\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9316\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @rrobertn @OrsonDzi @InPostPL @Allegro_Group Ja mialam taka sytuacje jak paczka byla rozwalona 🙃 Woec skontaktowalam sie ze sprzedawca i on od razu zweryfikowal co sie dzieje (zepsucie paczki) i nadal nowa\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions and convert to probabilities\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "raw_logits = predictions.predictions\n",
    "softmax_probs = F.softmax(torch.tensor(raw_logits), dim=1).numpy()\n",
    "\n",
    "# Load original test dataset that contains the text\n",
    "# You'll need to provide the original dataset that was used for tokenization\n",
    "\n",
    "# Create a DataFrame with texts and probabilities\n",
    "results_df = pd.DataFrame({\n",
    "    'text': test_df['text'].values,  # assuming text column is named 'text'\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': np.argmax(softmax_probs, axis=1),\n",
    "    'negative_prob': softmax_probs[:, 0],\n",
    "    'neutral_prob': softmax_probs[:, 1],\n",
    "    'positive_prob': softmax_probs[:, 2],\n",
    "    'confidence': np.max(softmax_probs, axis=1)\n",
    "})\n",
    "\n",
    "# Function to get label name\n",
    "def get_label_name(label_id):\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    return labels[label_id]\n",
    "\n",
    "# Add readable labels\n",
    "results_df['true_label_name'] = results_df['true_label'].apply(get_label_name)\n",
    "results_df['predicted_label_name'] = results_df['predicted_label'].apply(get_label_name)\n",
    "\n",
    "\n",
    "# Show high confidence mistakes\n",
    "confidence_threshold = 0.9\n",
    "misclassified = results_df[\n",
    "    (results_df['true_label'] != results_df['predicted_label']) & \n",
    "    (results_df['confidence'] > confidence_threshold)\n",
    "]\n",
    "\n",
    "print(\"\\nHighly confident misclassifications:\")\n",
    "for idx, row in misclassified.head(100).iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Tweet: {row['text']}\")\n",
    "    print(f\"True label: {row['true_label_name']}\")\n",
    "    print(f\"Predicted (wrong): {row['predicted_label_name']}\")\n",
    "    print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "\n",
    "# Save detailed results to CSV\n",
    "results_df.to_csv('predictions_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "misclassified_samples = {\n",
    "    \"Index\": misclassified_indices,\n",
    "    \"True Label\": true_labels[misclassified_indices],\n",
    "    \"Predicted Label\": predicted_labels[misclassified_indices],\n",
    "    \"Text\": [tokenizer.decode(tokenized_dataset[\"test\"][\"input_ids\"][i], skip_special_tokens=True) for i in misclassified_indices]\n",
    "}\n",
    "df_misclassified = pd.DataFrame(misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tweet Research",
   "language": "python",
   "name": "tweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
