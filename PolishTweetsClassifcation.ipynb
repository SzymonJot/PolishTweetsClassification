{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import spacy\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction\n",
    ")\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from functions import preprocess_tweet,replace_emoji,lemmatize_text,remove_stops,analyze_sentiment,create_tokenize_function\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "tool = language_tool_python.LanguageTool('pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv('TrainingData/annotation_dataset - general_label.csv',index_col=0)\n",
    "dataset_2 = pd.read_csv('TrainingData/annotation_dataset - annotation_second_round_labeled.csv',index_col=0)\n",
    "\n",
    "dataset_1 = dataset_1[['text','Overall']]\n",
    "dataset_1 =  dataset_1.rename(columns={'Overall':'labels'})\n",
    "\n",
    "dataset_2 = dataset_2[['text','label']]\n",
    "dataset_2 =  dataset_2.rename(columns={'label':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled = pd.concat([dataset_1,dataset_2],axis=0)\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='labels'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 86.84999999999991\n",
      "Max tokens: 358\n"
     ]
    }
   ],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "The first part was training the model with different basic preprocessing strategies.\n",
    "Then we evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_models = {}\n",
    "tested_models['ROBERT'] = \"sdadas/polish-roberta-base-v2\"\n",
    "tested_models['HERBERT']  = \"allegro/herbert-base-cased\"\n",
    "tested_models['POLBERT']  = \"dkleczek/bert-base-polish-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub(r'@\\w+|#\\w+', ' ', tweet)\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    # Replace underscores with space\n",
    "    tweet = re.sub(r'_', ' ', tweet)\n",
    "    # Changed \\w to use explicit character ranges instead\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing http\n",
    "def preprocess_tweet_https(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    return tweet\n",
    "\n",
    "#Removing hashtags\n",
    "def preprocess_tweet_hashtag(tweet):\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing mentions\n",
    "def preprocess_tweet_mention(tweet):\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing cashtag\n",
    "def preprocess_tweet_cashtag(tweet):\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing all charatcters except polish letter and ? !\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing repeated letters\n",
    "def preprocess_tweet_rep(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing white spaces\n",
    "def preprocess_tweet_norm(tweet):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "#Normalizing caps\n",
    "def preprocess_caps(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(^|[.!?]\\s+)(\\w)', lambda m: m.group(1) + m.group(2).upper(), tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_tco(tweet):\n",
    "    return re.sub(r\"https?://t\\.co/\\S+\", \"\", tweet).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(df, transformations, name):\n",
    "    \"\"\"Apply a sequence of transformations to the dataframe's text column\"\"\"\n",
    "    processed_df = reduce(\n",
    "        lambda acc, func: acc.assign(text=acc['text'].apply(func)).drop_duplicates(subset='text'),\n",
    "        transformations,\n",
    "        copy.deepcopy(df)\n",
    "    )\n",
    "    processed_df.to_csv(fr'TrainingData/processed_data_{name}.csv')\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipelines\n",
    "strategies_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_emoji': [\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'No_processing_spelling': [\n",
    "        tool.correct\n",
    "    ],\n",
    "    'No_processing_spelling_lem': [\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'No_processing_spelling_lem_sp': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ],\n",
    "    'Basic_processing': [\n",
    "        preprocess_tweet\n",
    "    ],\n",
    "    'Basic_processing_emoji': [\n",
    "        preprocess_tweet,\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'Basic_processing_spelling': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ]\n",
    "}\n",
    "\n",
    "basic_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_http': [\n",
    "        preprocess_tweet_https\n",
    "    ],\n",
    "    'No_processing_hashtag': [\n",
    "        preprocess_tweet_hashtag\n",
    "    ],\n",
    "    'No_processing_mention': [\n",
    "        preprocess_tweet_mention\n",
    "    ],\n",
    "    'No_processing_cashtag': [\n",
    "        preprocess_tweet_cashtag\n",
    "    ],\n",
    "    'No_processing__text': [\n",
    "        preprocess_tweet_text\n",
    "    ],\n",
    "    'No_processing__rep': [\n",
    "        preprocess_tweet_rep\n",
    "    ],\n",
    "    'No_processing_norm': [\n",
    "        preprocess_tweet_norm\n",
    "    ],\n",
    "    'No_processing_caps': [\n",
    "        preprocess_caps\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all variants\n",
    "datasets_basic = {\n",
    "    name: apply_transformations(dataset_labeled, pipeline,name = name)\n",
    "    for name, pipeline in basic_processing_pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_strategies = {\n",
    "    name: apply_transformations(dataset_labeled, pipeline, name = name)\n",
    "    for name, pipeline in strategies_processing_pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_basic\n",
    "model = tested_models['dkleczek/bert-base-polish-cased-v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 11391.55 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 7109.39 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2082/2082 [00:00<00:00, 13901.91 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 893/893 [00:00<00:00, 9140.88 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 14334.48 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14235.54 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2087/2087 [00:00<00:00, 10710.36 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 895/895 [00:00<00:00, 14196.10 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 12525.75 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 13561.60 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 13635.66 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 13053.34 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 12264.54 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 12790.78 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 2088/2088 [00:00<00:00, 14167.38 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 896/896 [00:00<00:00, 14266.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = {}\n",
    "for key, df in datasets.items():\n",
    "    # First split the data before balancing\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.remove_columns('__index_level_0__')\n",
    "    train_test = dataset.train_test_split(test_size=0.3, seed=16)\n",
    "    \n",
    "    # Convert to pandas to balance only training data\n",
    "    train_df = train_test['train'].to_pandas()\n",
    "    test_df = train_test['test'].to_pandas()\n",
    "    \n",
    "    # Balance only training data\n",
    "    #balanced_train_df = balance_text_data(train_df)\n",
    "    #balanced_train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    # Tokenize both datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    \n",
    "    # Tokenize training data\n",
    "    tokenized_train = train_dataset.map(\n",
    "        lambda examples: tokenizer(examples[\"text\"], \n",
    "                                 truncation=True, \n",
    "                                 padding=\"max_length\",\n",
    "                                 max_length=128),\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Tokenize test data\n",
    "    tokenized_test = test_dataset.map(\n",
    "        lambda examples: tokenizer(examples[\"text\"], \n",
    "                                 truncation=True, \n",
    "                                 padding=\"max_length\",\n",
    "                                 max_length=128),\n",
    "        batched=True,\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\", \n",
    "                              columns=[\"input_ids\", \n",
    "                                      \"attention_mask\", \n",
    "                                      \"labels\"])\n",
    "    tokenized_test.set_format(\"torch\", \n",
    "                             columns=[\"input_ids\", \n",
    "                                     \"attention_mask\", \n",
    "                                     \"labels\"])\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    tokenized_datasets[key] = DatasetDict({\n",
    "        'train': tokenized_train,\n",
    "        'test': tokenized_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(key, tokenized_dataset, base_model_name):\n",
    "\n",
    "    # Reload model for each run to reset weights\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=3)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{key}_{base_model_name}\",\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=4,  \n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\", \n",
    "        greater_is_better=True,\n",
    "        seed=45,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=25,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_grad_norm=15.0,\n",
    "        save_total_limit=2,  # Keep only last 2 checkpoints\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    # Initialize custom trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      " 14%|█▎        | 162/1192 [38:51<4:07:06, 14.39s/it]\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [03:30<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0584, 'grad_norm': 7.579248428344727, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [04:06<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9442, 'grad_norm': 5.10402774810791, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [04:42<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9708, 'grad_norm': 8.393890380859375, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [05:18<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9923, 'grad_norm': 6.954557418823242, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [05:53<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9422, 'grad_norm': 15.152691841125488, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [06:29<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8886, 'grad_norm': 7.321695327758789, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [07:05<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7993, 'grad_norm': 24.327877044677734, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [07:41<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7987, 'grad_norm': 13.519432067871094, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [08:17<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7334, 'grad_norm': 16.466838836669922, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [08:53<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7184, 'grad_norm': 60.5189208984375, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [09:44<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7776700258255005, 'eval_accuracy': 0.6462053571428571, 'eval_f1_macro': 0.553596193022971, 'eval_f1_weighted': 0.6324176569645957, 'eval_f1_0': 0.5923217550274223, 'eval_f1_1': 0.7455968688845401, 'eval_f1_2': 0.32286995515695066, 'eval_runtime': 34.9347, 'eval_samples_per_second': 25.648, 'eval_steps_per_second': 0.801, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased\\checkpoint-895] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [10:14<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.827, 'grad_norm': 6.911230087280273, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [10:50<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6914, 'grad_norm': 13.97821044921875, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [11:26<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.66, 'grad_norm': 9.693144798278809, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [12:02<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5136, 'grad_norm': 22.930253982543945, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [12:38<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6434, 'grad_norm': 16.391626358032227, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [13:14<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5813, 'grad_norm': 11.901056289672852, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [13:49<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.644, 'grad_norm': 6.371689796447754, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [14:25<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5784, 'grad_norm': 11.024168968200684, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [15:01<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6552, 'grad_norm': 16.798118591308594, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [15:37<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5794, 'grad_norm': 11.008252143859863, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [16:44<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6723803877830505, 'eval_accuracy': 0.7220982142857143, 'eval_f1_macro': 0.6434824533464543, 'eval_f1_weighted': 0.7075016795998206, 'eval_f1_0': 0.6506666666666666, 'eval_f1_1': 0.8006932409012132, 'eval_f1_2': 0.4790874524714829, 'eval_runtime': 34.9184, 'eval_samples_per_second': 25.66, 'eval_steps_per_second': 0.802, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [16:58<00:07,  1.41s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5235, 'grad_norm': 9.715731620788574, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [17:34<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3691, 'grad_norm': 10.675468444824219, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [18:09<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3971, 'grad_norm': 20.19683265686035, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [18:45<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.39, 'grad_norm': 13.456282615661621, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [19:20<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4194, 'grad_norm': 15.577534675598145, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [19:55<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3213, 'grad_norm': 11.546286582946777, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [20:30<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.346, 'grad_norm': 2.032898426055908, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [21:06<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4195, 'grad_norm': 20.23858642578125, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [21:41<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3751, 'grad_norm': 31.843387603759766, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [22:16<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3761, 'grad_norm': 12.364965438842773, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [22:52<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3205, 'grad_norm': 32.27821731567383, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [23:37<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7459568381309509, 'eval_accuracy': 0.7276785714285714, 'eval_f1_macro': 0.6841980396779271, 'eval_f1_weighted': 0.7314420046522663, 'eval_f1_0': 0.6788511749347258, 'eval_f1_1': 0.8007699711260827, 'eval_f1_2': 0.572972972972973, 'eval_runtime': 34.1339, 'eval_samples_per_second': 26.25, 'eval_steps_per_second': 0.82, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [24:11<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3095, 'grad_norm': 23.603229522705078, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [24:49<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2217, 'grad_norm': 22.091856002807617, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [25:25<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2742, 'grad_norm': 37.71381759643555, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [26:00<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2497, 'grad_norm': 4.561873912811279, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [26:35<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1486, 'grad_norm': 16.305206298828125, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [27:10<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1512, 'grad_norm': 11.46944522857666, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [27:46<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1938, 'grad_norm': 11.111149787902832, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [28:21<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2191, 'grad_norm': 29.15425682067871, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [28:56<00:07,  1.41s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1524, 'grad_norm': 6.459968090057373, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [29:32<00:07,  1.41s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1992, 'grad_norm': 7.665366172790527, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased\\checkpoint-1044] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [30:44<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7916117310523987, 'eval_accuracy': 0.7276785714285714, 'eval_f1_macro': 0.6779268832998712, 'eval_f1_weighted': 0.7270012800846715, 'eval_f1_0': 0.6896551724137931, 'eval_f1_1': 0.7981132075471699, 'eval_f1_2': 0.5460122699386503, 'eval_runtime': 35.0584, 'eval_samples_per_second': 25.557, 'eval_steps_per_second': 0.799, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_allegro\\herbert-base-cased\\checkpoint-1044] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_allegro/herbert-base-cased\\checkpoint-783 (score: 0.6841980396779271).\n",
      "\n",
      "100%|██████████| 1044/1044 [28:07<00:00,  1.62s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1687.3308, 'train_samples_per_second': 4.95, 'train_steps_per_second': 0.619, 'train_loss': 0.5229683376820151, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:33<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_http\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,082\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,040\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [32:13<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9616, 'grad_norm': 6.355753421783447, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [32:49<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9485, 'grad_norm': 4.766526699066162, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [33:25<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8903, 'grad_norm': 7.058584213256836, 'learning_rate': 1.953441295546559e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [34:01<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9098, 'grad_norm': 13.655077934265137, 'learning_rate': 1.9028340080971662e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [34:36<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9277, 'grad_norm': 8.609986305236816, 'learning_rate': 1.8522267206477735e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [35:12<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8783, 'grad_norm': 51.34620666503906, 'learning_rate': 1.801619433198381e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [35:47<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8503, 'grad_norm': 8.405860900878906, 'learning_rate': 1.751012145748988e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [36:23<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8243, 'grad_norm': 19.96824073791504, 'learning_rate': 1.7004048582995952e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [36:58<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7017, 'grad_norm': 13.482708930969238, 'learning_rate': 1.6497975708502025e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [37:34<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7038, 'grad_norm': 13.658818244934082, 'learning_rate': 1.59919028340081e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [38:23<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-260\n",
      "Configuration saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-260\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7645179629325867, 'eval_accuracy': 0.671892497200448, 'eval_f1_macro': 0.5817345999560392, 'eval_f1_weighted': 0.6482185946193219, 'eval_f1_0': 0.5281899109792285, 'eval_f1_1': 0.7725694444444444, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 34.1687, 'eval_samples_per_second': 26.135, 'eval_steps_per_second': 0.819, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-260\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [38:54<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6824, 'grad_norm': 9.513500213623047, 'learning_rate': 1.5485829959514172e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [39:29<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.507, 'grad_norm': 54.77683639526367, 'learning_rate': 1.4979757085020243e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [40:05<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5724, 'grad_norm': 24.30030632019043, 'learning_rate': 1.4473684210526317e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [40:40<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.573, 'grad_norm': 12.433663368225098, 'learning_rate': 1.396761133603239e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [41:15<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6753, 'grad_norm': 18.19736671447754, 'learning_rate': 1.3461538461538463e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [41:50<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5917, 'grad_norm': 18.045381546020508, 'learning_rate': 1.2955465587044535e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [42:26<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6298, 'grad_norm': 11.198561668395996, 'learning_rate': 1.2449392712550608e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [43:01<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5786, 'grad_norm': 14.74775218963623, 'learning_rate': 1.1943319838056682e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [43:36<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6177, 'grad_norm': 17.125152587890625, 'learning_rate': 1.1437246963562753e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [44:11<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5537, 'grad_norm': 16.752073287963867, 'learning_rate': 1.0931174089068828e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [45:15<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-521\n",
      "Configuration saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-521\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7256925702095032, 'eval_accuracy': 0.6931690929451287, 'eval_f1_macro': 0.6639107915728655, 'eval_f1_weighted': 0.696372811943066, 'eval_f1_0': 0.6318407960199005, 'eval_f1_1': 0.7574819401444789, 'eval_f1_2': 0.6024096385542169, 'eval_runtime': 34.3783, 'eval_samples_per_second': 25.976, 'eval_steps_per_second': 0.814, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-521\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_http_allegro\\herbert-base-cased\\checkpoint-260] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [45:32<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5574, 'grad_norm': 8.08380126953125, 'learning_rate': 1.04251012145749e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [46:08<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3664, 'grad_norm': 15.664647102355957, 'learning_rate': 9.919028340080973e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [46:43<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2952, 'grad_norm': 12.704466819763184, 'learning_rate': 9.412955465587045e-06, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [47:19<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3724, 'grad_norm': 25.150978088378906, 'learning_rate': 8.906882591093118e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [47:54<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4083, 'grad_norm': 35.108726501464844, 'learning_rate': 8.400809716599191e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [48:30<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3732, 'grad_norm': 17.238574981689453, 'learning_rate': 7.894736842105265e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [49:05<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3346, 'grad_norm': 16.509851455688477, 'learning_rate': 7.388663967611337e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [49:41<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3158, 'grad_norm': 24.231685638427734, 'learning_rate': 6.882591093117409e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [50:16<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3655, 'grad_norm': 25.443756103515625, 'learning_rate': 6.376518218623482e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [50:52<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3674, 'grad_norm': 24.787973403930664, 'learning_rate': 5.870445344129555e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [51:27<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2793, 'grad_norm': 23.90633201599121, 'learning_rate': 5.364372469635628e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [52:10<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-781\n",
      "Configuration saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-781\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7923822402954102, 'eval_accuracy': 0.7077267637178052, 'eval_f1_macro': 0.6624497628807974, 'eval_f1_weighted': 0.7037631607750735, 'eval_f1_0': 0.6205128205128205, 'eval_f1_1': 0.7816091954022989, 'eval_f1_2': 0.5852272727272727, 'eval_runtime': 34.3799, 'eval_samples_per_second': 25.974, 'eval_steps_per_second': 0.814, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-781\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_http_allegro\\herbert-base-cased\\checkpoint-288] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [52:47<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1905, 'grad_norm': 58.16408920288086, 'learning_rate': 4.8582995951417e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [53:22<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2189, 'grad_norm': 4.201410293579102, 'learning_rate': 4.3522267206477735e-06, 'epoch': 3.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [53:57<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1727, 'grad_norm': 16.82158660888672, 'learning_rate': 3.846153846153847e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [54:32<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2266, 'grad_norm': 12.685111045837402, 'learning_rate': 3.3400809716599193e-06, 'epoch': 3.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [55:07<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.201, 'grad_norm': 15.579649925231934, 'learning_rate': 2.834008097165992e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [55:42<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2217, 'grad_norm': 27.29026222229004, 'learning_rate': 2.327935222672065e-06, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [56:17<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2202, 'grad_norm': 21.230392456054688, 'learning_rate': 1.8218623481781379e-06, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [56:51<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1855, 'grad_norm': 8.595382690429688, 'learning_rate': 1.3157894736842106e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [57:26<00:07,  1.41s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1353, 'grad_norm': 3.361626625061035, 'learning_rate': 8.097165991902834e-07, 'epoch': 3.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [58:01<00:07,  1.41s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.222, 'grad_norm': 18.54242515563965, 'learning_rate': 3.0364372469635626e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\n",
      "Configuration saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\\config.json\n",
      "Model weights saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_http_allegro\\herbert-base-cased\\checkpoint-781] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [59:07<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\n",
      "Configuration saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8796806335449219, 'eval_accuracy': 0.7088465845464725, 'eval_f1_macro': 0.6582721266660506, 'eval_f1_weighted': 0.7014778792567007, 'eval_f1_0': 0.6321243523316062, 'eval_f1_1': 0.7817164179104478, 'eval_f1_2': 0.5609756097560976, 'eval_runtime': 34.8536, 'eval_samples_per_second': 25.621, 'eval_steps_per_second': 0.803, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-1040\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_http_allegro/herbert-base-cased\\checkpoint-521 (score: 0.6639107915728655).\n",
      "\n",
      "100%|██████████| 1040/1040 [27:50<00:00,  1.61s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 893\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1670.4757, 'train_samples_per_second': 4.985, 'train_steps_per_second': 0.623, 'train_loss': 0.4973429487301753, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:32<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_hashtag\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:00:40<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2666, 'grad_norm': 9.810128211975098, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:01:16<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9445, 'grad_norm': 10.379374504089355, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:01:51<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9843, 'grad_norm': 8.799097061157227, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:02:27<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0097, 'grad_norm': 6.557713985443115, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:03:02<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9217, 'grad_norm': 13.096837043762207, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:03:37<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8672, 'grad_norm': 7.984671115875244, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:04:12<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8071, 'grad_norm': 9.807134628295898, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:04:47<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7929, 'grad_norm': 17.833309173583984, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:05:22<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7948, 'grad_norm': 28.96474838256836, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:05:57<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7134, 'grad_norm': 37.10857009887695, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:06:48<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8301162719726562, 'eval_accuracy': 0.6272321428571429, 'eval_f1_macro': 0.5399914382943637, 'eval_f1_weighted': 0.6156561891515763, 'eval_f1_0': 0.564748201438849, 'eval_f1_1': 0.724950884086444, 'eval_f1_2': 0.3302752293577982, 'eval_runtime': 34.8671, 'eval_samples_per_second': 25.698, 'eval_steps_per_second': 0.803, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:07:19<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8488, 'grad_norm': 6.546280384063721, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:07:55<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6937, 'grad_norm': 15.201605796813965, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:08:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6469, 'grad_norm': 11.166773796081543, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:09:05<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5583, 'grad_norm': 14.214914321899414, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:09:41<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6034, 'grad_norm': 30.7203311920166, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:10:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6254, 'grad_norm': 14.33179759979248, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:10:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6718, 'grad_norm': 8.148767471313477, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:11:27<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6069, 'grad_norm': 8.152971267700195, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:12:02<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6152, 'grad_norm': 26.77111053466797, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:12:37<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5873, 'grad_norm': 12.964457511901855, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:13:43<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7047405242919922, 'eval_accuracy': 0.7220982142857143, 'eval_f1_macro': 0.6371091178034433, 'eval_f1_weighted': 0.7053113599249208, 'eval_f1_0': 0.6408839779005525, 'eval_f1_1': 0.8047945205479452, 'eval_f1_2': 0.46564885496183206, 'eval_runtime': 34.6702, 'eval_samples_per_second': 25.844, 'eval_steps_per_second': 0.808, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:13:58<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5488, 'grad_norm': 16.734477996826172, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:14:34<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3828, 'grad_norm': 9.237231254577637, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:15:09<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4242, 'grad_norm': 28.2421932220459, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:15:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3987, 'grad_norm': 23.189632415771484, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:16:19<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4216, 'grad_norm': 14.508130073547363, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:16:54<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.334, 'grad_norm': 18.695423126220703, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:17:28<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3746, 'grad_norm': 19.467069625854492, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:18:04<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4577, 'grad_norm': 21.100122451782227, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:18:39<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3393, 'grad_norm': 27.763952255249023, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:19:14<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3543, 'grad_norm': 9.613200187683105, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:19:49<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.342, 'grad_norm': 18.501638412475586, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:20:34<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7330573201179504, 'eval_accuracy': 0.7176339285714286, 'eval_f1_macro': 0.6591304545382598, 'eval_f1_weighted': 0.7137792176789548, 'eval_f1_0': 0.6702127659574468, 'eval_f1_1': 0.7930720145852325, 'eval_f1_2': 0.5141065830721003, 'eval_runtime': 34.1824, 'eval_samples_per_second': 26.212, 'eval_steps_per_second': 0.819, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_hashtag_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:21:08<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.324, 'grad_norm': 14.029914855957031, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:21:43<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2002, 'grad_norm': 8.272696495056152, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:22:18<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.269, 'grad_norm': 35.78861618041992, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:22:53<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2263, 'grad_norm': 14.765359878540039, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:23:28<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1716, 'grad_norm': 70.6908950805664, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:24:03<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1676, 'grad_norm': 8.2752046585083, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:24:38<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2442, 'grad_norm': 22.31203269958496, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:25:13<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.227, 'grad_norm': 12.892791748046875, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:25:48<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1441, 'grad_norm': 19.95402717590332, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:26:23<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2131, 'grad_norm': 18.507627487182617, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_hashtag_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [1:27:35<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8003533482551575, 'eval_accuracy': 0.7265625, 'eval_f1_macro': 0.6735278066278153, 'eval_f1_weighted': 0.7234917057108134, 'eval_f1_0': 0.6770833333333334, 'eval_f1_1': 0.7963302752293578, 'eval_f1_2': 0.5471698113207547, 'eval_runtime': 35.1529, 'eval_samples_per_second': 25.489, 'eval_steps_per_second': 0.797, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_hashtag_allegro/herbert-base-cased\\checkpoint-1044 (score: 0.6735278066278153).\n",
      "\n",
      "100%|██████████| 1044/1044 [27:40<00:00,  1.59s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1660.3716, 'train_samples_per_second': 5.03, 'train_steps_per_second': 0.629, 'train_loss': 0.535191115063269, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:33<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_mention\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,087\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:28:57<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0249, 'grad_norm': 4.894474029541016, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:29:33<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9915, 'grad_norm': 4.884174346923828, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:30:08<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.891, 'grad_norm': 4.899679183959961, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:30:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9657, 'grad_norm': 8.834877967834473, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:31:19<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8708, 'grad_norm': 5.929561614990234, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:31:55<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8659, 'grad_norm': 6.009470462799072, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:32:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.865, 'grad_norm': 7.084798336029053, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:33:05<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7651, 'grad_norm': 10.810303688049316, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:33:41<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7496, 'grad_norm': 12.429218292236328, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:34:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8041, 'grad_norm': 12.66268253326416, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:35:06<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7334654331207275, 'eval_accuracy': 0.6860335195530727, 'eval_f1_macro': 0.6229992979121347, 'eval_f1_weighted': 0.6820861747330732, 'eval_f1_0': 0.5985401459854015, 'eval_f1_1': 0.7752808988764045, 'eval_f1_2': 0.49517684887459806, 'eval_runtime': 34.6956, 'eval_samples_per_second': 25.796, 'eval_steps_per_second': 0.807, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:35:37<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.648, 'grad_norm': 25.641008377075195, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:36:12<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6137, 'grad_norm': 17.313800811767578, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:36:47<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5423, 'grad_norm': 32.60093307495117, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:37:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6011, 'grad_norm': 23.6112003326416, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:37:58<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6818, 'grad_norm': 8.752723693847656, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:38:33<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6514, 'grad_norm': 19.298137664794922, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:39:08<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6297, 'grad_norm': 10.848614692687988, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:39:43<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5347, 'grad_norm': 18.418996810913086, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:40:18<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6856, 'grad_norm': 9.013667106628418, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:40:53<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6147, 'grad_norm': 15.276374816894531, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:41:58<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6934391856193542, 'eval_accuracy': 0.7094972067039106, 'eval_f1_macro': 0.628316517372983, 'eval_f1_weighted': 0.6916226414637432, 'eval_f1_0': 0.6032608695652174, 'eval_f1_1': 0.7914163090128755, 'eval_f1_2': 0.490272373540856, 'eval_runtime': 34.4274, 'eval_samples_per_second': 25.997, 'eval_steps_per_second': 0.813, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:42:12<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4815, 'grad_norm': 33.4544677734375, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:42:48<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4096, 'grad_norm': 10.882494926452637, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:43:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3906, 'grad_norm': 6.647542476654053, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:43:58<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2921, 'grad_norm': 14.428661346435547, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:44:33<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3123, 'grad_norm': 7.420270919799805, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:45:08<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3355, 'grad_norm': 17.823585510253906, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:45:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3549, 'grad_norm': 28.787982940673828, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:46:19<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3893, 'grad_norm': 18.4284610748291, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:46:55<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2745, 'grad_norm': 30.609081268310547, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:47:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2633, 'grad_norm': 23.122207641601562, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:48:06<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3525, 'grad_norm': 12.750513076782227, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [1:48:52<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7736173868179321, 'eval_accuracy': 0.7206703910614525, 'eval_f1_macro': 0.6621509117186352, 'eval_f1_weighted': 0.7140528153932755, 'eval_f1_0': 0.5936599423631124, 'eval_f1_1': 0.7981981981981981, 'eval_f1_2': 0.5945945945945946, 'eval_runtime': 34.7312, 'eval_samples_per_second': 25.769, 'eval_steps_per_second': 0.806, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_mention_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:49:31<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2528, 'grad_norm': 5.148802280426025, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:50:07<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2404, 'grad_norm': 14.876453399658203, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:50:42<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2408, 'grad_norm': 8.604042053222656, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:51:17<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.217, 'grad_norm': 15.67208480834961, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:51:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.202, 'grad_norm': 2.300412654876709, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:52:28<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1795, 'grad_norm': 4.626592636108398, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:53:03<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.209, 'grad_norm': 24.563243865966797, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:53:39<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.228, 'grad_norm': 15.87127685546875, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:54:15<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1172, 'grad_norm': 15.023693084716797, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:54:53<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1649, 'grad_norm': 24.506616592407227, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_mention_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [1:56:12<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8305208683013916, 'eval_accuracy': 0.7162011173184357, 'eval_f1_macro': 0.666518159846707, 'eval_f1_weighted': 0.7134968633710488, 'eval_f1_0': 0.6134020618556701, 'eval_f1_1': 0.7892293407613742, 'eval_f1_2': 0.5969230769230769, 'eval_runtime': 38.9097, 'eval_samples_per_second': 23.002, 'eval_steps_per_second': 0.72, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_mention_allegro/herbert-base-cased\\checkpoint-1044 (score: 0.666518159846707).\n",
      "\n",
      "100%|██████████| 1044/1044 [28:00<00:00,  1.61s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 895\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1680.6018, 'train_samples_per_second': 4.967, 'train_steps_per_second': 0.621, 'train_loss': 0.5053994027590843, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:37<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_mention', 'accuracy': 0.7162011173184357, 'macro_f1': 0.666518159846707, 'weighted_f1': 0.7134968633710488, 'neutral_f1': 0.6134020618556701, 'positive_f1': 0.7892293407613742, 'negative_f1': 0.5969230769230769, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_cashtag\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [1:57:40<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0276, 'grad_norm': 7.492794513702393, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:58:18<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9568, 'grad_norm': 3.9794669151306152, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:58:55<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9647, 'grad_norm': 7.095371723175049, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [1:59:33<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0028, 'grad_norm': 7.143306255340576, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:00:11<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9235, 'grad_norm': 15.374015808105469, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:00:48<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8507, 'grad_norm': 9.172657012939453, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:01:26<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.788, 'grad_norm': 8.740941047668457, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:02:03<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7694, 'grad_norm': 22.003427505493164, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:02:41<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7219, 'grad_norm': 9.423047065734863, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:03:18<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6489, 'grad_norm': 16.618816375732422, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:04:13<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7343015670776367, 'eval_accuracy': 0.6796875, 'eval_f1_macro': 0.6245579079770117, 'eval_f1_weighted': 0.682688819858159, 'eval_f1_0': 0.603112840466926, 'eval_f1_1': 0.7687687687687688, 'eval_f1_2': 0.5017921146953405, 'eval_runtime': 38.3698, 'eval_samples_per_second': 23.352, 'eval_steps_per_second': 0.73, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:04:43<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8154, 'grad_norm': 6.6476006507873535, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:05:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6705, 'grad_norm': 13.682698249816895, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:05:58<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6281, 'grad_norm': 16.734712600708008, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:06:36<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5298, 'grad_norm': 14.98949146270752, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:07:13<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5897, 'grad_norm': 16.590360641479492, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:07:51<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5793, 'grad_norm': 12.290352821350098, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:08:29<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.619, 'grad_norm': 5.4935126304626465, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:09:06<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5648, 'grad_norm': 11.025948524475098, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:09:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6295, 'grad_norm': 27.893333435058594, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:10:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5184, 'grad_norm': 11.003823280334473, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:11:33<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.684441089630127, 'eval_accuracy': 0.7254464285714286, 'eval_f1_macro': 0.6395545730525524, 'eval_f1_weighted': 0.708543364681457, 'eval_f1_0': 0.6717557251908397, 'eval_f1_1': 0.8076923076923077, 'eval_f1_2': 0.4392156862745098, 'eval_runtime': 38.6745, 'eval_samples_per_second': 23.168, 'eval_steps_per_second': 0.724, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:11:46<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4892, 'grad_norm': 9.035557746887207, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:12:24<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3662, 'grad_norm': 9.58580493927002, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:13:01<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3725, 'grad_norm': 26.73890495300293, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:13:38<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3688, 'grad_norm': 18.428848266601562, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:14:15<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4262, 'grad_norm': 9.335040092468262, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:14:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3286, 'grad_norm': 14.724750518798828, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:15:29<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3283, 'grad_norm': 9.846274375915527, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:16:06<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.415, 'grad_norm': 25.14288330078125, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:16:43<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.332, 'grad_norm': 32.17760467529297, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:17:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3349, 'grad_norm': 15.575273513793945, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:17:58<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3192, 'grad_norm': 15.675735473632812, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:18:48<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7138248085975647, 'eval_accuracy': 0.7299107142857143, 'eval_f1_macro': 0.6820387041376067, 'eval_f1_weighted': 0.7292837929196836, 'eval_f1_0': 0.6862745098039216, 'eval_f1_1': 0.7981132075471699, 'eval_f1_2': 0.5617283950617284, 'eval_runtime': 38.1847, 'eval_samples_per_second': 23.465, 'eval_steps_per_second': 0.733, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_cashtag_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:19:25<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3414, 'grad_norm': 8.45695686340332, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:20:02<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2094, 'grad_norm': 11.394469261169434, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:20:39<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2086, 'grad_norm': 10.965614318847656, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:21:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2084, 'grad_norm': 2.460541009902954, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:21:53<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1392, 'grad_norm': 16.063310623168945, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:22:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1586, 'grad_norm': 23.066797256469727, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:23:07<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1642, 'grad_norm': 11.248421669006348, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:23:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2249, 'grad_norm': 23.81396484375, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:24:21<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1494, 'grad_norm': 5.55119514465332, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:24:58<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1605, 'grad_norm': 14.228339195251465, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_cashtag_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [2:26:14<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7794796228408813, 'eval_accuracy': 0.7388392857142857, 'eval_f1_macro': 0.690386324190106, 'eval_f1_weighted': 0.7379474003618675, 'eval_f1_0': 0.696969696969697, 'eval_f1_1': 0.8071161048689138, 'eval_f1_2': 0.5670731707317073, 'eval_runtime': 38.6379, 'eval_samples_per_second': 23.19, 'eval_steps_per_second': 0.725, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_cashtag_allegro/herbert-base-cased\\checkpoint-1044 (score: 0.690386324190106).\n",
      "\n",
      "100%|██████████| 1044/1044 [29:20<00:00,  1.69s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1760.3816, 'train_samples_per_second': 4.744, 'train_steps_per_second': 0.593, 'train_loss': 0.5040640132180576, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:36<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_mention', 'accuracy': 0.7162011173184357, 'macro_f1': 0.666518159846707, 'weighted_f1': 0.7134968633710488, 'neutral_f1': 0.6134020618556701, 'positive_f1': 0.7892293407613742, 'negative_f1': 0.5969230769230769, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_cashtag', 'accuracy': 0.7388392857142857, 'macro_f1': 0.690386324190106, 'weighted_f1': 0.7379474003618675, 'neutral_f1': 0.696969696969697, 'positive_f1': 0.8071161048689138, 'negative_f1': 0.5670731707317073, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing__text\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:27:40<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9866, 'grad_norm': 6.948268890380859, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:28:17<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9499, 'grad_norm': 4.8570876121521, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:28:54<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9725, 'grad_norm': 8.847456932067871, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:29:32<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0023, 'grad_norm': 6.360026836395264, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:30:09<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8893, 'grad_norm': 16.326791763305664, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:30:46<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8485, 'grad_norm': 10.472224235534668, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:31:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.799, 'grad_norm': 15.551191329956055, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:32:00<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7641, 'grad_norm': 15.283625602722168, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:32:37<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7499, 'grad_norm': 33.38019561767578, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:33:14<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7182, 'grad_norm': 21.22515296936035, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:34:09<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7669579386711121, 'eval_accuracy': 0.6573660714285714, 'eval_f1_macro': 0.6254973035844911, 'eval_f1_weighted': 0.6657692961112456, 'eval_f1_0': 0.6081871345029239, 'eval_f1_1': 0.725531914893617, 'eval_f1_2': 0.5427728613569321, 'eval_runtime': 38.3332, 'eval_samples_per_second': 23.374, 'eval_steps_per_second': 0.73, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:34:40<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.797, 'grad_norm': 8.365196228027344, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:35:17<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.673, 'grad_norm': 37.624961853027344, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:35:55<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6214, 'grad_norm': 25.412796020507812, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:36:32<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.456, 'grad_norm': 45.534786224365234, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:37:09<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5864, 'grad_norm': 25.3626651763916, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:37:46<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5571, 'grad_norm': 16.72199821472168, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:38:24<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6732, 'grad_norm': 7.924086570739746, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:39:01<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5843, 'grad_norm': 12.669818878173828, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:39:38<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6226, 'grad_norm': 25.893360137939453, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:40:15<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.564, 'grad_norm': 11.455657005310059, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:41:25<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7066437602043152, 'eval_accuracy': 0.7243303571428571, 'eval_f1_macro': 0.6419517447748676, 'eval_f1_weighted': 0.7084471536801333, 'eval_f1_0': 0.6564102564102564, 'eval_f1_1': 0.8048780487804879, 'eval_f1_2': 0.4645669291338583, 'eval_runtime': 38.0585, 'eval_samples_per_second': 23.543, 'eval_steps_per_second': 0.736, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:41:38<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5374, 'grad_norm': 24.613384246826172, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:42:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3798, 'grad_norm': 7.5463032722473145, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:42:53<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4139, 'grad_norm': 25.477962493896484, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:43:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4273, 'grad_norm': 19.59882354736328, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:44:07<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3676, 'grad_norm': 18.811235427856445, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:44:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2937, 'grad_norm': 17.931419372558594, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:45:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.352, 'grad_norm': 43.06508255004883, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:45:58<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4297, 'grad_norm': 44.44510269165039, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:46:36<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3715, 'grad_norm': 24.10392189025879, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:47:13<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4259, 'grad_norm': 22.240413665771484, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:47:50<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3251, 'grad_norm': 25.803693771362305, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [2:48:41<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7597010731697083, 'eval_accuracy': 0.703125, 'eval_f1_macro': 0.647697744517202, 'eval_f1_weighted': 0.7003960713728896, 'eval_f1_0': 0.6666666666666666, 'eval_f1_1': 0.7764265668849392, 'eval_f1_2': 0.5, 'eval_runtime': 38.2814, 'eval_samples_per_second': 23.406, 'eval_steps_per_second': 0.731, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing__text_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:49:14<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3121, 'grad_norm': 7.026042938232422, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:49:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2151, 'grad_norm': 9.046957015991211, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:50:29<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2935, 'grad_norm': 28.424816131591797, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:51:06<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2606, 'grad_norm': 6.325514793395996, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:51:43<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1849, 'grad_norm': 24.38848876953125, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:52:20<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1728, 'grad_norm': 12.310575485229492, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:52:57<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2062, 'grad_norm': 14.598958015441895, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:53:34<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.23, 'grad_norm': 14.299948692321777, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:54:11<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1814, 'grad_norm': 23.536081314086914, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:54:48<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2368, 'grad_norm': 31.817588806152344, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing__text_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [2:56:03<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8197928071022034, 'eval_accuracy': 0.7165178571428571, 'eval_f1_macro': 0.6586076231123302, 'eval_f1_weighted': 0.7121867109449609, 'eval_f1_0': 0.6579634464751958, 'eval_f1_1': 0.7905282331511839, 'eval_f1_2': 0.5273311897106109, 'eval_runtime': 38.3763, 'eval_samples_per_second': 23.348, 'eval_steps_per_second': 0.73, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing__text_allegro/herbert-base-cased\\checkpoint-1044 (score: 0.6586076231123302).\n",
      "\n",
      "100%|██████████| 1044/1044 [29:09<00:00,  1.68s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1749.3594, 'train_samples_per_second': 4.774, 'train_steps_per_second': 0.597, 'train_loss': 0.519488032750243, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:36<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_mention', 'accuracy': 0.7162011173184357, 'macro_f1': 0.666518159846707, 'weighted_f1': 0.7134968633710488, 'neutral_f1': 0.6134020618556701, 'positive_f1': 0.7892293407613742, 'negative_f1': 0.5969230769230769, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_cashtag', 'accuracy': 0.7388392857142857, 'macro_f1': 0.690386324190106, 'weighted_f1': 0.7379474003618675, 'neutral_f1': 0.696969696969697, 'positive_f1': 0.8071161048689138, 'negative_f1': 0.5670731707317073, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing__text', 'accuracy': 0.7165178571428571, 'macro_f1': 0.6586076231123302, 'weighted_f1': 0.7121867109449609, 'neutral_f1': 0.6579634464751958, 'positive_f1': 0.7905282331511839, 'negative_f1': 0.5273311897106109, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing__rep\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [2:57:29<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.978, 'grad_norm': 6.474445819854736, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:58:06<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9426, 'grad_norm': 19.15464973449707, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:58:45<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9896, 'grad_norm': 8.419326782226562, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [2:59:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.006, 'grad_norm': 7.338185787200928, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:00:00<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9255, 'grad_norm': 12.527100563049316, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:00:37<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.905, 'grad_norm': 5.507363319396973, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:01:12<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8363, 'grad_norm': 6.798643112182617, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:01:48<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8223, 'grad_norm': 11.175823211669922, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:02:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8025, 'grad_norm': 16.69486427307129, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:02:59<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7319, 'grad_norm': 21.728782653808594, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:03:49<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7223297953605652, 'eval_accuracy': 0.6875, 'eval_f1_macro': 0.6078934223774276, 'eval_f1_weighted': 0.6757577450955459, 'eval_f1_0': 0.6373165618448637, 'eval_f1_1': 0.7734082397003745, 'eval_f1_2': 0.41295546558704455, 'eval_runtime': 34.879, 'eval_samples_per_second': 25.689, 'eval_steps_per_second': 0.803, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:04:20<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7624, 'grad_norm': 8.138566017150879, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:04:56<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6826, 'grad_norm': 45.163002014160156, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:05:31<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6756, 'grad_norm': 11.802772521972656, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:06:06<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.547, 'grad_norm': 13.547041893005371, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:06:42<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6864, 'grad_norm': 19.321163177490234, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:07:17<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5772, 'grad_norm': 10.163596153259277, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:07:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6472, 'grad_norm': 14.119427680969238, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:08:28<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5851, 'grad_norm': 14.522808074951172, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:09:03<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6263, 'grad_norm': 13.050724983215332, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:09:38<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5744, 'grad_norm': 6.544736385345459, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:10:44<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6598154902458191, 'eval_accuracy': 0.7209821428571429, 'eval_f1_macro': 0.6459367872271099, 'eval_f1_weighted': 0.7086225300237398, 'eval_f1_0': 0.6505376344086021, 'eval_f1_1': 0.8, 'eval_f1_2': 0.48727272727272725, 'eval_runtime': 34.8923, 'eval_samples_per_second': 25.679, 'eval_steps_per_second': 0.802, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:10:58<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5766, 'grad_norm': 14.647172927856445, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:11:33<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3704, 'grad_norm': 8.386198043823242, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:12:09<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4167, 'grad_norm': 30.22507095336914, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:12:44<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4105, 'grad_norm': 14.929781913757324, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:13:19<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3935, 'grad_norm': 13.293603897094727, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:13:54<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3026, 'grad_norm': 8.914870262145996, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:14:30<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3521, 'grad_norm': 24.499855041503906, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:15:05<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4494, 'grad_norm': 21.153841018676758, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:15:40<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3862, 'grad_norm': 25.81466293334961, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:16:15<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3807, 'grad_norm': 14.174323081970215, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:16:51<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3443, 'grad_norm': 10.669713973999023, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:17:36<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7026795148849487, 'eval_accuracy': 0.71875, 'eval_f1_macro': 0.6744246469570054, 'eval_f1_weighted': 0.7209168858400653, 'eval_f1_0': 0.6666666666666666, 'eval_f1_1': 0.789272030651341, 'eval_f1_2': 0.5673352435530086, 'eval_runtime': 34.6139, 'eval_samples_per_second': 25.886, 'eval_steps_per_second': 0.809, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing__rep_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:18:10<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.302, 'grad_norm': 34.09278869628906, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:18:46<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2277, 'grad_norm': 7.255012035369873, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:19:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2408, 'grad_norm': 37.44159698486328, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:19:56<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2549, 'grad_norm': 7.49323844909668, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:20:32<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1555, 'grad_norm': 6.895448207855225, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:21:07<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1632, 'grad_norm': 15.301708221435547, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:21:42<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1581, 'grad_norm': 15.352621078491211, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:22:18<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2595, 'grad_norm': 13.11240005493164, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:22:53<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1458, 'grad_norm': 10.536887168884277, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:23:29<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2078, 'grad_norm': 49.127418518066406, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing__rep_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [3:24:39<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7908392548561096, 'eval_accuracy': 0.7254464285714286, 'eval_f1_macro': 0.6717639964784513, 'eval_f1_weighted': 0.7234146993398293, 'eval_f1_0': 0.6649484536082474, 'eval_f1_1': 0.7992599444958371, 'eval_f1_2': 0.5510835913312694, 'eval_runtime': 35.5346, 'eval_samples_per_second': 25.215, 'eval_steps_per_second': 0.788, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing__rep_allegro/herbert-base-cased\\checkpoint-783 (score: 0.6744246469570054).\n",
      "\n",
      "100%|██████████| 1044/1044 [27:56<00:00,  1.61s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1676.3745, 'train_samples_per_second': 4.982, 'train_steps_per_second': 0.623, 'train_loss': 0.5278295047895205, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:34<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_mention', 'accuracy': 0.7162011173184357, 'macro_f1': 0.666518159846707, 'weighted_f1': 0.7134968633710488, 'neutral_f1': 0.6134020618556701, 'positive_f1': 0.7892293407613742, 'negative_f1': 0.5969230769230769, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_cashtag', 'accuracy': 0.7388392857142857, 'macro_f1': 0.690386324190106, 'weighted_f1': 0.7379474003618675, 'neutral_f1': 0.696969696969697, 'positive_f1': 0.8071161048689138, 'negative_f1': 0.5670731707317073, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing__text', 'accuracy': 0.7165178571428571, 'macro_f1': 0.6586076231123302, 'weighted_f1': 0.7121867109449609, 'neutral_f1': 0.6579634464751958, 'positive_f1': 0.7905282331511839, 'negative_f1': 0.5273311897106109, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing__rep', 'accuracy': 0.71875, 'macro_f1': 0.6744246469570054, 'weighted_f1': 0.7209168858400653, 'neutral_f1': 0.6666666666666666, 'positive_f1': 0.789272030651341, 'negative_f1': 0.5673352435530086, 'epochs': 4.0}]\n",
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing_norm\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1,044\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:26:01<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9782, 'grad_norm': 6.571975231170654, 'learning_rate': 9.433962264150944e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:26:37<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9442, 'grad_norm': 5.246313095092773, 'learning_rate': 1.8867924528301888e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:27:12<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9662, 'grad_norm': 7.684013843536377, 'learning_rate': 1.9556004036326944e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:27:47<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9912, 'grad_norm': 7.154625415802002, 'learning_rate': 1.905146316851665e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:28:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9278, 'grad_norm': 12.600197792053223, 'learning_rate': 1.854692230070636e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:28:59<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8871, 'grad_norm': 7.2632551193237305, 'learning_rate': 1.8042381432896066e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:29:34<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8046, 'grad_norm': 10.716758728027344, 'learning_rate': 1.7537840565085772e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:30:10<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7787, 'grad_norm': 11.317590713500977, 'learning_rate': 1.703329969727548e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:30:45<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7455, 'grad_norm': 11.911060333251953, 'learning_rate': 1.6528758829465188e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:31:21<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6923, 'grad_norm': 39.65741729736328, 'learning_rate': 1.6024217961654894e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:32:11<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-261\n",
      "Configuration saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-261\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.723423957824707, 'eval_accuracy': 0.6941964285714286, 'eval_f1_macro': 0.6471683117247453, 'eval_f1_weighted': 0.6977458118131666, 'eval_f1_0': 0.6381156316916489, 'eval_f1_1': 0.7721393034825871, 'eval_f1_2': 0.53125, 'eval_runtime': 34.5206, 'eval_samples_per_second': 25.956, 'eval_steps_per_second': 0.811, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-261\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:32:41<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7867, 'grad_norm': 9.755497932434082, 'learning_rate': 1.5519677093844603e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:33:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6854, 'grad_norm': 10.871246337890625, 'learning_rate': 1.5015136226034311e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:33:52<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6372, 'grad_norm': 9.017191886901855, 'learning_rate': 1.4510595358224017e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:34:27<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5133, 'grad_norm': 18.896930694580078, 'learning_rate': 1.4006054490413725e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:35:03<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.638, 'grad_norm': 24.572006225585938, 'learning_rate': 1.3501513622603433e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:35:38<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5736, 'grad_norm': 10.499712944030762, 'learning_rate': 1.299697275479314e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:36:14<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6731, 'grad_norm': 7.593390941619873, 'learning_rate': 1.2492431886982847e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:36:49<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6114, 'grad_norm': 10.501842498779297, 'learning_rate': 1.1987891019172555e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:37:25<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6425, 'grad_norm': 15.055221557617188, 'learning_rate': 1.1483350151362261e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:38:01<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5472, 'grad_norm': 9.683588981628418, 'learning_rate': 1.0978809283551967e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:39:07<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-522\n",
      "Configuration saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-522\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.68370121717453, 'eval_accuracy': 0.7232142857142857, 'eval_f1_macro': 0.6511428408957771, 'eval_f1_weighted': 0.7118575826646774, 'eval_f1_0': 0.6490765171503958, 'eval_f1_1': 0.800702370500439, 'eval_f1_2': 0.5036496350364964, 'eval_runtime': 34.7949, 'eval_samples_per_second': 25.751, 'eval_steps_per_second': 0.805, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-522\\model.safetensors\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:39:20<00:07,  1.41s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5174, 'grad_norm': 10.397738456726074, 'learning_rate': 1.0474268415741675e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:39:56<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3792, 'grad_norm': 14.449845314025879, 'learning_rate': 9.969727547931384e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:40:32<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4463, 'grad_norm': 23.52960968017578, 'learning_rate': 9.46518668012109e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:41:07<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.395, 'grad_norm': 16.761066436767578, 'learning_rate': 8.960645812310798e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:41:42<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4847, 'grad_norm': 9.536358833312988, 'learning_rate': 8.456104944500505e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:42:18<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3207, 'grad_norm': 11.95839786529541, 'learning_rate': 7.951564076690212e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:42:53<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3561, 'grad_norm': 8.449557304382324, 'learning_rate': 7.44702320887992e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:43:28<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4285, 'grad_norm': 15.550485610961914, 'learning_rate': 6.942482341069627e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:44:05<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3792, 'grad_norm': 39.58838653564453, 'learning_rate': 6.437941473259335e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:44:41<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3771, 'grad_norm': 21.7515869140625, 'learning_rate': 5.933400605449042e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:45:16<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3013, 'grad_norm': 38.92605972290039, 'learning_rate': 5.428859737638749e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      " 74%|███████▎  | 14/19 [3:46:02<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-783\n",
      "Configuration saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-783\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7481830716133118, 'eval_accuracy': 0.7220982142857143, 'eval_f1_macro': 0.6853123009143359, 'eval_f1_weighted': 0.7267058356694854, 'eval_f1_0': 0.6666666666666666, 'eval_f1_1': 0.7881773399014779, 'eval_f1_2': 0.6010928961748634, 'eval_runtime': 34.417, 'eval_samples_per_second': 26.034, 'eval_steps_per_second': 0.814, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-783\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_norm_allegro\\herbert-base-cased\\checkpoint-261] due to args.save_total_limit\n",
      "\n",
      " 74%|███████▎  | 14/19 [3:46:36<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3064, 'grad_norm': 9.900237083435059, 'learning_rate': 4.924318869828457e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:47:11<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2051, 'grad_norm': 7.91973352432251, 'learning_rate': 4.419778002018164e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:47:47<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2846, 'grad_norm': 33.42485809326172, 'learning_rate': 3.915237134207871e-06, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:48:23<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2219, 'grad_norm': 10.530095100402832, 'learning_rate': 3.4106962663975787e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:48:59<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1819, 'grad_norm': 36.138092041015625, 'learning_rate': 2.906155398587286e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:49:34<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1495, 'grad_norm': 22.783309936523438, 'learning_rate': 2.401614530776993e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:50:10<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2126, 'grad_norm': 13.325297355651855, 'learning_rate': 1.8970736629667005e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:50:46<00:07,  1.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2128, 'grad_norm': 14.419760704040527, 'learning_rate': 1.3925327951564077e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:51:21<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1528, 'grad_norm': 23.838537216186523, 'learning_rate': 8.879919273461152e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▎  | 14/19 [3:51:57<00:07,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1664, 'grad_norm': 59.500118255615234, 'learning_rate': 3.8345105953582244e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n",
      "Model weights saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "Deleting older checkpoint [results_No_processing_norm_allegro\\herbert-base-cased\\checkpoint-522] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      " 74%|███████▎  | 14/19 [3:53:08<00:07,  1.41s/it]\n",
      "\u001b[ASaving model checkpoint to ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\n",
      "Configuration saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8021496534347534, 'eval_accuracy': 0.7176339285714286, 'eval_f1_macro': 0.6670503173208534, 'eval_f1_weighted': 0.7167072058511741, 'eval_f1_0': 0.6633906633906634, 'eval_f1_1': 0.7894736842105263, 'eval_f1_2': 0.5482866043613707, 'eval_runtime': 35.0264, 'eval_samples_per_second': 25.581, 'eval_steps_per_second': 0.799, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-1044\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing_norm_allegro/herbert-base-cased\\checkpoint-783 (score: 0.6853123009143359).\n",
      "\n",
      "100%|██████████| 1044/1044 [27:52<00:00,  1.60s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 896\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1672.2361, 'train_samples_per_second': 4.995, 'train_steps_per_second': 0.624, 'train_loss': 0.5210267323643768, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:34<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7276785714285714, 'macro_f1': 0.6841980396779271, 'weighted_f1': 0.7314420046522663, 'neutral_f1': 0.6788511749347258, 'positive_f1': 0.8007699711260827, 'negative_f1': 0.572972972972973, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_http', 'accuracy': 0.6931690929451287, 'macro_f1': 0.6639107915728655, 'weighted_f1': 0.696372811943066, 'neutral_f1': 0.6318407960199005, 'positive_f1': 0.7574819401444789, 'negative_f1': 0.6024096385542169, 'epochs': 3.9923224568138194}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_hashtag', 'accuracy': 0.7265625, 'macro_f1': 0.6735278066278153, 'weighted_f1': 0.7234917057108134, 'neutral_f1': 0.6770833333333334, 'positive_f1': 0.7963302752293578, 'negative_f1': 0.5471698113207547, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_mention', 'accuracy': 0.7162011173184357, 'macro_f1': 0.666518159846707, 'weighted_f1': 0.7134968633710488, 'neutral_f1': 0.6134020618556701, 'positive_f1': 0.7892293407613742, 'negative_f1': 0.5969230769230769, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_cashtag', 'accuracy': 0.7388392857142857, 'macro_f1': 0.690386324190106, 'weighted_f1': 0.7379474003618675, 'neutral_f1': 0.696969696969697, 'positive_f1': 0.8071161048689138, 'negative_f1': 0.5670731707317073, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing__text', 'accuracy': 0.7165178571428571, 'macro_f1': 0.6586076231123302, 'weighted_f1': 0.7121867109449609, 'neutral_f1': 0.6579634464751958, 'positive_f1': 0.7905282331511839, 'negative_f1': 0.5273311897106109, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing__rep', 'accuracy': 0.71875, 'macro_f1': 0.6744246469570054, 'weighted_f1': 0.7209168858400653, 'neutral_f1': 0.6666666666666666, 'positive_f1': 0.789272030651341, 'negative_f1': 0.5673352435530086, 'epochs': 4.0}, {'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing_norm', 'accuracy': 0.7220982142857143, 'macro_f1': 0.6853123009143359, 'weighted_f1': 0.7267058356694854, 'neutral_f1': 0.6666666666666666, 'positive_f1': 0.7881773399014779, 'negative_f1': 0.6010928961748634, 'epochs': 4.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Main comparison loop\n",
    "results = []\n",
    "\n",
    "for key, tokenized_dataset in tokenized_datasets.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing preprocessing variant: {key}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f'Training model: {model}')\n",
    "        metrics = train_and_evaluate(key, tokenized_dataset,model)\n",
    "        \n",
    "        results.append({\n",
    "            \"model\":model,\n",
    "            \"preprocessing\": key,\n",
    "            \"accuracy\": metrics[\"eval_accuracy\"],\n",
    "            \"macro_f1\": metrics[\"eval_f1_macro\"],\n",
    "            \"weighted_f1\": metrics[\"eval_f1_weighted\"],\n",
    "            \"neutral_f1\": metrics[\"eval_f1_0\"],\n",
    "            \"positive_f1\": metrics[\"eval_f1_1\"],\n",
    "            \"negative_f1\": metrics[\"eval_f1_2\"],\n",
    "            \"epochs\": metrics[\"epoch\"]\n",
    "        })\n",
    "\n",
    "        print(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with variant {key}: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(rf\"preprocessing_comparison_general_label_preprocessing_{model}.csv\"), exist_ok=True)\n",
    "\n",
    "# Now save the CSV\n",
    "results_df.to_csv(rf\"preprocessing_comparison_general_label_preprocessing_{model}.csv\", index=False)\n",
    "print(\"\\nComparison saved to preprocessing_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"preprocessing_comparison_general_label_sdas_parts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing</td>\n",
       "      <td>0.727679</td>\n",
       "      <td>0.684198</td>\n",
       "      <td>0.731442</td>\n",
       "      <td>0.678851</td>\n",
       "      <td>0.800770</td>\n",
       "      <td>0.572973</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_http</td>\n",
       "      <td>0.693169</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.696373</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>0.757482</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>3.992322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_hashtag</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.673528</td>\n",
       "      <td>0.723492</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_mention</td>\n",
       "      <td>0.716201</td>\n",
       "      <td>0.666518</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>0.613402</td>\n",
       "      <td>0.789229</td>\n",
       "      <td>0.596923</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_cashtag</td>\n",
       "      <td>0.738839</td>\n",
       "      <td>0.690386</td>\n",
       "      <td>0.737947</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.807116</td>\n",
       "      <td>0.567073</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing__text</td>\n",
       "      <td>0.716518</td>\n",
       "      <td>0.658608</td>\n",
       "      <td>0.712187</td>\n",
       "      <td>0.657963</td>\n",
       "      <td>0.790528</td>\n",
       "      <td>0.527331</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing__rep</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.674425</td>\n",
       "      <td>0.720917</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.789272</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>No_processing_norm</td>\n",
       "      <td>0.722098</td>\n",
       "      <td>0.685312</td>\n",
       "      <td>0.726706</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>0.601093</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model          preprocessing  accuracy  macro_f1  \\\n",
       "0  allegro/herbert-base-cased          No_processing  0.727679  0.684198   \n",
       "1  allegro/herbert-base-cased     No_processing_http  0.693169  0.663911   \n",
       "2  allegro/herbert-base-cased  No_processing_hashtag  0.726562  0.673528   \n",
       "3  allegro/herbert-base-cased  No_processing_mention  0.716201  0.666518   \n",
       "4  allegro/herbert-base-cased  No_processing_cashtag  0.738839  0.690386   \n",
       "5  allegro/herbert-base-cased    No_processing__text  0.716518  0.658608   \n",
       "6  allegro/herbert-base-cased     No_processing__rep  0.718750  0.674425   \n",
       "7  allegro/herbert-base-cased     No_processing_norm  0.722098  0.685312   \n",
       "\n",
       "   weighted_f1  neutral_f1  positive_f1  negative_f1    epochs  \n",
       "0     0.731442    0.678851     0.800770     0.572973  4.000000  \n",
       "1     0.696373    0.631841     0.757482     0.602410  3.992322  \n",
       "2     0.723492    0.677083     0.796330     0.547170  4.000000  \n",
       "3     0.713497    0.613402     0.789229     0.596923  4.000000  \n",
       "4     0.737947    0.696970     0.807116     0.567073  4.000000  \n",
       "5     0.712187    0.657963     0.790528     0.527331  4.000000  \n",
       "6     0.720917    0.666667     0.789272     0.567335  4.000000  \n",
       "7     0.726706    0.666667     0.788177     0.601093  4.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search using best preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results_robust.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file E:\\Project\\Repo_Classification\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased\\checkpoint-1192\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file E:\\Project\\Repo_Classification\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased\\checkpoint-1192\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at E:\\Project\\Repo_Classification\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased\\checkpoint-1192.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:43<00:00,  2.58it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.89      0.87       190\n",
      "     Neutral       0.95      0.91      0.93       535\n",
      "    Positive       0.83      0.89      0.86       171\n",
      "\n",
      "    accuracy                           0.90       896\n",
      "   macro avg       0.88      0.90      0.89       896\n",
      "weighted avg       0.91      0.90      0.90       896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = r'E:\\Project\\Repo_Classification\\PolishTweetsClassification\\results_No_processing_allegro\\herbert-base-cased\\checkpoint-1192'\n",
    "\n",
    "tokenized_dataset = tokenized_datasets['No_processing']\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Create and display confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:35<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions with probabilities:\n",
      "Example 0:\n",
      "Negative prob: 0.0585\n",
      "Neutral prob: 0.9265\n",
      "Positive prob: 0.0149\n",
      "Predicted class: 1\n",
      "\n",
      "Example 1:\n",
      "Negative prob: 0.0070\n",
      "Neutral prob: 0.9882\n",
      "Positive prob: 0.0047\n",
      "Predicted class: 1\n",
      "\n",
      "Example 2:\n",
      "Negative prob: 0.6083\n",
      "Neutral prob: 0.3718\n",
      "Positive prob: 0.0200\n",
      "Predicted class: 0\n",
      "\n",
      "Example 3:\n",
      "Negative prob: 0.0054\n",
      "Neutral prob: 0.9887\n",
      "Positive prob: 0.0059\n",
      "Predicted class: 1\n",
      "\n",
      "Example 4:\n",
      "Negative prob: 0.9544\n",
      "Neutral prob: 0.0265\n",
      "Positive prob: 0.0191\n",
      "Predicted class: 0\n",
      "\n",
      "Average confidence score: 0.9236\n",
      "\n",
      "Number of uncertain predictions: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions (these are logits)\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "raw_logits = predictions.predictions\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "softmax_probs = F.softmax(torch.tensor(raw_logits), dim=1).numpy()\n",
    "\n",
    "# Now you can:\n",
    "# 1. Print probabilities for individual predictions\n",
    "print(\"First 5 predictions with probabilities:\")\n",
    "for i in range(5):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Negative prob: {softmax_probs[i][0]:.4f}\")\n",
    "    print(f\"Neutral prob: {softmax_probs[i][1]:.4f}\")\n",
    "    print(f\"Positive prob: {softmax_probs[i][2]:.4f}\")\n",
    "    print(f\"Predicted class: {np.argmax(softmax_probs[i])}\")\n",
    "    print()\n",
    "\n",
    "# 2. Get average confidence scores\n",
    "avg_confidence = np.mean(np.max(softmax_probs, axis=1))\n",
    "print(f\"Average confidence score: {avg_confidence:.4f}\")\n",
    "\n",
    "# 3. Find highly uncertain predictions (where max probability is below a threshold)\n",
    "uncertainty_threshold = 0.7\n",
    "uncertain_preds = np.where(np.max(softmax_probs, axis=1) < uncertainty_threshold)[0]\n",
    "print(f\"\\nNumber of uncertain predictions: {len(uncertain_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of maximum probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.max(softmax_probs, axis=1), bins=50)\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2088\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 896\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 896\n",
      "  Batch size = 8\n",
      "100%|██████████| 112/112 [00:34<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Highly confident misclassifications:\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MarcinBanas3 @Felipe___666 @AdamAdam4949 Dla tego Allegro mogło dostać po dupie, a Orlenu nie wolno dotknąć. Bo pięknie rżnie takich jak ja. Niemiec 5,40, ja 6,78/l. By udziałowcy byli zadowoleni, szczególnie większościowi. To jest zorganizowana mafia. https://t.co/BTNYu8gwh4\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9161\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Suilerua_ Pisałem, żeby trzymać to ccc\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9885\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Hello InPost @InPostPL od rana czekam na przesyłkę a tu jeszcze lepszy zong teraz się pokazał... Gratuluję organizacji pracy.... https://t.co/47mtmV00IC\n",
      "True label: Negative\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9675\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @m_gdula @__Lewica Pan w ogóle nie rozumie na czym polega problem. CI którzy nie płacą i tak nie zapłacą a obciąży Pan dodatkowo taki Inpost. Ma Pan chociaż kawałem mózgu ? Bo ta propozycja sugeruje, że nie bardzo.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9495\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MakrelG Kurwa ludzie sprzedają CDR bo jakaś gówna gra , wogole niekojarzonej firmy globalnie - nie wypaliła. Takie rzeczy tylko na Bananie xD jedna z najlepszych firm na świecie traci 5% czyli ponad połowę kapitalizacji 11bit bo tamtym gra nie wypaliła. Ludzie są debilami z pieluchami\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9745\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @RoyalNavvy inpost&gt;&gt;&gt; wszystko inne\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9871\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Jeszcze w tym roku znana platforma zakupowa ma uruchomić czeskojęzyczny serwis https://t.co/exyDHXXqUf. Marcin Półchłopek z Allegro opowiada Business Insiderowi, jakie firma ma plany – i co z tego wynika dla polskiego użytkownika. https://t.co/Op4cVmx3rY\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9532\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Wilk_z_GPW a jak tu okazja na xtb? to tylko moze spadac w dol\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9722\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Konsolowe wydania Sonic Superstars taniej na Allegro. Ceny zaczynają się od 176,66 zł https://t.co/bqtItp5VVk\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9664\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: #CCC Ultro obejmie w ofercie conajmniej 5 mln akcji, co daje utrzymanie proporcji w głosach i efektywnie objęcie akcji za conajmniej 194 mln PLN. Czyli dla innych inwestorów zostaje mniej\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9523\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @amthreehu ale w inpost tez nie trzeba drukować :( ludzie to chujki\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9083\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @bocianovvsky Jak nie trzymasz w pudełkach to spoko są te gablotki z Allegro. Ja narazie trzymam w pudełkach. https://t.co/pNVO8fGAiJ\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9146\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Dostałem dzisiaj buty z CCC. Zastanawiam się czy oni nie pomylili adresu. Może miały być dostarczone na Nowogejowską ? Od lat kupuję przez Internet, czegoś takiego jeszcze nie miałem. 😡🤬😡🤬 https://t.co/DKQuPEqQGq\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9390\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @InPostPL prosze inpost tak bardzo Was lubie\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9210\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @bednarow_t @MarcinBrixen Ja słyszałem, że te wszystkie paczki inpostu to Kaczyński i Morawiecki roznoszą nocami i to dzięki nim inpost osiągnął taki sukces\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9061\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Mysterybuzu Nie dość, że luksemburscy właściciele Heket Topco S.à r.l. oraz PG Investment Company 1113B S.à r.l. zachowali kontrolę nad żabką to jeszcze zgarnęli z polskiego rynku ponad 7 miliardów zł. https://t.co/DlrGDUhu3W\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9244\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @InPostPL @JOlkiewicz widziales, najpierw STS teraz inpost ;(\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9445\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @buniabuba Jeśli ktoś chce się tak pobawić, to polecam kupno zestawu do szycia chirurgicznego na allegro (igła, nici, pęsetą, te \"dzikie\" nożyczki) oraz świńskiej lub drobiowej nogi, ewentualnie kurzy cycek\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9851\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @moraw_m Kupuję materac za 700 zł u producenta ..przesyłka 300 zł !!!. Więc kupuje ten sam materac u producenta, na Allegro...przesyłka gratis... i jak ich wspierać?\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9787\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @Jandor4119 @ewale21 Dla pocieszenia UNITRA została reaktywowaną inwestor osoba CD Projekt będą polskie wierzę dla audiofili\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9134\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @marcinwarchol Warchoł, robisz z siebie mało rozgarniętą osobę, a może nią jesteś? Doktorat zakupiłeś na allegro.lokalnie?\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9763\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @doktor_90_fx @ZofiaGambetti 11% straty to żaden wstyd. Zdarza się. A grillowac nie zamierzam, nie mój styl, czasami się wygłupiam i tyle. Twoje życie, twoje decyzje. A CDR i tak odjeżdża ;))))\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9440\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Dzisiaj na otwarciu sprzedane #GRX z zyskiem 37% co pokryło stratę z 11BIT A do prywatnego portfela za GRX trafiło #SNT po 171zł 😉 https://t.co/Ie68ZrEnj9\n",
      "True label: Negative\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9806\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @ewa_szczesniak Wiemy o tym. Ceny ze wszystkim szaleją. Nawozy na allegro. Ale też gnojówki robię. Opryski robię z jodyny, sody. Na aksamitkach w tym roku zaoszczędzilam, bo zrobiłam ponad 300 sadzonek 😁.\n",
      "True label: Positive\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9092\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @KIRIMASU_MMA @PiotrWiel @Allegro_Group @zrzutkapl @GrzegorzBraun_ No i co chciałeś przez to powiedzieć? Że jesteś jebaćpisem? Po pierwsze nie ty jeden, po drugie nikogo to nie interesuje.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9544\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @natnwyd Gll i ccc mam nadzieję bo rozjebiecie\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9381\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @MakrelG Całe szczęście że zacząłem sprzedaż polskich spółek na USA przy dolarze za 4 PLN. Zostały mi tylko te które zarabiają zagranicą. Livechat, Toya, Autopartner. Dobrałem CCC. Nieprędko wrócę na GPW kapitałem. Powodzenia wszystkim\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9765\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @ilar1958 W sumie dobrze, nic mnie tak nie wkurwia jak PP. Jak Sasin się za to zabiera to znaczy, że rychło upadnie. Może inpost z rządowym kontraktem na usługi pocztowe posprząta ten burdel.\n",
      "True label: Positive\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9406\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @EwuniaXYZ Poszło😁a z allegro cisza w sprawie puszek.\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9704\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Garść wiadomości z obozu Sony i Microsoftu oraz zwolnienia w CD Projekt RED. Te i wiele innych newsów w tym wydaniu Niecodziennika Okiem Deva. Zapraszam! https://t.co/R7gkKWIB2G https://t.co/Lg6fT0rLja #okiemdeva #gamedev #gamedevelopment #newsy #niecodziennik\n",
      "True label: Negative\n",
      "Predicted (wrong): Neutral\n",
      "Confidence: 0.9849\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: Wiecie dlaczego #Żabka cofnęła się do IPO price? Otóż, “Kurs przebił 23.6% zniesienia Fibonaciego” 👀🙃 Wiara w 🪄 magiczne poziomy technicznej analizy wciąż potrafi mnie zaskakiwać ;)\n",
      "True label: Neutral\n",
      "Predicted (wrong): Positive\n",
      "Confidence: 0.9316\n",
      "\n",
      "====================================================================================================\n",
      "Tweet: @rrobertn @OrsonDzi @InPostPL @Allegro_Group Ja mialam taka sytuacje jak paczka byla rozwalona 🙃 Woec skontaktowalam sie ze sprzedawca i on od razu zweryfikowal co sie dzieje (zepsucie paczki) i nadal nowa\n",
      "True label: Neutral\n",
      "Predicted (wrong): Negative\n",
      "Confidence: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions and convert to probabilities\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "raw_logits = predictions.predictions\n",
    "softmax_probs = F.softmax(torch.tensor(raw_logits), dim=1).numpy()\n",
    "\n",
    "# Load original test dataset that contains the text\n",
    "# You'll need to provide the original dataset that was used for tokenization\n",
    "\n",
    "# Create a DataFrame with texts and probabilities\n",
    "results_df = pd.DataFrame({\n",
    "    'text': test_df['text'].values,  # assuming text column is named 'text'\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': np.argmax(softmax_probs, axis=1),\n",
    "    'negative_prob': softmax_probs[:, 0],\n",
    "    'neutral_prob': softmax_probs[:, 1],\n",
    "    'positive_prob': softmax_probs[:, 2],\n",
    "    'confidence': np.max(softmax_probs, axis=1)\n",
    "})\n",
    "\n",
    "# Function to get label name\n",
    "def get_label_name(label_id):\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    return labels[label_id]\n",
    "\n",
    "# Add readable labels\n",
    "results_df['true_label_name'] = results_df['true_label'].apply(get_label_name)\n",
    "results_df['predicted_label_name'] = results_df['predicted_label'].apply(get_label_name)\n",
    "\n",
    "\n",
    "# Show high confidence mistakes\n",
    "confidence_threshold = 0.9\n",
    "misclassified = results_df[\n",
    "    (results_df['true_label'] != results_df['predicted_label']) & \n",
    "    (results_df['confidence'] > confidence_threshold)\n",
    "]\n",
    "\n",
    "print(\"\\nHighly confident misclassifications:\")\n",
    "for idx, row in misclassified.head(100).iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Tweet: {row['text']}\")\n",
    "    print(f\"True label: {row['true_label_name']}\")\n",
    "    print(f\"Predicted (wrong): {row['predicted_label_name']}\")\n",
    "    print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "\n",
    "# Save detailed results to CSV\n",
    "results_df.to_csv('predictions_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "misclassified_samples = {\n",
    "    \"Index\": misclassified_indices,\n",
    "    \"True Label\": true_labels[misclassified_indices],\n",
    "    \"Predicted Label\": predicted_labels[misclassified_indices],\n",
    "    \"Text\": [tokenizer.decode(tokenized_dataset[\"test\"][\"input_ids\"][i], skip_special_tokens=True) for i in misclassified_indices]\n",
    "}\n",
    "df_misclassified = pd.DataFrame(misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyperclipWindowsException",
     "evalue": "Error calling OpenClipboard ([WinError 0] Operacja ukończona pomyślnie.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPyperclipWindowsException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_misclassified\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_clipboard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3247\u001b[0m, in \u001b[0;36mNDFrame.to_clipboard\u001b[1;34m(self, excel, sep, **kwargs)\u001b[0m\n\u001b[0;32m   3180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3181\u001b[0m \u001b[38;5;124;03mCopy object to the system clipboard.\u001b[39;00m\n\u001b[0;32m   3182\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m   pyperclip.copy(html)\u001b[39;00m\n\u001b[0;32m   3244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3245\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clipboards\n\u001b[1;32m-> 3247\u001b[0m \u001b[43mclipboards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_clipboard\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\io\\clipboards.py:178\u001b[0m, in \u001b[0;36mto_clipboard\u001b[1;34m(obj, excel, sep, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     obj\u001b[38;5;241m.\u001b[39mto_csv(buf, sep\u001b[38;5;241m=\u001b[39msep, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m     text \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m--> 178\u001b[0m     \u001b[43mclipboard_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\io\\clipboard\\__init__.py:659\u001b[0m, in \u001b[0;36mlazy_load_stub_copy\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m copy, paste\n\u001b[0;32m    658\u001b[0m copy, paste \u001b[38;5;241m=\u001b[39m determine_clipboard()\n\u001b[1;32m--> 659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\io\\clipboard\\__init__.py:470\u001b[0m, in \u001b[0;36minit_windows_clipboard.<locals>.copy_windows\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    462\u001b[0m text \u001b[38;5;241m=\u001b[39m _stringifyText(text)  \u001b[38;5;66;03m# Converts non-str values to str.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m window() \u001b[38;5;28;01mas\u001b[39;00m hwnd:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# http://msdn.com/ms649048\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# If an application calls OpenClipboard with hwnd set to NULL,\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# EmptyClipboard sets the clipboard owner to NULL;\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# this causes SetClipboardData to fail.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# => We need a valid hwnd to copy something.\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m clipboard(hwnd):\n\u001b[0;32m    471\u001b[0m         safeEmptyClipboard()\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[0;32m    474\u001b[0m             \u001b[38;5;66;03m# http://msdn.com/ms649051\u001b[39;00m\n\u001b[0;32m    475\u001b[0m             \u001b[38;5;66;03m# If the hMem parameter identifies a memory object,\u001b[39;00m\n\u001b[0;32m    476\u001b[0m             \u001b[38;5;66;03m# the object must have been allocated using the\u001b[39;00m\n\u001b[0;32m    477\u001b[0m             \u001b[38;5;66;03m# function with the GMEM_MOVEABLE flag.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Project\\.venv\\Lib\\site-packages\\pandas\\io\\clipboard\\__init__.py:451\u001b[0m, in \u001b[0;36minit_windows_clipboard.<locals>.clipboard\u001b[1;34m(hwnd)\u001b[0m\n\u001b[0;32m    449\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyperclipWindowsException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError calling OpenClipboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[1;31mPyperclipWindowsException\u001b[0m: Error calling OpenClipboard ([WinError 0] Operacja ukończona pomyślnie.)"
     ]
    }
   ],
   "source": [
    "df_misclassified.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tweet Research",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
