{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pl_core_news_lg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from functions import *\n",
    "from evaluation_functions import *\n",
    "import pandas as pd\n",
    "tool = language_tool_python.LanguageTool('pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_1 = pd.read_csv('TrainingData/annotation_dataset - general_label.csv',index_col=0)\n",
    "# dataset_2 = pd.read_csv('TrainingData/annotation_dataset - annotation_second_round_labeled.csv',index_col=0)\n",
    "\n",
    "# dataset_1 = dataset_1[['text','Overall']]\n",
    "# dataset_1 =  dataset_1.rename(columns={'Overall':'labels'})\n",
    "\n",
    "# dataset_2 = dataset_2[['text','label']]\n",
    "# dataset_2 =  dataset_2.rename(columns={'label':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.read_csv(r'TrainingData\\dataset_labeled.csv', nrows=0).columns.tolist()\n",
    "\n",
    "columns_to_exclude = ['Unnamed: 0','Unnamed: 0.1' , 'Unnamed: 0.2']  # example columns to skip\n",
    "wanted_columns = [col for col in all_columns if col not in columns_to_exclude]\n",
    "\n",
    "dataset_labeled = pd.read_csv(r'TrainingData\\dataset_labeled.csv', usecols=wanted_columns)\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='labels'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 77.0\n",
      "Max tokens: 221\n"
     ]
    }
   ],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "The first part was training the model with different basic preprocessing strategies.\n",
    "Then we evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing http\n",
    "def preprocess_tweet_https(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    return tweet\n",
    "\n",
    "#Removing hashtags\n",
    "def preprocess_tweet_hashtag(tweet):\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing mentions\n",
    "def preprocess_tweet_mention(tweet):\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing cashtag\n",
    "def preprocess_tweet_cashtag(tweet):\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing all charatcters except polish letter and ? !\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing repeated letters\n",
    "def preprocess_tweet_rep(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing white spaces\n",
    "def preprocess_tweet_norm(tweet):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "#Normalizing caps\n",
    "def preprocess_caps(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(^|[.!?]\\s+)(\\w)', lambda m: m.group(1) + m.group(2).upper(), tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_tco(tweet):\n",
    "    return re.sub(r\"https?://t\\.co/\\S+\", \"\", tweet).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = preprocess_tweet_mention(tweet)\n",
    "    tweet = preprocess_tweet_cashtag(tweet)\n",
    "    tweet = preprocess_tweet_hashtag(tweet)\n",
    "    tweet = replace_emoji(tweet)\n",
    "    tweet = preprocess_tweet_rep(tweet)\n",
    "    tweet = preprocess_caps(tweet)\n",
    "    tweet = preprocess_tweet_norm(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipelines\n",
    "basic_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_emoji': [\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'No_processing_http': [\n",
    "        preprocess_tweet_https\n",
    "    ],\n",
    "    'No_processing_hashtag': [\n",
    "        preprocess_tweet_hashtag\n",
    "    ],\n",
    "    'No_processing_mention': [\n",
    "        preprocess_tweet_mention\n",
    "    ],\n",
    "    'No_processing_cashtag': [\n",
    "        preprocess_tweet_cashtag\n",
    "    ],\n",
    "    'No_processing__text': [\n",
    "        preprocess_tweet_text\n",
    "    ],\n",
    "    'No_processing__rep': [\n",
    "        preprocess_tweet_rep\n",
    "    ],\n",
    "    'No_processing_norm': [\n",
    "        preprocess_tweet_norm\n",
    "    ],\n",
    "    'No_processing_caps': [\n",
    "        preprocess_caps\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROBERT', 'HERBERT', 'POLBERT', 'MBERT']\n"
     ]
    }
   ],
   "source": [
    "tested_models = {}\n",
    "tested_models['ROBERT'] = \"sdadas/polish-roberta-base-v2\"\n",
    "tested_models['HERBERT']  = \"allegro/herbert-base-cased\"\n",
    "tested_models['POLBERT']  = \"dkleczek/bert-base-polish-cased-v1\"\n",
    "tested_models['MBERT'] = 'google-bert/bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best basic data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData/processed_data_No_processing.csv\n",
      "TrainingData/processed_data_No_processing_emoji.csv\n",
      "TrainingData/processed_data_No_processing_http.csv\n",
      "TrainingData/processed_data_No_processing_hashtag.csv\n",
      "TrainingData/processed_data_No_processing_mention.csv\n",
      "TrainingData/processed_data_No_processing_cashtag.csv\n",
      "TrainingData/processed_data_No_processing__text.csv\n",
      "TrainingData/processed_data_No_processing__rep.csv\n",
      "TrainingData/processed_data_No_processing_norm.csv\n",
      "TrainingData/processed_data_No_processing_caps.csv\n",
      "\n",
      "========================================\n",
      "Testing seed: 470\n",
      "\n",
      "========================================\n",
      "Testing model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9764dee5c47c431a882ffae3c088b549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d12909c76654616aca4797ce82a4bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sdadas/polish-roberta-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50001\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sdadas/polish-roberta-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/polish-roberta-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 17:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.717029</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.531585</td>\n",
       "      <td>0.668893</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.820946</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.598528</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.763102</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.581064</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.712441</td>\n",
       "      <td>0.771311</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.837945</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_470\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_sdadas/polish-roberta-base-v2_470\\checkpoint-297 (score: 0.7124405996172644).\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_470\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_sdad_470_No_processing_20250525_130948.csv\n",
      "[{'model': 'sdadas/polish-roberta-base-v2', 'preprocessing': 'No_processing', 'accuracy': 0.7721518987341772, 'macro_f1': 0.7124405996172644, 'weighted_f1': 0.7713108899526905, 'neutral_f1': 0.6962025316455697, 'positive_f1': 0.8379446640316205, 'negative_f1': 0.6031746031746031, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_sdad.csv\n",
      "\n",
      "========================================\n",
      "Testing model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998b43a8c16940b6a1e6efc9a4abf7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830b7db9d5344dae9b9db5bfc31fd55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 17:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.823643</td>\n",
       "      <td>0.675949</td>\n",
       "      <td>0.432847</td>\n",
       "      <td>0.599829</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.792144</td>\n",
       "      <td>0.149254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.591704</td>\n",
       "      <td>0.769620</td>\n",
       "      <td>0.690527</td>\n",
       "      <td>0.760250</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.838590</td>\n",
       "      <td>0.556522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.559156</td>\n",
       "      <td>0.789873</td>\n",
       "      <td>0.740939</td>\n",
       "      <td>0.791833</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_470\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_allegro/herbert-base-cased_470\\checkpoint-297 (score: 0.7409392775246434).\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_470\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_alle_470_No_processing_20250525_132824.csv\n",
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.789873417721519, 'macro_f1': 0.7409392775246434, 'weighted_f1': 0.7918330511723534, 'neutral_f1': 0.761904761904762, 'positive_f1': 0.8455284552845528, 'negative_f1': 0.6153846153846154, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_alle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996ed6f6e9cb4d428efd727114d5e227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b414611e78149d2887225a875cbdb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1996191a3b416c96c2db3fabbe7309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/489k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ac6b4b0cd0443ca5fde5f836e5fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb00cbc44dd4bce8e24de8779c66931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1adfd38fc944d84a4857bf7412c3d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42386f9973b14641b0a55b301c307c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dkleczek/bert-base-polish-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Attempting to create safetensors variant\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 132,123,651\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f630a1d80496bbf754e8209a6d840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 17:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.888300</td>\n",
       "      <td>0.726090</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.608793</td>\n",
       "      <td>0.706744</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.813765</td>\n",
       "      <td>0.396396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.633252</td>\n",
       "      <td>0.739241</td>\n",
       "      <td>0.656241</td>\n",
       "      <td>0.739057</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.830040</td>\n",
       "      <td>0.480620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.657391</td>\n",
       "      <td>0.749367</td>\n",
       "      <td>0.666953</td>\n",
       "      <td>0.748501</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_470\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_470\\checkpoint-297 (score: 0.6669525961850403).\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_470\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_dkle_470_No_processing_20250525_134754.csv\n",
      "[{'model': 'dkleczek/bert-base-polish-cased-v1', 'preprocessing': 'No_processing', 'accuracy': 0.7493670886075949, 'macro_f1': 0.6669525961850403, 'weighted_f1': 0.7485011982520992, 'neutral_f1': 0.6666666666666667, 'positive_f1': 0.8383233532934131, 'negative_f1': 0.49586776859504134, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_dkle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8b27ddd9fd4206a36c2446406c2999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900cb51bce4a46dcb9663b9ba9f00e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
      "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 177,855,747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 19:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.969300</td>\n",
       "      <td>0.835703</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.258799</td>\n",
       "      <td>0.491391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.781409</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>0.521763</td>\n",
       "      <td>0.645789</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.800738</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.675300</td>\n",
       "      <td>0.774299</td>\n",
       "      <td>0.665823</td>\n",
       "      <td>0.560080</td>\n",
       "      <td>0.663255</td>\n",
       "      <td>0.402439</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_470\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_google-bert/bert-base-multilingual-cased_470\\checkpoint-297 (score: 0.5600795869618774).\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_470\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_goog_470_No_processing_20250525_140826.csv\n",
      "[{'model': 'google-bert/bert-base-multilingual-cased', 'preprocessing': 'No_processing', 'accuracy': 0.6658227848101266, 'macro_f1': 0.5600795869618774, 'weighted_f1': 0.6632549531179006, 'neutral_f1': 0.40243902439024387, 'positive_f1': 0.7944664031620554, 'negative_f1': 0.4833333333333333, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_goog.csv\n",
      "\n",
      "========================================\n",
      "Testing seed: 432\n",
      "\n",
      "========================================\n",
      "Testing model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2966f7df1144309c1c0e2852961cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6545f00c07f64b48bf58a8d1087a8770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sdadas/polish-roberta-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50001\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sdadas/polish-roberta-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/polish-roberta-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 20:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>0.754829</td>\n",
       "      <td>0.688608</td>\n",
       "      <td>0.555984</td>\n",
       "      <td>0.647088</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>0.769539</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.605734</td>\n",
       "      <td>0.741772</td>\n",
       "      <td>0.699009</td>\n",
       "      <td>0.737123</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.793991</td>\n",
       "      <td>0.590164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.604830</td>\n",
       "      <td>0.749367</td>\n",
       "      <td>0.716859</td>\n",
       "      <td>0.748426</td>\n",
       "      <td>0.728155</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.626866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_432\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_sdadas/polish-roberta-base-v2_432\\checkpoint-297 (score: 0.716858855667724).\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_432\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_sdad_432_No_processing_20250525_143017.csv\n",
      "[{'model': 'sdadas/polish-roberta-base-v2', 'preprocessing': 'No_processing', 'accuracy': 0.7493670886075949, 'macro_f1': 0.716858855667724, 'weighted_f1': 0.7484262078245045, 'neutral_f1': 0.7281553398058253, 'positive_f1': 0.7955555555555556, 'negative_f1': 0.626865671641791, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_sdad.csv\n",
      "\n",
      "========================================\n",
      "Testing model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3e361e4564899b7bd2489fb1c6baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43572a30f944f6bb23a8ef56f16b977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 18:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.786360</td>\n",
       "      <td>0.653165</td>\n",
       "      <td>0.483463</td>\n",
       "      <td>0.595606</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.760512</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>0.661452</td>\n",
       "      <td>0.729114</td>\n",
       "      <td>0.713056</td>\n",
       "      <td>0.734461</td>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>0.654762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.630109</td>\n",
       "      <td>0.736709</td>\n",
       "      <td>0.710122</td>\n",
       "      <td>0.737949</td>\n",
       "      <td>0.706422</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.642336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_432\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_allegro/herbert-base-cased_432\\checkpoint-198 (score: 0.7130559957442185).\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_432\\checkpoint-297] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_alle_432_No_processing_20250525_144947.csv\n",
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7291139240506329, 'macro_f1': 0.7130559957442185, 'weighted_f1': 0.7344607758824889, 'neutral_f1': 0.7174887892376682, 'positive_f1': 0.7669172932330828, 'negative_f1': 0.6547619047619048, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_alle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf3a2ab086b4156a2692d2d41afab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8a99267d70450584201c9253bbdf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dkleczek/bert-base-polish-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Attempting to create safetensors variant\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 132,123,651\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 18:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.771715</td>\n",
       "      <td>0.665823</td>\n",
       "      <td>0.536063</td>\n",
       "      <td>0.626771</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.329897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.693940</td>\n",
       "      <td>0.716456</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.713835</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>0.792291</td>\n",
       "      <td>0.579310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.744219</td>\n",
       "      <td>0.701266</td>\n",
       "      <td>0.655823</td>\n",
       "      <td>0.699788</td>\n",
       "      <td>0.623116</td>\n",
       "      <td>0.772926</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_432\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_432\\checkpoint-198 (score: 0.6669383495101329).\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_432\\checkpoint-297] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_dkle_432_No_processing_20250525_150950.csv\n",
      "[{'model': 'dkleczek/bert-base-polish-cased-v1', 'preprocessing': 'No_processing', 'accuracy': 0.7164556962025317, 'macro_f1': 0.6669383495101329, 'weighted_f1': 0.7138353070278224, 'neutral_f1': 0.6292134831460675, 'positive_f1': 0.7922912205567452, 'negative_f1': 0.5793103448275863, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_dkle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f67c90856b4450a263d6989f6822fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0e285347d4f2691ac8a75bc42fd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 177,855,747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 28:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>0.924458</td>\n",
       "      <td>0.574684</td>\n",
       "      <td>0.243301</td>\n",
       "      <td>0.419464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.895059</td>\n",
       "      <td>0.625316</td>\n",
       "      <td>0.437477</td>\n",
       "      <td>0.565441</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.761726</td>\n",
       "      <td>0.101266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.871176</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.553213</td>\n",
       "      <td>0.616995</td>\n",
       "      <td>0.506787</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.429907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_432\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_google-bert/bert-base-multilingual-cased_432\\checkpoint-297 (score: 0.5532125317721799).\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_432\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_goog_432_No_processing_20250525_154009.csv\n",
      "[{'model': 'google-bert/bert-base-multilingual-cased', 'preprocessing': 'No_processing', 'accuracy': 0.6227848101265823, 'macro_f1': 0.5532125317721799, 'weighted_f1': 0.6169951388523285, 'neutral_f1': 0.5067873303167421, 'positive_f1': 0.722943722943723, 'negative_f1': 0.4299065420560747, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_goog.csv\n",
      "\n",
      "========================================\n",
      "Testing seed: 122\n",
      "\n",
      "========================================\n",
      "Testing model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df42528943c4ffcbd50e8ca31ece968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2533a8d288114721981f14f799a9ed11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sdadas/polish-roberta-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50001\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: sdadas/polish-roberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sdadas/polish-roberta-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/polish-roberta-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 20:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.984200</td>\n",
       "      <td>0.885795</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.382839</td>\n",
       "      <td>0.539052</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.155844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.695255</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.642469</td>\n",
       "      <td>0.711006</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.482143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.684814</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>0.654839</td>\n",
       "      <td>0.716270</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_122\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_sdadas/polish-roberta-base-v2_122\\checkpoint-297 (score: 0.6548385999240623).\n",
      "Deleting older checkpoint [results\\results_No_processing_sdadas\\polish-roberta-base-v2_122\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--sdadas--polish-roberta-base-v2\\snapshots\\de1a6641c65e5a774a50ec165e7544279f9da79f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_sdad_122_No_processing_20250525_160336.csv\n",
      "[{'model': 'sdadas/polish-roberta-base-v2', 'preprocessing': 'No_processing', 'accuracy': 0.7189873417721518, 'macro_f1': 0.6548385999240623, 'weighted_f1': 0.7162701588619409, 'neutral_f1': 0.6553672316384181, 'positive_f1': 0.7975206611570249, 'negative_f1': 0.5116279069767442, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_sdad.csv\n",
      "\n",
      "========================================\n",
      "Testing model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ef13f3e5324d9980301df82498f0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dd9ed8b38246b39a38d6febd8f4e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: allegro/herbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Safetensors PR exists\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 124,445,187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 27:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>0.846889</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.335224</td>\n",
       "      <td>0.511980</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.761438</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.737766</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.617986</td>\n",
       "      <td>0.672811</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.534091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>0.677938</td>\n",
       "      <td>0.729114</td>\n",
       "      <td>0.674343</td>\n",
       "      <td>0.728507</td>\n",
       "      <td>0.707865</td>\n",
       "      <td>0.796646</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_122\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_allegro/herbert-base-cased_122\\checkpoint-297 (score: 0.6743431297879746).\n",
      "Deleting older checkpoint [results\\results_No_processing_allegro\\herbert-base-cased_122\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_alle_122_No_processing_20250525_163345.csv\n",
      "[{'model': 'allegro/herbert-base-cased', 'preprocessing': 'No_processing', 'accuracy': 0.7291139240506329, 'macro_f1': 0.6743431297879746, 'weighted_f1': 0.728507266561651, 'neutral_f1': 0.7078651685393258, 'positive_f1': 0.7966457023060796, 'negative_f1': 0.5185185185185185, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_alle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5163a02b4944e45aff5affa75283d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207abd45770141f781e6a4ec6b5044be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: dkleczek/bert-base-polish-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n",
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dkleczek/bert-base-polish-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 132,123,651\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 30:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.862600</td>\n",
       "      <td>0.762818</td>\n",
       "      <td>0.660759</td>\n",
       "      <td>0.546013</td>\n",
       "      <td>0.638931</td>\n",
       "      <td>0.530387</td>\n",
       "      <td>0.763566</td>\n",
       "      <td>0.344086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.853501</td>\n",
       "      <td>0.637975</td>\n",
       "      <td>0.574545</td>\n",
       "      <td>0.643768</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.742489</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.832036</td>\n",
       "      <td>0.698734</td>\n",
       "      <td>0.624304</td>\n",
       "      <td>0.694122</td>\n",
       "      <td>0.612717</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>0.472441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_122\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_dkleczek/bert-base-polish-cased-v1_122\\checkpoint-297 (score: 0.6243042699761622).\n",
      "Deleting older checkpoint [results\\results_No_processing_dkleczek\\bert-base-polish-cased-v1_122\\checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--dkleczek--bert-base-polish-cased-v1\\snapshots\\fed744e81ebd16cf099b5c64c40688bc3e6ace67\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dkleczek/bert-base-polish-cased-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60000\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_dkle_122_No_processing_20250525_170618.csv\n",
      "[{'model': 'dkleczek/bert-base-polish-cased-v1', 'preprocessing': 'No_processing', 'accuracy': 0.6987341772151898, 'macro_f1': 0.6243042699761622, 'weighted_f1': 0.6941221594078693, 'neutral_f1': 0.6127167630057804, 'positive_f1': 0.7877551020408162, 'negative_f1': 0.4724409448818897, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_dkle.csv\n",
      "\n",
      "========================================\n",
      "Testing model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2685ea505a4df182f6bb9cfb15f9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc5796a3ad147d694a9e6c43a4c46dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n",
      "Training model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project_clean\\PolishTweetsClassification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 297\n",
      "  Number of trainable parameters = 177,855,747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 18:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.960591</td>\n",
       "      <td>0.612658</td>\n",
       "      <td>0.297233</td>\n",
       "      <td>0.484183</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.760835</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.873402</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.477845</td>\n",
       "      <td>0.585608</td>\n",
       "      <td>0.322148</td>\n",
       "      <td>0.744722</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.896651</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.467909</td>\n",
       "      <td>0.581388</td>\n",
       "      <td>0.352201</td>\n",
       "      <td>0.743833</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-99\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-99\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-99\\model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-198\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-198\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-198\\model.safetensors\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\\model.safetensors\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_122\\checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\n",
      "Configuration saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\\config.json\n",
      "Model weights saved in ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-297\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/results_No_processing_google-bert/bert-base-multilingual-cased_122\\checkpoint-198 (score: 0.4778453355776264).\n",
      "Deleting older checkpoint [results\\results_No_processing_google-bert\\bert-base-multilingual-cased_122\\checkpoint-297] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Unnamed: 0.2, Unnamed: 0.1. If Unnamed: 0.2, Unnamed: 0.1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved misclassified examples to ./errors/misclassified_goog_122_No_processing_20250525_172635.csv\n",
      "[{'model': 'google-bert/bert-base-multilingual-cased', 'preprocessing': 'No_processing', 'accuracy': 0.6075949367088608, 'macro_f1': 0.4778453355776264, 'weighted_f1': 0.5856084286653283, 'neutral_f1': 0.3221476510067114, 'positive_f1': 0.744721689059501, 'negative_f1': 0.36666666666666664, 'epochs': 3.0}]\n",
      "\n",
      "Comparison saved to preprocessing_comparison_model_goog.csv\n"
     ]
    }
   ],
   "source": [
    "datasets = transform_data(processing_pipelines = basic_processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "\n",
    "for seed in range(5):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Testing seed: {seed}\")\n",
    "    train_test_seed = seed\n",
    "    model_seed = seed\n",
    "    for model in tested_models.values():\n",
    "        tokenized_datasets = {}\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing model: {model}\")\n",
    "\n",
    "        for key, df in datasets.items():\n",
    "            tokenized_datasets[key] = prepare_datasets(df = df, model = model, test_size=test_size, random_state=train_test_seed, max_length=128, column='text')\n",
    "\n",
    "        for key,df in datasets.items():\n",
    "            results = run_evaluation(tokenized_datasets = tokenized_datasets, model = model, train_test_seed = train_test_seed, model_seed = model_seed)\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df['seed'] = seed\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        results_df.to_csv(rf\"./preprocessing_comparison/preprocessing_comparison_model_{model[:4]}_{train_test_seed}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_results(directory):\n",
    "    \"\"\"Compiles and processes results from CSV files in 'preprocessing_comparison' directory.\"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Filter and drop only if columns exist\n",
    "    if 'seed' in combined_df.columns:\n",
    "        combined_df = combined_df[combined_df['seed'] < 5]\n",
    "        combined_df.drop(['seed'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    combined_df.drop(['epochs'], axis=1, inplace=True, errors='ignore')\n",
    "    combined_df.sort_values(by=['model', 'preprocessing'], inplace=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "all_results = compile_results('preprocessing_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weighted_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "neutral_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "positive_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negative_f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "53b3b4f3-ddd9-4532-a86d-6b7bab2e4f4e",
       "rows": [
        [
         "3",
         "sdadas/polish-roberta-base-v2",
         "0.7454177215189874",
         "0.6930433520488936",
         "0.7405007364750914",
         "0.6908784672889965",
         "0.8125841131105977",
         "0.5756674757470865"
        ],
        [
         "0",
         "allegro/herbert-base-cased",
         "0.7373670886075949",
         "0.6889302454176035",
         "0.7335569564687989",
         "0.6743162752263433",
         "0.8029187586550289",
         "0.5895557023714385"
        ],
        [
         "1",
         "dkleczek/bert-base-polish-cased-v1",
         "0.7198481012658228",
         "0.6568022370886933",
         "0.7150549178459749",
         "0.6173824215499552",
         "0.8056804167303423",
         "0.5473438729857825"
        ],
        [
         "2",
         "google-bert/bert-base-multilingual-cased",
         "0.6412658227848101",
         "0.5274935551571125",
         "0.6190879138481044",
         "0.4189967953590671",
         "0.7666314955203909",
         "0.3968523745918796"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sdadas/polish-roberta-base-v2</td>\n",
       "      <td>0.745418</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>0.740501</td>\n",
       "      <td>0.690878</td>\n",
       "      <td>0.812584</td>\n",
       "      <td>0.575667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allegro/herbert-base-cased</td>\n",
       "      <td>0.737367</td>\n",
       "      <td>0.688930</td>\n",
       "      <td>0.733557</td>\n",
       "      <td>0.674316</td>\n",
       "      <td>0.802919</td>\n",
       "      <td>0.589556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dkleczek/bert-base-polish-cased-v1</td>\n",
       "      <td>0.719848</td>\n",
       "      <td>0.656802</td>\n",
       "      <td>0.715055</td>\n",
       "      <td>0.617382</td>\n",
       "      <td>0.805680</td>\n",
       "      <td>0.547344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google-bert/bert-base-multilingual-cased</td>\n",
       "      <td>0.641266</td>\n",
       "      <td>0.527494</td>\n",
       "      <td>0.619088</td>\n",
       "      <td>0.418997</td>\n",
       "      <td>0.766631</td>\n",
       "      <td>0.396852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  accuracy  macro_f1  weighted_f1  \\\n",
       "3             sdadas/polish-roberta-base-v2  0.745418  0.693043     0.740501   \n",
       "0                allegro/herbert-base-cased  0.737367  0.688930     0.733557   \n",
       "1        dkleczek/bert-base-polish-cased-v1  0.719848  0.656802     0.715055   \n",
       "2  google-bert/bert-base-multilingual-cased  0.641266  0.527494     0.619088   \n",
       "\n",
       "   neutral_f1  positive_f1  negative_f1  \n",
       "3    0.690878     0.812584     0.575667  \n",
       "0    0.674316     0.802919     0.589556  \n",
       "1    0.617382     0.805680     0.547344  \n",
       "2    0.418997     0.766631     0.396852  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "preprocessing",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weighted_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "neutral_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "positive_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negative_f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c82f6699-60e0-4c28-8c74-6f56fd8ad1fe",
       "rows": [
        [
         "8",
         "No_processing_mention",
         "0.7158227848101266",
         "0.6516757047488693",
         "0.7100774897205044",
         "0.6083653929903848",
         "0.801046343747973",
         "0.5456153775082496"
        ],
        [
         "4",
         "No_processing_cashtag",
         "0.7172151898734177",
         "0.6514545573851088",
         "0.7095255451675788",
         "0.60573453128391",
         "0.8005250056663288",
         "0.5481041352050878"
        ],
        [
         "6",
         "No_processing_hashtag",
         "0.7170886075949368",
         "0.6492140879332945",
         "0.7083559579088221",
         "0.6044124135482385",
         "0.801311968824257",
         "0.5419178814273881"
        ],
        [
         "5",
         "No_processing_emoji",
         "0.7151898734177216",
         "0.6441499059923977",
         "0.705688266310995",
         "0.605608512318955",
         "0.8020353801988526",
         "0.5248058254593857"
        ],
        [
         "1",
         "No_processing__rep",
         "0.7132911392405064",
         "0.6443282158383035",
         "0.70397741110067",
         "0.6110421257211135",
         "0.7974910309744375",
         "0.5244514908193594"
        ],
        [
         "3",
         "No_processing_caps",
         "0.7126582278481013",
         "0.6413858909636905",
         "0.7026377258091947",
         "0.6049236126161905",
         "0.7979843485757657",
         "0.521249711699115"
        ],
        [
         "9",
         "No_processing_norm",
         "0.7079746835443038",
         "0.6383406812658621",
         "0.6992760304240464",
         "0.5957591340279955",
         "0.794146246378979",
         "0.5251166633906117"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No_processing_mention</td>\n",
       "      <td>0.715823</td>\n",
       "      <td>0.651676</td>\n",
       "      <td>0.710077</td>\n",
       "      <td>0.608365</td>\n",
       "      <td>0.801046</td>\n",
       "      <td>0.545615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No_processing_cashtag</td>\n",
       "      <td>0.717215</td>\n",
       "      <td>0.651455</td>\n",
       "      <td>0.709526</td>\n",
       "      <td>0.605735</td>\n",
       "      <td>0.800525</td>\n",
       "      <td>0.548104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No_processing_hashtag</td>\n",
       "      <td>0.717089</td>\n",
       "      <td>0.649214</td>\n",
       "      <td>0.708356</td>\n",
       "      <td>0.604412</td>\n",
       "      <td>0.801312</td>\n",
       "      <td>0.541918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No_processing_emoji</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.644150</td>\n",
       "      <td>0.705688</td>\n",
       "      <td>0.605609</td>\n",
       "      <td>0.802035</td>\n",
       "      <td>0.524806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No_processing__rep</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>0.644328</td>\n",
       "      <td>0.703977</td>\n",
       "      <td>0.611042</td>\n",
       "      <td>0.797491</td>\n",
       "      <td>0.524451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No_processing_caps</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.641386</td>\n",
       "      <td>0.702638</td>\n",
       "      <td>0.604924</td>\n",
       "      <td>0.797984</td>\n",
       "      <td>0.521250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No_processing_norm</td>\n",
       "      <td>0.707975</td>\n",
       "      <td>0.638341</td>\n",
       "      <td>0.699276</td>\n",
       "      <td>0.595759</td>\n",
       "      <td>0.794146</td>\n",
       "      <td>0.525117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           preprocessing  accuracy  macro_f1  weighted_f1  neutral_f1  \\\n",
       "8  No_processing_mention  0.715823  0.651676     0.710077    0.608365   \n",
       "4  No_processing_cashtag  0.717215  0.651455     0.709526    0.605735   \n",
       "6  No_processing_hashtag  0.717089  0.649214     0.708356    0.604412   \n",
       "5    No_processing_emoji  0.715190  0.644150     0.705688    0.605609   \n",
       "1     No_processing__rep  0.713291  0.644328     0.703977    0.611042   \n",
       "3     No_processing_caps  0.712658  0.641386     0.702638    0.604924   \n",
       "9     No_processing_norm  0.707975  0.638341     0.699276    0.595759   \n",
       "\n",
       "   positive_f1  negative_f1  \n",
       "8     0.801046     0.545615  \n",
       "4     0.800525     0.548104  \n",
       "6     0.801312     0.541918  \n",
       "5     0.802035     0.524806  \n",
       "1     0.797491     0.524451  \n",
       "3     0.797984     0.521250  \n",
       "9     0.794146     0.525117  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_model_comparison = all_results.groupby(['model'])[all_results.select_dtypes(include='number').columns].agg('mean').reset_index().sort_values(by = ['weighted_f1'], ascending = False)\n",
    "display(by_model_comparison)\n",
    "by_preprocessing_comparison = all_results.groupby(['preprocessing'])[all_results.select_dtypes(include='number').columns].agg('mean').reset_index().sort_values(by = ['weighted_f1'], ascending = False)\n",
    "benchmark = by_preprocessing_comparison.loc[by_preprocessing_comparison['preprocessing']=='No_processing','weighted_f1'][0]\n",
    "beneficial_preprocessing = by_preprocessing_comparison[by_preprocessing_comparison['weighted_f1'] > benchmark]\n",
    "display(beneficial_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best data preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_processing_pipelines = {\n",
    "    'Basic_processing': [\n",
    "        preprocess_tweet\n",
    "    ],\n",
    "    'Basic_processing_spelling': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'Basic_processing_spelling_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        remove_stops\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = transform_data(processing_pipelines = strategies_processing_pipelines, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "os.path.exists(r'./strategy_comparison') or os.makedirs(r'./strategy_comparison')\n",
    "\n",
    "for seed in range(6):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Testing seed: {seed}\")\n",
    "    train_test_seed = seed\n",
    "    model_seed = seed\n",
    "    for model in tested_models.values():\n",
    "        tokenized_datasets = {}\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing model: {model}\")\n",
    "\n",
    "        for key, df in datasets.items():\n",
    "            tokenized_datasets[key] = prepare_datasets(df = df, model = model, test_size=test_size, random_state=train_test_seed, max_length=128, column='text')\n",
    "\n",
    "        results = run_evaluation(tokenized_datasets = tokenized_datasets, model = model, train_test_seed = train_test_seed, model_seed = model_seed)\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df['seed'] = seed\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        results_df.to_csv(rf\"./strategy_comparison/preprocessing_comparison_model_{model[:4]}_{train_test_seed}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
