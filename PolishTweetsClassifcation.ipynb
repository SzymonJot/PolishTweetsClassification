{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pl_core_news_lg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "from functions import *\n",
    "from evaluation_functions import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import product\n",
    "tool = language_tool_python.LanguageTool('pl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.read_csv(r'TrainingData\\dataset_labeled.csv', nrows=0).columns.tolist()\n",
    "\n",
    "columns_to_exclude = ['Unnamed: 0','Unnamed: 0.1' , 'Unnamed: 0.2']  # example columns to skip\n",
    "wanted_columns = [col for col in all_columns if col not in columns_to_exclude]\n",
    "\n",
    "dataset_labeled = pd.read_csv(r'TrainingData\\dataset_labeled.csv', usecols=wanted_columns)\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "The first part was training the model with different basic preprocessing strategies.\n",
    "Then we evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing http\n",
    "def preprocess_tweet_https(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', tweet, flags=re.MULTILINE)\n",
    "    return tweet\n",
    "\n",
    "#Removing hashtags\n",
    "def preprocess_tweet_hashtag(tweet):\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing mentions\n",
    "def preprocess_tweet_mention(tweet):\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing cashtag\n",
    "def preprocess_tweet_cashtag(tweet):\n",
    "    tweet = re.sub(r'\\$\\w+', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing all charatcters except polish letter and ? !\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = re.sub(r'[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż0-9\\s?!]', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing repeated letters\n",
    "def preprocess_tweet_rep(tweet):\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "#Removing white spaces\n",
    "def preprocess_tweet_norm(tweet):\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "#Normalizing caps\n",
    "def preprocess_caps(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(^|[.!?]\\s+)(\\w)', lambda m: m.group(1) + m.group(2).upper(), tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_tco(tweet):\n",
    "    return re.sub(r\"https?://t\\.co/\\S+\", \"\", tweet).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipelines\n",
    "basic_processing_pipelines = {\n",
    "    'No_processing': [],\n",
    "    'No_processing_emoji': [\n",
    "        replace_emoji\n",
    "    ],\n",
    "    'No_processing_http': [\n",
    "        preprocess_tweet_https\n",
    "    ],\n",
    "    'No_processing_hashtag': [\n",
    "        preprocess_tweet_hashtag\n",
    "    ],\n",
    "    'No_processing_mention': [\n",
    "        preprocess_tweet_mention\n",
    "    ],\n",
    "    'No_processing_cashtag': [\n",
    "        preprocess_tweet_cashtag\n",
    "    ],\n",
    "    'No_processing__text': [\n",
    "        preprocess_tweet_text\n",
    "    ],\n",
    "    'No_processing__rep': [\n",
    "        preprocess_tweet_rep\n",
    "    ],\n",
    "    'No_processing_norm': [\n",
    "        preprocess_tweet_norm\n",
    "    ],\n",
    "    'No_processing_caps': [\n",
    "        preprocess_caps\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_models = {}\n",
    "tested_models['ROBERT'] = \"sdadas/polish-roberta-base-v2\"\n",
    "tested_models['HERBERT']  = \"allegro/herbert-base-cased\"\n",
    "tested_models['POLBERT']  = \"dkleczek/bert-base-polish-cased-v1\"\n",
    "tested_models['MBERT'] = 'google-bert/bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best basic data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(r'./preprocessing_comparison') or os.makedirs(r'./preprocessing_comparison')\n",
    "os.path.exists(r'./results') or os.makedirs(r'./results')\n",
    "\n",
    "datasets = transform_data(processing_pipelines = basic_processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "\n",
    "params={'train_seed': [12], 'model_seed': [12,53,42]}\n",
    "all_params = [dict(zip(params.keys(), values)) \n",
    "             for values in itertools.product(*params.values())]\n",
    "\n",
    "all_combinations = list(product(all_params, tested_models.values(), datasets.keys()))\n",
    "\n",
    "for params_set, model, dataset_name in tqdm.tqdm(all_combinations, desc=\"Grid Search\", total=len(all_combinations)):\n",
    "    try:\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=datasets[dataset_name], params = params_set, model_name=model, strategy_name=dataset_name)\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%''H%M%S')\n",
    "        results_df.to_csv(rf\"./preprocessing_comparison/preprocessing_comparison_model_{model[:4]}_{params_set['train_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best data preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the preprocessing steps\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = preprocess_tweet_mention(tweet)\n",
    "    tweet = preprocess_tweet_https(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_processing_pipelines = {\n",
    "    'Basic_processing': [\n",
    "        preprocess_tweet\n",
    "    ],\n",
    "    'Basic_processing_spelling': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text\n",
    "    ],\n",
    "    'Basic_processing_spelling_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        remove_stops\n",
    "    ],\n",
    "    'Basic_processing_spelling_lem_SP': [\n",
    "        preprocess_tweet,\n",
    "        tool.correct,\n",
    "        lemmatize_text,\n",
    "        remove_stops\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = transform_data(processing_pipelines = strategies_processing_pipelines, dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "os.path.exists(r'./strategy_comparison') or os.makedirs(r'./strategy_comparison')\n",
    "\n",
    "datasets = transform_data(processing_pipelines = strategies_processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "\n",
    "params={'train_seed': [12], 'model_seed': [12,53,42]}\n",
    "all_params = [dict(zip(params.keys(), values)) \n",
    "             for values in itertools.product(*params.values())]\n",
    "\n",
    "all_combinations = list(product(all_params, tested_models.values(), datasets.keys()))\n",
    "\n",
    "for params_set, model, dataset_name in tqdm.tqdm(all_combinations, desc=\"Grid Search\", total=len(all_combinations)):\n",
    "    try:\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=datasets[dataset_name], params = params_set, model_name=model, strategy_name=dataset_name)\n",
    "        # Save results  \n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%' \\\n",
    "        'H%M%S')\n",
    "        results_df.to_csv(rf\"./strategy_comparison/preprocessing_comparison_model_{model[:4]}_{params['train_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with best preprocessing strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipelines = {}\n",
    "processing_pipelines['Basic_processing'] = strategies_processing_pipelines['Basic_processing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 150\n",
    "dataset = transform_data(processing_pipelines = processing_pipelines,dataset=dataset_labeled, csv_dir='TrainingData/')\n",
    "dataset_name = list(dataset.keys())[0]\n",
    "os.path.exists(r'./grid_search_comparison') or os.makedirs(r'./grid_search_comparison')\n",
    "\n",
    "model = tested_models['ROBERT']\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5,1.5e-5, 2e-5],     \n",
    "    \"batch_size\": [4,8, 16],               \n",
    "    \"epochs\": [3,4,5],                    \n",
    "    \"weight_decay\": [0.05,0.01, 0.001],       \n",
    "    \"class_weight_floor\": [0.5, 1.0],\n",
    "    \"train_seed\": [42],  \n",
    "    \"model_seed\": [42, 53, 12]\n",
    "}\n",
    "all_params = [dict(zip(param_grid.keys(), values)) \n",
    "             for values in itertools.product(*param_grid.values())]\n",
    "\n",
    "sampled_params = random.sample(all_params, k=min(N_SAMPLES, len(all_params)))\n",
    "\n",
    "for params_set in tqdm.tqdm(sampled_params, desc=\"Grid Search\", total=len(sampled_params)):\n",
    "    try:\n",
    "        \n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        # Run cross-validation\n",
    "        print(f\"Running cross-validation with params: {params_set}\")\n",
    "        cv_results = cross_val_score(df=dataset['Basic_processing'], params = params_set, model_name=model)\n",
    "        # Save results  \n",
    "        results_df = pd.DataFrame(cv_results, index=[0])\n",
    "        results_df['model_seed'] = params_set['model_seed']\n",
    "        results_df['train_seed'] = params_set['train_seed']\n",
    "        results_df['model'] = model\n",
    "        results_df['dataset'] = dataset_name\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        results_df.to_csv(rf\"./grid_search_comparison/preprocessing_comparison_model_{model[:4]}_{params_set['model_seed']}_{timestamp}.csv\", index=False)\n",
    "        print(f\"\\nComparison saved to preprocessing_comparison_model_{model[:4]}.csv\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params_set}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
