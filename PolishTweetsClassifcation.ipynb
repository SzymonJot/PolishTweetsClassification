{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import spacy\n",
    "import copy\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import itertools\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction\n",
    ")\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from functions import preprocess_tweet,replace_emoji,analyze_sentiment,create_tokenize_function\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The following code aims at using pretrained polish BERT models for tweet classifications. \n",
    "* Dataset has been labeled to classify all netrual/not relevant tweets as neutral.\n",
    "* This allows for filtering out noise - tweets that aren't aimed at specific company.\n",
    "* Models used were chose based on the KLEJ bechmark t(https://klejbenchmark.com/leaderboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38253</th>\n",
       "      <td>@lila_h17 Ja te znalazłam na Allegro,po 8 zł :p ale jak gdzieś szukałam to już te nowsze</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59140</th>\n",
       "      <td>@_rmrmg_ @PSlopnicki @PervWizz @Mecenas_Kon_ Daj spokój, typowe branżowe mity. Już widzę jak Żabka znajduje w kącie paletę mleka i jeździ po województwie, wciskając przerażonym ajentom po kilka kartonów.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57205</th>\n",
       "      <td>@FNkejz_ @Qlpik1 Daje mu unv bo jest zjebem i tyle a jak ktoś ma sobie zmarnować CCC bo ma z nim zagrać to proszę bardzo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60259</th>\n",
       "      <td>Melduję się z Bukaresztu - Żabka podbija ziemię rumuńską. 🐸🇷🇴 https://t.co/S56Gi1vDyg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33346</th>\n",
       "      <td>@jakubhajost @RBrzoska Gratulacje dla @RBrzoska i zespołu InPost! Imponujące wyniki zwiększenia wolumenów i przychodów. To świetne, że firma osiąga dwucyfrowy wzrost i lepsze marże. Przejście na 9 miliardów przychodu brzmi obiecująco!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                             text  labels\n",
       "38253                                                                                                                                                    @lila_h17 Ja te znalazłam na Allegro,po 8 zł :p ale jak gdzieś szukałam to już te nowsze     0.0\n",
       "59140                                 @_rmrmg_ @PSlopnicki @PervWizz @Mecenas_Kon_ Daj spokój, typowe branżowe mity. Już widzę jak Żabka znajduje w kącie paletę mleka i jeździ po województwie, wciskając przerażonym ajentom po kilka kartonów.     0.0\n",
       "57205                                                                                                                    @FNkejz_ @Qlpik1 Daje mu unv bo jest zjebem i tyle a jak ktoś ma sobie zmarnować CCC bo ma z nim zagrać to proszę bardzo     0.0\n",
       "60259                                                                                                                                                       Melduję się z Bukaresztu - Żabka podbija ziemię rumuńską. 🐸🇷🇴 https://t.co/S56Gi1vDyg     1.0\n",
       "33346  @jakubhajost @RBrzoska Gratulacje dla @RBrzoska i zespołu InPost! Imponujące wyniki zwiększenia wolumenów i przychodów. To świetne, że firma osiąga dwucyfrowy wzrost i lepsze marże. Przejście na 9 miliardów przychodu brzmi obiecująco!     1.0"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1 = pd.read_excel('annotation_dataset_labeled.xlsx',index_col=0)\n",
    "dataset_1 = dataset_1[['text','label']]\n",
    "dataset_1 =  dataset_1.rename(columns={'label':'labels'})\n",
    "dataset_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1923 entries, 38253 to 48370\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1923 non-null   object\n",
      " 1   labels  1923 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 45.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_labeled = dataset_1\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'] + 1\n",
    "dataset_labeled['labels'] = dataset_labeled['labels'].astype(int)\n",
    "dataset_labeled = dataset_labeled.dropna()\n",
    "dataset_labeled = dataset_labeled.drop_duplicates(subset='text')\n",
    "dataset_labeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Powoli kończy się sasinada w polskiej energetyce. Z głównych pomysłów Sasina NABE właśnie skreślono. Czas na to samo w przypadku mrzonek PGE + ZE PAK + KHNP. Niech PGE zajmie się OZE a nie projektami większymi NIŻ JEJ KAPITALIZACJA.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PO sprzedało KGHM dobrze ten gość przypomina. #debata</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KGHM stawia na kształcenie elektryków. Uruchomił kolejną klasę patronacką.  @wnppl   https://t.co/JrTT9aBf0K</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sjkaleta Ja już dostałem prognozę z PGE. Do lipca płace 231 zł miesięcznie, a od lipca 378 zł. No ale to się nie liczy, to uśmiechnięta podwyżka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hennigkloska @PL_2050 @Platforma_org @__Lewica @MiroslawSuchon @szymon_holownia @KancelariaSejmu A dlaczego tak wszyscy idą w wiatraki? Wielkie bum farm fotowoltaicznych pokazało to że operatorzy nie są przygotowani na odbiór energii od B2C, nie ma magazynów energii , zmiany w programach Mój prąd pokazuje tylko że Polak potrafi. Farmy wiatrowe to również PGE i Balcic Sea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                     text  labels\n",
       "0                                                                                                                                                Powoli kończy się sasinada w polskiej energetyce. Z głównych pomysłów Sasina NABE właśnie skreślono. Czas na to samo w przypadku mrzonek PGE + ZE PAK + KHNP. Niech PGE zajmie się OZE a nie projektami większymi NIŻ JEJ KAPITALIZACJA.       0\n",
       "1                                                                                                                                                                                                                                                                                                                                   PO sprzedało KGHM dobrze ten gość przypomina. #debata       1\n",
       "2                                                                                                                                                                                                                                                                            KGHM stawia na kształcenie elektryków. Uruchomił kolejną klasę patronacką.  @wnppl   https://t.co/JrTT9aBf0K       2\n",
       "3                                                                                                                                                                                                                                    @sjkaleta Ja już dostałem prognozę z PGE. Do lipca płace 231 zł miesięcznie, a od lipca 378 zł. No ale to się nie liczy, to uśmiechnięta podwyżka...       0\n",
       "4  @hennigkloska @PL_2050 @Platforma_org @__Lewica @MiroslawSuchon @szymon_holownia @KancelariaSejmu A dlaczego tak wszyscy idą w wiatraki? Wielkie bum farm fotowoltaicznych pokazało to że operatorzy nie są przygotowani na odbiór energii od B2C, nie ma magazynów energii , zmiany w programach Mój prąd pokazuje tylko że Polak potrafi. Farmy wiatrowe to również PGE i Balcic Sea       0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_labeled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset is imbalanced what will be addressed in the later stage of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='labels'>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = dataset_labeled['labels'].value_counts()\n",
    "count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max length will be set as 128. It covers more than 95% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tweet_lengths = [len(tokenizer.tokenize(tweet)) for tweet in dataset_labeled[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 86.0\n",
      "Max tokens: 260\n"
     ]
    }
   ],
   "source": [
    "print(f\"95th percentile: {np.percentile(tweet_lengths, 95)}\")  \n",
    "print(f\"Max tokens: {max(tweet_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Text Preprocessing Strategies for BERT Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of different text preprocessing techniques on BERT model performance using a systematic comparison approach.\n",
    "\n",
    "#### Methodology\n",
    "A baseline BERT model with default parameters was trained on each preprocessed version of the datasets. Due to class imbalance and the focus on positive/negative classification, the F1 score serves as the primary evaluation metric.\n",
    "\n",
    "#### Preprocessing Strategies\n",
    "We evaluated six distinct preprocessing approaches, incrementally adding complexity to assess the impact of each step:\n",
    "\n",
    "1. Raw text without any preprocessing\n",
    "2. Removal of non-textual characters\n",
    "3. Conversion of emojis to corresponding text + Removal of non-textual characters\n",
    "4. Removal of non-textual characters + Spelling correction\n",
    "5. Removal of non-textual characters + Spelling correction + Lemmatization\n",
    "6. Removal of non-textual characters + Spelling correction + Lemmatization + Stopword removal\n",
    "\n",
    "Model performance is evaluated using the F1 score, which provides a balanced measure of precision and recall, particularly important for our imbalanced dataset classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_TO_POLISH = {\n",
    "    # Twarze i emocje\n",
    "    \"😀\": \"usmiech \", \"😃\": \"szeroki_usmiech \", \"😄\": \"usmiech_z_oczami \", \"😁\": \"szeroki_usmiech_z_oczami \",\n",
    "    \"😆\": \"usmiech_zmruzone_oczy \", \"😅\": \"usmiech_pot \", \"🤣\": \"tarzanie_sie_ze_smiechu \",\n",
    "    \"😂\": \"lzy_radosci \", \"🙂\": \"lekki_usmiech \", \"🙃\": \"odwrocona_twarz \", \"😉\": \"mrugnięcie \",\n",
    "    \"😊\": \"usmiechniete_oczy \", \"😇\": \"usmiech_aureola \", \"🥰\": \"usmiech_serca \", \"😍\": \"oczy_serca \",\n",
    "    \"🤩\": \"gwiazdy_w_oczach \", \"😘\": \"przesylam_calusa \", \"😗\": \"calus \", \"☺️\": \"ciepły_usmiech \",\n",
    "    \"😚\": \"calus_z_zamknietymi_oczami \", \"😙\": \"calus_z_usmiechnietymi_oczami \", \"🥲\": \"usmiech_ze_lzą \",\n",
    "    \"😋\": \"mniam \", \"😛\": \"jezyk_na_wierzchu \", \"😜\": \"jezyk_mrugnięcie \", \"🤪\": \"szalona_twarz \",\n",
    "    \"😝\": \"jezyk_zmruzone_oczy \", \"🤑\": \"twarz_z_pieniędzmi \", \"🤗\": \"przytulenie \", \"🤭\": \"ups \",\n",
    "    \"🤫\": \"ciii \", \"🤔\": \"zamyslenie \", \"🤐\": \"milczenie \", \"🤨\": \"uniesiona_brew \",\n",
    "    \"😐\": \"neutralna_twarz \", \"😑\": \"bez_wyrazu \", \"😶\": \"bez_ust \", \"😏\": \"uśmieszek \",\n",
    "    \"😒\": \"niezadowolenie \", \"🙄\": \"przewracanie_oczami \", \"😬\": \"grymas \", \"🤥\": \"kłamca \",\n",
    "    \"😌\": \"ulga \", \"😔\": \"zaduma \", \"😪\": \"sennosc \", \"🤤\": \"slinka \", \"😴\": \"spanie \",\n",
    "    \"😷\": \"maska_medyczna \", \"🤒\": \"termometr \", \"🤕\": \"bandaz \", \"🤢\": \"mdłosci \",\n",
    "    \"🤮\": \"wymioty \", \"🤧\": \"kichanie \", \"🥵\": \"goraco \", \"🥶\": \"zimno \",\n",
    "    \"🥴\": \"oszołomienie \", \"😵\": \"zawroty_głowy \", \"🤯\": \"eksplozja_mózgu \", \"🤠\": \"kowboj \",\n",
    "    \"🥳\": \"impreza \", \"😎\": \"cool \", \"🤓\": \"kujon \", \"🧐\": \"monokl \",\n",
    "    \"😕\": \"zmieszanie \", \"😟\": \"zmartwienie \", \"🙁\": \"lekki_smutek \", \"☹️\": \"smutek \",\n",
    "    \"😮\": \"zdziwienie \", \"😯\": \"osłupienie \", \"😲\": \"szok \", \"😳\": \"rumieniec \",\n",
    "    \"🥺\": \"błagalne_oczy \", \"😦\": \"otwarte_usta \", \"😧\": \"udręka \", \"😨\": \"strach \",\n",
    "    \"😰\": \"zimny_pot \", \"😥\": \"rozczarowanie_ulga \", \"😢\": \"płacz \", \"😭\": \"głośny_płacz \",\n",
    "    \"😱\": \"krzyk \", \"😖\": \"zakłopotanie \", \"😣\": \"wytrwałosc \", \"😞\": \"rozczarowanie \",\n",
    "    \"😓\": \"pot \", \"😩\": \"zmęczenie \", \"😫\": \"wyczerpanie \", \"🥱\": \"ziewanie \",\n",
    "    \n",
    "    # Gesty i ludzie\n",
    "    \"👋\": \"machanie \", \"🤚\": \"uniesiona_dłoń \", \"🖐️\": \"rozstawione_palce \", \"✋\": \"wysoka_piątka \",\n",
    "    \"🖖\": \"vulcan \", \"👌\": \"ok \", \"🤌\": \"włoski_gest \", \"🤏\": \"mały_gest \",\n",
    "    \"✌️\": \"victoria \", \"🤞\": \"skrzyżowane_palce \", \"🤟\": \"kocham_cię_gest \", \"🤘\": \"rogi \",\n",
    "    \"🤙\": \"zadzwoń_do_mnie \", \"👈\": \"wskazywanie_w_lewo \", \"👉\": \"wskazywanie_w_prawo \", \"👆\": \"wskazywanie_w_górę \",\n",
    "    \"🖕\": \"środkowy_palec \", \"👇\": \"wskazywanie_w_dół \", \"☝️\": \"wskazywanie_w_górę_palcem \", \"👍\": \"kciuk_w_górę \",\n",
    "    \"👎\": \"kciuk_w_dół \", \"✊\": \"pięść \", \"👊\": \"pięść_naprzód \", \"🤛\": \"pięść_w_lewo \",\n",
    "    \"🤜\": \"pięść_w_prawo \", \"👏\": \"oklaski \", \"🙌\": \"uniesione_ręce \", \"👐\": \"otwarte_dłonie \",\n",
    "    \"🤲\": \"błagalne_ręce \", \"🤝\": \"uścisk_dłoni \", \"🙏\": \"złożone_ręce \", \"✍️\": \"pisanie \",\n",
    "    \"💪\": \"biceps \", \"🦾\": \"mechaniczne_ramię \", \"🦿\": \"mechaniczna_noga \", \"🦵\": \"noga \",\n",
    "    \"🦶\": \"stopa \", \"👂\": \"ucho \", \"🦻\": \"ucho_z_aparatem \", \"👃\": \"nos \",\n",
    "    \"🧠\": \"mózg \", \"🫀\": \"serce_anatomiczne \", \"🫁\": \"płuca \", \"🦷\": \"ząb \",\n",
    "    \"🦴\": \"kość \", \"👀\": \"oczy \", \"👁️\": \"oko \", \"👅\": \"język \",\n",
    "    \"👄\": \"usta \", \"👶\": \"dziecko \", \"🧒\": \"dziecko \", \"👦\": \"chłopiec \",\n",
    "    \"👧\": \"dziewczynka \", \"🧑\": \"osoba \", \"👱\": \"blondyn \", \"👨\": \"mężczyzna \",\n",
    "    \"🧔\": \"brodacz \", \"👩\": \"kobieta \", \"🧓\": \"starsza_osoba \", \"👴\": \"starszy_mężczyzna \",\n",
    "    \"👵\": \"starsza_kobieta \",\n",
    "}\n",
    "\n",
    "def replace_emoji(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces emojis in a given text based on the emoji dictionary provided.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing emojis.\n",
    "        emoji_dict (dict): A dictionary where keys are emojis and values are replacements.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with emojis replaced.\n",
    "    \"\"\"\n",
    "    for emoji, replacement in EMOJI_TO_POLISH.items():\n",
    "        text = text.replace(emoji, f\" {replacement}\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    #tweet = tweet.lower()\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub(r'@\\w+|#\\w+', '', tweet)\n",
    "    # Replace underscores with space\n",
    "    tweet = re.sub(r'_', ' ', tweet)\n",
    "    # Changed \\w to use explicit character ranges instead\n",
    "    tweet = re.sub(r'[^a-zA-Z0-9\\s\\u0105\\u0107\\u0119\\u0142\\u0144\\u00f3\\u015b\\u017a\\u017c]', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_stopwords = [\n",
    "    'a', 'aby', 'ach', 'acz', 'aczkolwiek', 'aj', 'albo', 'ale', 'alez', 'ależ',\n",
    "    'ani', 'az', 'aż', 'bardziej', 'bardzo', 'beda', 'bedzie', 'bez', 'deda', 'będą',\n",
    "    'bede', 'będę', 'będzie', 'bo', 'bowiem', 'by', 'byc', 'być', 'byl', 'byla',\n",
    "    'byli', 'bylo', 'byly', 'był', 'była', 'było', 'były', 'bynajmniej', 'cala',\n",
    "    'cali', 'caly', 'cała', 'cały', 'ci', 'cie', 'ciebie', 'cię', 'co', 'cokolwiek',\n",
    "    'cos', 'coś', 'czasami', 'czasem', 'czemu', 'czy', 'czyli', 'daleko', 'dla',\n",
    "    'dlaczego', 'dlatego', 'do', 'dobrze', 'dokad', 'dokąd', 'dosc', 'dość', 'duzo',\n",
    "    'dużo', 'dwa', 'dwaj', 'dwie', 'dwoje', 'dzis', 'dzisiaj', 'dziś', 'gdy', 'gdyby',\n",
    "    'gdyz', 'gdyż', 'gdzie', 'gdziekolwiek', 'gdzies', 'gdzieś', 'go', 'i', 'ich',\n",
    "    'ile', 'im', 'inna', 'inne', 'inny', 'innych', 'iz', 'iż', 'ja', 'jak', 'jakas',\n",
    "    'jakaś', 'jakby', 'jaki', 'jakichs', 'jakichś', 'jakie', 'jakis', 'jakiś', 'jakiz',\n",
    "    'jakiż', 'jakkolwiek', 'jako', 'jakos', 'jakoś', 'ją', 'je', 'jeden', 'jedna',\n",
    "    'jednak', 'jednakze', 'jednakże', 'jedno', 'jego', 'jej', 'jemu', 'jesli', 'jest',\n",
    "    'jestem', 'jeszcze', 'jeśli', 'jezeli', 'jeżeli', 'juz', 'już', 'kazdy', 'każdy',\n",
    "    'kiedy', 'kilka', 'kims', 'kimś', 'kto', 'ktokolwiek', 'ktora', 'ktore', 'ktorego',\n",
    "    'ktorej', 'ktory', 'ktorych', 'ktorym', 'ktorzy', 'ktos', 'ktoś', 'która', 'które',\n",
    "    'którego', 'której', 'który', 'których', 'którym', 'którzy', 'ku', 'lat', 'lecz',\n",
    "    'lub', 'ma', 'mają', 'mało', 'mam', 'mi', 'miedzy', 'między', 'mimo', 'mna', 'mną',\n",
    "    'mnie', 'moga', 'mogą', 'moi', 'moim', 'moj', 'moja', 'moje', 'moze', 'mozliwe',\n",
    "    'mozna', 'może', 'możliwe', 'można', 'mój', 'mu', 'musi', 'my', 'na', 'nad', 'nam',\n",
    "    'nami', 'nas', 'nasi', 'nasz', 'nasza', 'nasze', 'naszego', 'naszych', 'natomiast',\n",
    "    'natychmiast', 'nawet', 'nia', 'nią', 'nic', 'nich', 'niech', 'niego', 'niej',\n",
    "    'niemu', 'nigdy', 'nim', 'nimi', 'niz', 'niż', 'no', 'o', 'obok', 'od', 'około',\n",
    "    'on', 'ona', 'one', 'oni', 'ono', 'oraz', 'oto', 'owszem', 'pan', 'pana', 'pani',\n",
    "    'po', 'pod', 'podczas', 'pomimo', 'ponad', 'poniewaz', 'ponieważ', 'powinien',\n",
    "    'powinna', 'powinni', 'powinno', 'poza', 'prawie', 'przeciez', 'przecież', 'przed',\n",
    "    'przede', 'przedtem', 'przez', 'przy', 'roku', 'rowniez', 'również', 'sam', 'sama',\n",
    "    'są', 'sie', 'się', 'skad', 'skąd', 'soba', 'sobą', 'sobie', 'sposob', 'sposób',\n",
    "    'swoje', 'ta', 'taka', 'taki', 'takie', 'takze', 'także', 'tam', 'te',\n",
    "    'tego', 'tej', 'ten', 'teraz', 'też', 'to', 'toba', 'tobą', 'tobie', 'totez',\n",
    "    'toteż', 'totobą', 'trzeba', 'tu', 'tutaj', 'twoi', 'twoim', 'twoj', 'twoja',\n",
    "    'twoje', 'twój', 'twym', 'ty', 'tych', 'tylko', 'tym', 'u', 'w', 'wam', 'wami',\n",
    "    'was', 'wasz', 'wasza', 'wasze', 'we', 'według', 'wiele', 'wielu', 'więc', 'więcej',\n",
    "    'wlasnie', 'właśnie', 'wszyscy', 'wszystkich', 'wszystkie', 'wszystkim', 'wszystko',\n",
    "    'wtedy', 'wy', 'z', 'za', 'zaden', 'zadna', 'zadne', 'zadnych', 'zapewne', 'zawsze',\n",
    "    'ze', 'zeby', 'zeznowu', 'zł', 'znow', 'znowu', 'znów', 'zostal', 'został', 'żaden',\n",
    "    'żadna', 'żadne', 'żadnych', 'że', 'żeby'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in polish_stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset processing\n",
    "datasets = {}\n",
    "\n",
    "datasets['No_processing'] = dataset_labeled\n",
    "\n",
    "dataset_labeled_basic_proc = copy.deepcopy(dataset_labeled)\n",
    "dataset_labeled_basic_proc['text'] = dataset_labeled['text'].apply(lambda x:preprocess_tweet(x))\n",
    "dataset_labeled_basic_proc = dataset_labeled_basic_proc.drop_duplicates(subset='text')\n",
    "datasets['Basic_processing'] = dataset_labeled_basic_proc\n",
    "\n",
    "dataset_labeled_proc_emoji = copy.deepcopy(dataset_labeled)\n",
    "dataset_labeled_proc_emoji['text'] = dataset_labeled_proc_emoji['text'].apply(lambda x:replace_emoji(x))\n",
    "dataset_labeled_proc_emoji['text'] = dataset_labeled_proc_emoji['text'].apply(lambda x:preprocess_tweet(x))\n",
    "dataset_labeled_proc_emoji = dataset_labeled_proc_emoji.drop_duplicates(subset='text')\n",
    "datasets['Basic_processing_emoji'] = dataset_labeled_proc_emoji\n",
    "\n",
    "tool = language_tool_python.LanguageTool('pl')\n",
    "dataset_labeled_proc_spell = copy.deepcopy(dataset_labeled)\n",
    "dataset_labeled_proc_spell['text'] = dataset_labeled_proc_spell['text'].apply(lambda x:preprocess_tweet(x))\n",
    "dataset_labeled_proc_spell['text'] = dataset_labeled_proc_spell['text'].apply(lambda x: tool.correct(x))\n",
    "datasets['Basic_processing_spelling'] = dataset_labeled_proc_spell\n",
    "\n",
    "dataset_labeled_proc_spell_lem = copy.deepcopy(dataset_labeled_proc_spell)\n",
    "dataset_labeled_proc_spell_lem['text'] = dataset_labeled_proc_spell_lem['text'].map(lemmatize_text)\n",
    "dataset_labeled_proc_spell_lem =dataset_labeled_proc_spell_lem.drop_duplicates(subset='text')\n",
    "datasets['Basic_processing_spelling_lem'] = dataset_labeled_proc_spell_lem\n",
    "\n",
    "dataset_labeled_proc_spell_lem_SP = copy.deepcopy(dataset_labeled_proc_spell_lem)\n",
    "dataset_labeled_proc_spell_lem_SP['text'] = dataset_labeled_proc_spell_lem_SP['text'].map(remove_stops)\n",
    "dataset_labeled_proc_spell_lem_SP =dataset_labeled_proc_spell_lem_SP.drop_duplicates(subset='text')\n",
    "datasets['Basic_processing_spelling_lem_SP'] = dataset_labeled_proc_spell_lem_SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Dataset.from_pandas(dataset_labeled_proc_emoji)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",max_length=128),\n",
    "    batched=True,\n",
    "    remove_columns=['text','__index_level_0__']\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=12).train_test_split(test_size=0.2)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1923/1923 [00:00<00:00, 5515.16 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1911/1911 [00:00<00:00, 9551.31 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1912/1912 [00:00<00:00, 16412.33 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1923/1923 [00:00<00:00, 14409.53 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1910/1910 [00:00<00:00, 12427.64 examples/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 1909/1909 [00:00<00:00, 15846.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = {}\n",
    "for key, df in datasets.items():\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",max_length=128),\n",
    "    batched=True,\n",
    "    remove_columns=['text','__index_level_0__']\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.shuffle(seed=12).train_test_split(test_size=0.2)\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "    tokenized_datasets[key] = tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: No_processing\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,538\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [04:27<12:26,  5.18s/it] \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [05:20<12:26,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7100661993026733, 'eval_accuracy': 0.7454545454545455, 'eval_f1_macro': 0.2847222222222222, 'eval_f1_weighted': 0.6367424242424242, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8541666666666666, 'eval_f1_2': 0.0, 'eval_runtime': 46.8685, 'eval_samples_per_second': 8.214, 'eval_steps_per_second': 0.149, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_No_processing\\checkpoint-48\n",
      "Configuration saved in ./results_No_processing\\checkpoint-48\\config.json\n",
      "Model weights saved in ./results_No_processing\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [05:59<52:41, 22.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7854, 'grad_norm': 2.5444118976593018, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 97/192 [09:23<06:07,  3.87s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 51%|█████     | 97/192 [09:39<06:07,  3.87s/it]Saving model checkpoint to ./results_No_processing\\checkpoint-97\n",
      "Configuration saved in ./results_No_processing\\checkpoint-97\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6817612648010254, 'eval_accuracy': 0.7480519480519481, 'eval_f1_macro': 0.2970513093463913, 'eval_f1_weighted': 0.6427934105494251, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8554396423248882, 'eval_f1_2': 0.03571428571428571, 'eval_runtime': 15.3324, 'eval_samples_per_second': 25.11, 'eval_steps_per_second': 0.457, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing\\checkpoint-97\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [10:01<12:02,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6821, 'grad_norm': 2.8615920543670654, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 145/192 [13:19<03:19,  4.25s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 76%|███████▌  | 145/192 [13:34<03:19,  4.25s/it]Saving model checkpoint to ./results_No_processing\\checkpoint-145\n",
      "Configuration saved in ./results_No_processing\\checkpoint-145\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.65580815076828, 'eval_accuracy': 0.7506493506493507, 'eval_f1_macro': 0.30896395216897965, 'eval_f1_weighted': 0.6486682105535208, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8567164179104477, 'eval_f1_2': 0.07017543859649122, 'eval_runtime': 15.1358, 'eval_samples_per_second': 25.436, 'eval_steps_per_second': 0.462, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing\\checkpoint-145\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [14:02<04:07,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6072, 'grad_norm': 3.502885341644287, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [17:06<00:00,  4.00s/it]Saving model checkpoint to ./results_No_processing\\checkpoint-192\n",
      "Configuration saved in ./results_No_processing\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_No_processing\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [17:31<00:00,  4.00s/it]Saving model checkpoint to ./results_No_processing\\checkpoint-192\n",
      "Configuration saved in ./results_No_processing\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6529662013053894, 'eval_accuracy': 0.7558441558441559, 'eval_f1_macro': 0.34713915915379806, 'eval_f1_weighted': 0.669988674183979, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8649468892261002, 'eval_f1_2': 0.17647058823529413, 'eval_runtime': 16.4714, 'eval_samples_per_second': 23.374, 'eval_steps_per_second': 0.425, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_No_processing\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_No_processing\\checkpoint-192 (score: 0.34713915915379806).\n",
      "100%|██████████| 192/192 [17:38<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1058.623, 'train_samples_per_second': 5.811, 'train_steps_per_second': 0.181, 'train_loss': 0.6610554258028666, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [17:39<00:00,  5.52s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "100%|██████████| 7/7 [00:13<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: Basic_processing\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [03:49<11:10,  4.66s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [04:06<11:10,  4.66s/it]Saving model checkpoint to ./results_Basic_processing\\checkpoint-48\n",
      "Configuration saved in ./results_Basic_processing\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6460792422294617, 'eval_accuracy': 0.7780678851174935, 'eval_f1_macro': 0.29172785119921685, 'eval_f1_weighted': 0.6809522166373364, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8751835535976505, 'eval_f1_2': 0.0, 'eval_runtime': 16.9332, 'eval_samples_per_second': 22.618, 'eval_steps_per_second': 0.354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [04:30<26:56, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8059, 'grad_norm': 3.1425628662109375, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 96/192 [07:57<06:57,  4.35s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 50%|█████     | 96/192 [08:13<06:57,  4.35s/it]Saving model checkpoint to ./results_Basic_processing\\checkpoint-96\n",
      "Configuration saved in ./results_Basic_processing\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6037546396255493, 'eval_accuracy': 0.793733681462141, 'eval_f1_macro': 0.45148666466724485, 'eval_f1_weighted': 0.7426388340127253, 'eval_f1_0': 0.19607843137254902, 'eval_f1_1': 0.8871951219512195, 'eval_f1_2': 0.2711864406779661, 'eval_runtime': 16.2818, 'eval_samples_per_second': 23.523, 'eval_steps_per_second': 0.369, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing\\checkpoint-96\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [08:40<10:50,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6576, 'grad_norm': 4.727313995361328, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 144/192 [11:57<03:23,  4.24s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 75%|███████▌  | 144/192 [12:13<03:23,  4.24s/it]Saving model checkpoint to ./results_Basic_processing\\checkpoint-144\n",
      "Configuration saved in ./results_Basic_processing\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5576422214508057, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.5202978271357099, 'eval_f1_weighted': 0.7594914032609636, 'eval_f1_0': 0.32786885245901637, 'eval_f1_1': 0.8785942492012779, 'eval_f1_2': 0.35443037974683544, 'eval_runtime': 15.5658, 'eval_samples_per_second': 24.605, 'eval_steps_per_second': 0.385, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing\\checkpoint-144\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [12:50<04:06,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5493, 'grad_norm': 6.42683744430542, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [15:59<00:00,  4.30s/it]Saving model checkpoint to ./results_Basic_processing\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_Basic_processing\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [16:23<00:00,  4.30s/it]Saving model checkpoint to ./results_Basic_processing\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5560557842254639, 'eval_accuracy': 0.7702349869451697, 'eval_f1_macro': 0.5317892367035305, 'eval_f1_weighted': 0.7568059856649032, 'eval_f1_0': 0.3225806451612903, 'eval_f1_1': 0.8682926829268293, 'eval_f1_2': 0.4044943820224719, 'eval_runtime': 16.509, 'eval_samples_per_second': 23.199, 'eval_steps_per_second': 0.363, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_Basic_processing\\checkpoint-192 (score: 0.5317892367035305).\n",
      "100%|██████████| 192/192 [16:30<00:00,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 990.384, 'train_samples_per_second': 6.171, 'train_steps_per_second': 0.194, 'train_loss': 0.6254257758458456, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [16:30<00:00,  5.16s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: Basic_processing_emoji\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,529\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [03:40<10:26,  4.35s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [03:56<10:26,  4.35s/it]Saving model checkpoint to ./results_Basic_processing_emoji\\checkpoint-48\n",
      "Configuration saved in ./results_Basic_processing_emoji\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6644154787063599, 'eval_accuracy': 0.7676240208877284, 'eval_f1_macro': 0.2895125553914328, 'eval_f1_weighted': 0.6667103756011585, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8685376661742984, 'eval_f1_2': 0.0, 'eval_runtime': 15.8031, 'eval_samples_per_second': 24.236, 'eval_steps_per_second': 0.38, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_emoji\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [04:13<22:24,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7731, 'grad_norm': 2.662282705307007, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 96/192 [07:46<07:06,  4.45s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 50%|█████     | 96/192 [08:02<07:06,  4.45s/it]Saving model checkpoint to ./results_Basic_processing_emoji\\checkpoint-96\n",
      "Configuration saved in ./results_Basic_processing_emoji\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6246981024742126, 'eval_accuracy': 0.7702349869451697, 'eval_f1_macro': 0.33133020650120515, 'eval_f1_weighted': 0.6818270571328903, 'eval_f1_0': 0.046511627906976744, 'eval_f1_1': 0.8690476190476191, 'eval_f1_2': 0.0784313725490196, 'eval_runtime': 16.2493, 'eval_samples_per_second': 23.57, 'eval_steps_per_second': 0.369, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_emoji\\checkpoint-96\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [08:30<11:01,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6433, 'grad_norm': 3.773179531097412, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 144/192 [11:48<03:19,  4.16s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      " 75%|███████▌  | 144/192 [12:04<03:19,  4.16s/it]Saving model checkpoint to ./results_Basic_processing_emoji\\checkpoint-144\n",
      "Configuration saved in ./results_Basic_processing_emoji\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6114988327026367, 'eval_accuracy': 0.7754569190600522, 'eval_f1_macro': 0.4431189619379106, 'eval_f1_weighted': 0.7275575291274975, 'eval_f1_0': 0.18181818181818182, 'eval_f1_1': 0.8788819875776398, 'eval_f1_2': 0.26865671641791045, 'eval_runtime': 15.9921, 'eval_samples_per_second': 23.949, 'eval_steps_per_second': 0.375, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_emoji\\checkpoint-144\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [12:42<04:12,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5058, 'grad_norm': 6.353649139404297, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [15:50<00:00,  4.16s/it]Saving model checkpoint to ./results_Basic_processing_emoji\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_emoji\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_Basic_processing_emoji\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [16:15<00:00,  4.16s/it]Saving model checkpoint to ./results_Basic_processing_emoji\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_emoji\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6152315735816956, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.4229626275677672, 'eval_f1_weighted': 0.7158300311681413, 'eval_f1_0': 0.1935483870967742, 'eval_f1_1': 0.8724409448818897, 'eval_f1_2': 0.2028985507246377, 'eval_runtime': 16.9674, 'eval_samples_per_second': 22.573, 'eval_steps_per_second': 0.354, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_emoji\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_Basic_processing_emoji\\checkpoint-144 (score: 0.4431189619379106).\n",
      "100%|██████████| 192/192 [16:22<00:00,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 982.0763, 'train_samples_per_second': 6.228, 'train_steps_per_second': 0.196, 'train_loss': 0.5959161917368571, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [16:22<00:00,  5.12s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 64\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: Basic_processing_spelling\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,538\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [03:33<10:30,  4.38s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [03:49<10:30,  4.38s/it]Saving model checkpoint to ./results_Basic_processing_spelling\\checkpoint-48\n",
      "Configuration saved in ./results_Basic_processing_spelling\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6697341799736023, 'eval_accuracy': 0.7714285714285715, 'eval_f1_macro': 0.2903225806451613, 'eval_f1_weighted': 0.671889400921659, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8709677419354839, 'eval_f1_2': 0.0, 'eval_runtime': 16.2648, 'eval_samples_per_second': 23.671, 'eval_steps_per_second': 0.43, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [04:05<21:52,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7897, 'grad_norm': 2.878434419631958, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 97/192 [07:26<05:43,  3.62s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 51%|█████     | 97/192 [07:41<05:43,  3.62s/it]Saving model checkpoint to ./results_Basic_processing_spelling\\checkpoint-97\n",
      "Configuration saved in ./results_Basic_processing_spelling\\checkpoint-97\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6301880478858948, 'eval_accuracy': 0.7558441558441559, 'eval_f1_macro': 0.35166756880542954, 'eval_f1_weighted': 0.6908206254336079, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8632218844984803, 'eval_f1_2': 0.1917808219178082, 'eval_runtime': 14.5703, 'eval_samples_per_second': 26.424, 'eval_steps_per_second': 0.48, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling\\checkpoint-97\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [08:03<11:40,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.645, 'grad_norm': 4.219155311584473, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 145/192 [11:10<03:15,  4.15s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      " 76%|███████▌  | 145/192 [11:26<03:15,  4.15s/it]Saving model checkpoint to ./results_Basic_processing_spelling\\checkpoint-145\n",
      "Configuration saved in ./results_Basic_processing_spelling\\checkpoint-145\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6224602460861206, 'eval_accuracy': 0.7688311688311689, 'eval_f1_macro': 0.42829312049047913, 'eval_f1_weighted': 0.722833836096367, 'eval_f1_0': 0.13333333333333333, 'eval_f1_1': 0.8730650154798761, 'eval_f1_2': 0.27848101265822783, 'eval_runtime': 14.6924, 'eval_samples_per_second': 26.204, 'eval_steps_per_second': 0.476, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling\\checkpoint-145\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [11:54<04:13,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5477, 'grad_norm': 9.674766540527344, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [15:04<00:00,  4.88s/it]Saving model checkpoint to ./results_Basic_processing_spelling\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_Basic_processing_spelling\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [15:31<00:00,  4.88s/it]Saving model checkpoint to ./results_Basic_processing_spelling\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6285790801048279, 'eval_accuracy': 0.7532467532467533, 'eval_f1_macro': 0.4660055581447535, 'eval_f1_weighted': 0.7269506310581346, 'eval_f1_0': 0.26666666666666666, 'eval_f1_1': 0.8630573248407644, 'eval_f1_2': 0.2682926829268293, 'eval_runtime': 17.867, 'eval_samples_per_second': 21.548, 'eval_steps_per_second': 0.392, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_Basic_processing_spelling\\checkpoint-192 (score: 0.4660055581447535).\n",
      "100%|██████████| 192/192 [15:39<00:00,  4.89s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 385\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 939.2101, 'train_samples_per_second': 6.55, 'train_steps_per_second': 0.204, 'train_loss': 0.6194236675898234, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: Basic_processing_spelling_lem\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [03:54<11:20,  4.73s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [04:12<11:20,  4.73s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem\\checkpoint-48\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.722029983997345, 'eval_accuracy': 0.7408376963350786, 'eval_f1_macro': 0.2837092731829574, 'eval_f1_weighted': 0.6305475731212848, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8511278195488722, 'eval_f1_2': 0.0, 'eval_runtime': 18.0464, 'eval_samples_per_second': 21.168, 'eval_steps_per_second': 0.332, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [04:30<24:33, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.798, 'grad_norm': 1.7632604837417603, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 96/192 [08:21<07:14,  4.53s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 50%|█████     | 96/192 [08:38<07:14,  4.53s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem\\checkpoint-96\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6692366003990173, 'eval_accuracy': 0.7408376963350786, 'eval_f1_macro': 0.31105499215367005, 'eval_f1_weighted': 0.6434518962957526, 'eval_f1_0': 0.046511627906976744, 'eval_f1_1': 0.8554033485540334, 'eval_f1_2': 0.03125, 'eval_runtime': 17.0665, 'eval_samples_per_second': 22.383, 'eval_steps_per_second': 0.352, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem\\checkpoint-96\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [09:14<12:27,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6403, 'grad_norm': 5.991548538208008, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 144/192 [12:37<03:24,  4.25s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 75%|███████▌  | 144/192 [12:53<03:24,  4.25s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem\\checkpoint-144\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.655320942401886, 'eval_accuracy': 0.7513089005235603, 'eval_f1_macro': 0.41048218029350103, 'eval_f1_weighted': 0.6889163291514374, 'eval_f1_0': 0.16666666666666666, 'eval_f1_1': 0.8647798742138365, 'eval_f1_2': 0.2, 'eval_runtime': 16.7502, 'eval_samples_per_second': 22.806, 'eval_steps_per_second': 0.358, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem\\checkpoint-144\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [13:36<04:29,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.537, 'grad_norm': 4.500427722930908, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [16:53<00:00,  4.26s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_Basic_processing_spelling_lem\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [17:19<00:00,  4.26s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6597473621368408, 'eval_accuracy': 0.7513089005235603, 'eval_f1_macro': 0.4492142646625405, 'eval_f1_weighted': 0.7032466102832595, 'eval_f1_0': 0.23076923076923078, 'eval_f1_1': 0.864, 'eval_f1_2': 0.25287356321839083, 'eval_runtime': 16.9838, 'eval_samples_per_second': 22.492, 'eval_steps_per_second': 0.353, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_Basic_processing_spelling_lem\\checkpoint-192 (score: 0.4492142646625405).\n",
      "100%|██████████| 192/192 [17:29<00:00,  5.46s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1049.1581, 'train_samples_per_second': 5.826, 'train_steps_per_second': 0.183, 'train_loss': 0.61900594830513, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:14<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing preprocessing variant: Basic_processing_spelling_lem_SP\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,527\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      " 25%|██▌       | 48/192 [03:51<11:04,  4.61s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 25%|██▌       | 48/192 [04:07<11:04,  4.61s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem_SP\\checkpoint-48\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7147764563560486, 'eval_accuracy': 0.743455497382199, 'eval_f1_macro': 0.28428428428428426, 'eval_f1_weighted': 0.634058141911545, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8528528528528528, 'eval_f1_2': 0.0, 'eval_runtime': 16.4141, 'eval_samples_per_second': 23.273, 'eval_steps_per_second': 0.366, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-48\\model.safetensors\n",
      " 26%|██▌       | 50/192 [04:26<23:43, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7847, 'grad_norm': 2.6425821781158447, 'learning_rate': 1.6511627906976747e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 96/192 [08:11<07:28,  4.67s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 50%|█████     | 96/192 [08:29<07:28,  4.67s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem_SP\\checkpoint-96\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6581470370292664, 'eval_accuracy': 0.7408376963350786, 'eval_f1_macro': 0.2837092731829574, 'eval_f1_weighted': 0.6327756564185333, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8511278195488722, 'eval_f1_2': 0.0, 'eval_runtime': 17.9503, 'eval_samples_per_second': 21.281, 'eval_steps_per_second': 0.334, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-96\\model.safetensors\n",
      " 52%|█████▏    | 100/192 [08:57<11:32,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6561, 'grad_norm': 4.145880699157715, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 144/192 [12:36<03:32,  4.42s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      " 75%|███████▌  | 144/192 [12:52<03:32,  4.42s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem_SP\\checkpoint-144\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6598867177963257, 'eval_accuracy': 0.743455497382199, 'eval_f1_macro': 0.3079365079365079, 'eval_f1_weighted': 0.6468461730241835, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8571428571428571, 'eval_f1_2': 0.06666666666666667, 'eval_runtime': 16.2313, 'eval_samples_per_second': 23.535, 'eval_steps_per_second': 0.37, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-144\\model.safetensors\n",
      " 78%|███████▊  | 150/192 [13:31<04:22,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5868, 'grad_norm': 4.204592227935791, 'learning_rate': 4.883720930232559e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [16:59<00:00,  4.62s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\\config.json\n",
      "Model weights saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 192/192 [17:26<00:00,  4.62s/it]Saving model checkpoint to ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\n",
      "Configuration saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6541637778282166, 'eval_accuracy': 0.7460732984293194, 'eval_f1_macro': 0.4153751329029077, 'eval_f1_weighted': 0.6884955193194984, 'eval_f1_0': 0.14285714285714285, 'eval_f1_1': 0.8566929133858268, 'eval_f1_2': 0.2465753424657534, 'eval_runtime': 18.6333, 'eval_samples_per_second': 20.501, 'eval_steps_per_second': 0.322, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_Basic_processing_spelling_lem_SP\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_Basic_processing_spelling_lem_SP\\checkpoint-192 (score: 0.4153751329029077).\n",
      "100%|██████████| 192/192 [17:35<00:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1055.2616, 'train_samples_per_second': 5.788, 'train_steps_per_second': 0.182, 'train_loss': 0.6435764829317728, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [17:35<00:00,  5.50s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 382\n",
      "  Batch size = 64\n",
      "100%|██████████| 6/6 [00:15<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison saved to preprocessing_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Modified compute_metrics function\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        \"f1_0\": f1_score(labels, predictions, average=None)[0],\n",
    "        \"f1_1\": f1_score(labels, predictions, average=None)[1],\n",
    "        \"f1_2\": f1_score(labels, predictions, average=None)[2],\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(key, tokenized_dataset, base_model_name=\"allegro/herbert-base-cased\"):\n",
    "  \n",
    "    # Reload model for each run to reset weights\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=3)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{key}\",\n",
    "        num_train_epochs=4,  #\n",
    "        per_device_train_batch_size=16,  \n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\", \n",
    "        greater_is_better=True,\n",
    "        seed=42,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        gradient_accumulation_steps=2,\n",
    "    )\n",
    "\n",
    "    # Initialize custom trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "# Main comparison loop\n",
    "results = []\n",
    "\n",
    "for key, tokenized_dataset in tokenized_datasets.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing preprocessing variant: {key}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        metrics = train_and_evaluate(key, tokenized_dataset)\n",
    "        \n",
    "        results.append({\n",
    "            \"preprocessing\": key,\n",
    "            \"accuracy\": metrics[\"eval_accuracy\"],\n",
    "            \"macro_f1\": metrics[\"eval_f1_macro\"],\n",
    "            \"weighted_f1\": metrics[\"eval_f1_weighted\"],\n",
    "            \"neutral_f1\": metrics[\"eval_f1_0\"],\n",
    "            \"positive_f1\": metrics[\"eval_f1_1\"],\n",
    "            \"negative_f1\": metrics[\"eval_f1_2\"],\n",
    "            \"epochs\": metrics[\"epoch\"]\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with variant {key}: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"preprocessing_comparison.csv\", index=False)\n",
    "print(\"\\nComparison saved to preprocessing_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No_processing</td>\n",
       "      <td>0.755844</td>\n",
       "      <td>0.347139</td>\n",
       "      <td>0.669989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864947</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>3.958763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_processing</td>\n",
       "      <td>0.770235</td>\n",
       "      <td>0.531789</td>\n",
       "      <td>0.756806</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.868293</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Basic_processing_emoji</td>\n",
       "      <td>0.775457</td>\n",
       "      <td>0.443119</td>\n",
       "      <td>0.727558</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basic_processing_spelling</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.466006</td>\n",
       "      <td>0.726951</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.863057</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>3.958763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basic_processing_spelling_lem</td>\n",
       "      <td>0.751309</td>\n",
       "      <td>0.449214</td>\n",
       "      <td>0.703247</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Basic_processing_spelling_lem_SP</td>\n",
       "      <td>0.746073</td>\n",
       "      <td>0.415375</td>\n",
       "      <td>0.688496</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      preprocessing  accuracy  macro_f1  weighted_f1  neutral_f1  positive_f1  negative_f1    epochs\n",
       "0                     No_processing  0.755844  0.347139     0.669989    0.000000     0.864947     0.176471  3.958763\n",
       "1                  Basic_processing  0.770235  0.531789     0.756806    0.322581     0.868293     0.404494  4.000000\n",
       "2            Basic_processing_emoji  0.775457  0.443119     0.727558    0.181818     0.878882     0.268657  4.000000\n",
       "3         Basic_processing_spelling  0.753247  0.466006     0.726951    0.266667     0.863057     0.268293  3.958763\n",
       "4     Basic_processing_spelling_lem  0.751309  0.449214     0.703247    0.230769     0.864000     0.252874  4.000000\n",
       "5  Basic_processing_spelling_lem_SP  0.746073  0.415375     0.688496    0.142857     0.856693     0.246575  4.000000"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Dataset.from_pandas(dataset_labeled_proc_emoji)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",max_length=128),\n",
    "    batched=True,\n",
    "    remove_columns=['text','__index_level_0__']\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=12).train_test_split(test_size=0.2)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   0%|          | 0/32 [00:00<?, ?it/s]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:   0%|          | 0/32 [02:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0828, 'grad_norm': 6.178417682647705, 'learning_rate': 9.868421052631579e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "Grid Search:   0%|          | 0/32 [04:25<?, ?it/s]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3797132400165030647\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-3797132400165030647\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0364373922348022, 'eval_accuracy': 0.4621409921671018, 'eval_f1_macro': 0.3692101497470625, 'eval_f1_weighted': 0.5256156940230047, 'eval_f1_0': 0.2702702702702703, 'eval_f1_1': 0.6040268456375839, 'eval_f1_2': 0.23333333333333334, 'eval_runtime': 16.1654, 'eval_samples_per_second': 23.693, 'eval_steps_per_second': 0.742, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3797132400165030647\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:   0%|          | 0/32 [04:47<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0701, 'grad_norm': 6.517819881439209, 'learning_rate': 1.4770501994544978e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   0%|          | 0/32 [06:56<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9653, 'grad_norm': 10.031152725219727, 'learning_rate': 1.2911770957897966e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "Grid Search:   0%|          | 0/32 [08:58<?, ?it/s]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3797132400165030647\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-3797132400165030647\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8841254115104675, 'eval_accuracy': 0.6684073107049608, 'eval_f1_macro': 0.5351011103996052, 'eval_f1_weighted': 0.7036639697047318, 'eval_f1_0': 0.3790849673202614, 'eval_f1_1': 0.7871939736346516, 'eval_f1_2': 0.43902439024390244, 'eval_runtime': 17.4814, 'eval_samples_per_second': 21.909, 'eval_steps_per_second': 0.686, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3797132400165030647\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:   0%|          | 0/32 [09:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8442, 'grad_norm': 11.051763534545898, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   0%|          | 0/32 [11:34<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7485, 'grad_norm': 13.322094917297363, 'learning_rate': 5.809510829881456e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "Grid Search:   0%|          | 0/32 [13:14<?, ?it/s]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3797132400165030647\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-3797132400165030647\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8227585554122925, 'eval_accuracy': 0.6736292428198434, 'eval_f1_macro': 0.5469349988433958, 'eval_f1_weighted': 0.7069448798067777, 'eval_f1_0': 0.4, 'eval_f1_1': 0.7862595419847328, 'eval_f1_2': 0.45454545454545453, 'eval_runtime': 14.5127, 'eval_samples_per_second': 26.391, 'eval_steps_per_second': 0.827, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3797132400165030647\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:   0%|          | 0/32 [13:56<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6532, 'grad_norm': 13.985437393188477, 'learning_rate': 2.4203882128069435e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   0%|          | 0/32 [15:49<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5976, 'grad_norm': 19.82370948791504, 'learning_rate': 3.5755677843889197e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-3797132400165030647\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-3797132400165030647\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-3797132400165030647\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "Grid Search:   0%|          | 0/32 [17:25<?, ?it/s]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3797132400165030647\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-3797132400165030647\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8347712159156799, 'eval_accuracy': 0.6762402088772846, 'eval_f1_macro': 0.5319132822558419, 'eval_f1_weighted': 0.7058015272805691, 'eval_f1_0': 0.3595505617977528, 'eval_f1_1': 0.7917448405253283, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 16.0535, 'eval_samples_per_second': 23.858, 'eval_steps_per_second': 0.748, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3797132400165030647\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-3797132400165030647\\checkpoint-286 (score: 0.5469349988433958).\n",
      "\n",
      "100%|██████████| 380/380 [17:29<00:00,  2.76s/it]s]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1049.9197, 'train_samples_per_second': 5.821, 'train_steps_per_second': 0.362, 'train_loss': 0.828236921210038, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.19s/it]\n",
      "Grid Search:   3%|▎         | 1/32 [17:51<9:13:24, 1071.13s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:   3%|▎         | 1/32 [19:48<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1133, 'grad_norm': 5.074554920196533, 'learning_rate': 9.868421052631579e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   3%|▎         | 1/32 [21:46<9:13:24, 1071.13s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3418545846365590156\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-3418545846365590156\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9472247362136841, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.3066496163682864, 'eval_f1_weighted': 0.6870620288074363, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8764705882352941, 'eval_f1_2': 0.043478260869565216, 'eval_runtime': 14.3486, 'eval_samples_per_second': 26.693, 'eval_steps_per_second': 0.836, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3418545846365590156\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:   3%|▎         | 1/32 [22:06<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9902, 'grad_norm': 5.795541763305664, 'learning_rate': 1.4770501994544978e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   3%|▎         | 1/32 [24:02<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9095, 'grad_norm': 8.590526580810547, 'learning_rate': 1.2911770957897966e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   3%|▎         | 1/32 [25:50<9:13:24, 1071.13s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3418545846365590156\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-3418545846365590156\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7995398044586182, 'eval_accuracy': 0.7545691906005222, 'eval_f1_macro': 0.5397607986021753, 'eval_f1_weighted': 0.7540645195910042, 'eval_f1_0': 0.37735849056603776, 'eval_f1_1': 0.8609715242881072, 'eval_f1_2': 0.38095238095238093, 'eval_runtime': 14.3311, 'eval_samples_per_second': 26.725, 'eval_steps_per_second': 0.837, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3418545846365590156\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:   3%|▎         | 1/32 [26:19<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8028, 'grad_norm': 7.530238151550293, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   3%|▎         | 1/32 [28:13<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.724, 'grad_norm': 19.474069595336914, 'learning_rate': 5.809510829881456e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   3%|▎         | 1/32 [29:49<9:13:24, 1071.13s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3418545846365590156\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-3418545846365590156\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7352597117424011, 'eval_accuracy': 0.7127937336814621, 'eval_f1_macro': 0.5263336391182275, 'eval_f1_weighted': 0.7272146886492109, 'eval_f1_0': 0.3333333333333333, 'eval_f1_1': 0.8266199649737302, 'eval_f1_2': 0.41904761904761906, 'eval_runtime': 13.8794, 'eval_samples_per_second': 27.595, 'eval_steps_per_second': 0.865, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3418545846365590156\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:   3%|▎         | 1/32 [30:28<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6784, 'grad_norm': 14.99860668182373, 'learning_rate': 2.4203882128069435e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   3%|▎         | 1/32 [32:20<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5872, 'grad_norm': 16.280630111694336, 'learning_rate': 3.5755677843889197e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-3418545846365590156\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-3418545846365590156\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-3418545846365590156\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   3%|▎         | 1/32 [33:53<9:13:24, 1071.13s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3418545846365590156\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-3418545846365590156\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7193928360939026, 'eval_accuracy': 0.7310704960835509, 'eval_f1_macro': 0.5414918390152661, 'eval_f1_weighted': 0.7389155946230196, 'eval_f1_0': 0.345679012345679, 'eval_f1_1': 0.8364888123924269, 'eval_f1_2': 0.4423076923076923, 'eval_runtime': 14.0509, 'eval_samples_per_second': 27.258, 'eval_steps_per_second': 0.854, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3418545846365590156\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-3418545846365590156\\checkpoint-380 (score: 0.5414918390152661).\n",
      "\n",
      "Grid Search:   3%|▎         | 1/32 [34:02<9:13:24, 1071.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 969.4038, 'train_samples_per_second': 6.305, 'train_steps_per_second': 0.392, 'train_loss': 0.8052685737609864, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [16:09<00:00,  2.55s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n",
      "Grid Search:   6%|▋         | 2/32 [34:17<8:30:36, 1021.21s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:   6%|▋         | 2/32 [36:10<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1319, 'grad_norm': 5.511590003967285, 'learning_rate': 9.868421052631579e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   6%|▋         | 2/32 [38:04<8:30:36, 1021.21s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8422084521469549044\\checkpoint-95\n",
      "Configuration saved in ./grid_results/8422084521469549044\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0443320274353027, 'eval_accuracy': 0.6370757180156658, 'eval_f1_macro': 0.4472302331170392, 'eval_f1_weighted': 0.6668366837907388, 'eval_f1_0': 0.21052631578947367, 'eval_f1_1': 0.775, 'eval_f1_2': 0.3561643835616438, 'eval_runtime': 13.7164, 'eval_samples_per_second': 27.923, 'eval_steps_per_second': 0.875, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8422084521469549044\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:   6%|▋         | 2/32 [38:23<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0751, 'grad_norm': 6.3630499839782715, 'learning_rate': 1.4770501994544978e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   6%|▋         | 2/32 [40:14<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9864, 'grad_norm': 10.221540451049805, 'learning_rate': 1.2911770957897966e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   6%|▋         | 2/32 [41:59<8:30:36, 1021.21s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8422084521469549044\\checkpoint-191\n",
      "Configuration saved in ./grid_results/8422084521469549044\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8944219350814819, 'eval_accuracy': 0.6135770234986945, 'eval_f1_macro': 0.47045450840021635, 'eval_f1_weighted': 0.6600801522869071, 'eval_f1_0': 0.3103448275862069, 'eval_f1_1': 0.7543520309477756, 'eval_f1_2': 0.3466666666666667, 'eval_runtime': 14.0762, 'eval_samples_per_second': 27.209, 'eval_steps_per_second': 0.853, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8422084521469549044\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:   6%|▋         | 2/32 [42:36<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8835, 'grad_norm': 7.622568607330322, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   6%|▋         | 2/32 [44:27<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7938, 'grad_norm': 14.132046699523926, 'learning_rate': 5.809510829881456e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   6%|▋         | 2/32 [46:00<8:30:36, 1021.21s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8422084521469549044\\checkpoint-286\n",
      "Configuration saved in ./grid_results/8422084521469549044\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7911185622215271, 'eval_accuracy': 0.6501305483028721, 'eval_f1_macro': 0.5407467868988391, 'eval_f1_weighted': 0.6857546877232635, 'eval_f1_0': 0.4166666666666667, 'eval_f1_1': 0.7578125, 'eval_f1_2': 0.44776119402985076, 'eval_runtime': 13.7499, 'eval_samples_per_second': 27.855, 'eval_steps_per_second': 0.873, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8422084521469549044\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:   6%|▋         | 2/32 [46:40<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7054, 'grad_norm': 16.159067153930664, 'learning_rate': 2.4203882128069435e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   6%|▋         | 2/32 [48:31<8:30:36, 1021.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6269, 'grad_norm': 15.850446701049805, 'learning_rate': 3.5755677843889197e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/8422084521469549044\\checkpoint-380\n",
      "Configuration saved in ./grid_results/8422084521469549044\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/8422084521469549044\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:   6%|▋         | 2/32 [50:01<8:30:36, 1021.21s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8422084521469549044\\checkpoint-380\n",
      "Configuration saved in ./grid_results/8422084521469549044\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.786878764629364, 'eval_accuracy': 0.6945169712793734, 'eval_f1_macro': 0.5648758709581005, 'eval_f1_weighted': 0.7210260341111671, 'eval_f1_0': 0.425531914893617, 'eval_f1_1': 0.7985074626865671, 'eval_f1_2': 0.47058823529411764, 'eval_runtime': 14.1766, 'eval_samples_per_second': 27.016, 'eval_steps_per_second': 0.846, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8422084521469549044\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/8422084521469549044\\checkpoint-380 (score: 0.5648758709581005).\n",
      "\n",
      "100%|██████████| 380/380 [15:51<00:00,  2.50s/it], 1021.21s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 951.0615, 'train_samples_per_second': 6.427, 'train_steps_per_second': 0.4, 'train_loss': 0.8620653202659205, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:   9%|▉         | 3/32 [50:23<8:01:31, 996.27s/it] C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:   9%|▉         | 3/32 [52:15<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1133, 'grad_norm': 5.074616432189941, 'learning_rate': 9.868421052631579e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                              \n",
      "Grid Search:   9%|▉         | 3/32 [54:09<8:01:31, 996.27s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7997577208568208382\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-7997577208568208382\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9472248554229736, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.3066496163682864, 'eval_f1_weighted': 0.6870620288074363, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8764705882352941, 'eval_f1_2': 0.043478260869565216, 'eval_runtime': 13.6217, 'eval_samples_per_second': 28.117, 'eval_steps_per_second': 0.881, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7997577208568208382\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:   9%|▉         | 3/32 [54:28<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9902, 'grad_norm': 5.795707702636719, 'learning_rate': 1.4770501994544978e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   9%|▉         | 3/32 [56:19<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9095, 'grad_norm': 8.588546752929688, 'learning_rate': 1.2911770957897966e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                              \n",
      "Grid Search:   9%|▉         | 3/32 [58:04<8:01:31, 996.27s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7997577208568208382\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-7997577208568208382\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8028464913368225, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.5401484213414377, 'eval_f1_weighted': 0.7509683358786147, 'eval_f1_0': 0.3584905660377358, 'eval_f1_1': 0.8557046979865772, 'eval_f1_2': 0.40625, 'eval_runtime': 13.8316, 'eval_samples_per_second': 27.69, 'eval_steps_per_second': 0.868, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7997577208568208382\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:   9%|▉         | 3/32 [58:33<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8041, 'grad_norm': 7.568936824798584, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   9%|▉         | 3/32 [1:00:27<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7255, 'grad_norm': 10.184531211853027, 'learning_rate': 5.809510829881456e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:   9%|▉         | 3/32 [1:02:03<8:01:31, 996.27s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7997577208568208382\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-7997577208568208382\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7344803214073181, 'eval_accuracy': 0.7180156657963447, 'eval_f1_macro': 0.5275148247531969, 'eval_f1_weighted': 0.7295696511913343, 'eval_f1_0': 0.3488372093023256, 'eval_f1_1': 0.8298611111111112, 'eval_f1_2': 0.40384615384615385, 'eval_runtime': 13.8456, 'eval_samples_per_second': 27.662, 'eval_steps_per_second': 0.867, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7997577208568208382\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:   9%|▉         | 3/32 [1:02:46<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6803, 'grad_norm': 15.831894874572754, 'learning_rate': 2.4203882128069435e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:   9%|▉         | 3/32 [1:04:36<8:01:31, 996.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5868, 'grad_norm': 15.300342559814453, 'learning_rate': 3.5755677843889197e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-7997577208568208382\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-7997577208568208382\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-7997577208568208382\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:   9%|▉         | 3/32 [1:06:06<8:01:31, 996.27s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7997577208568208382\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-7997577208568208382\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7207823395729065, 'eval_accuracy': 0.7284595300261096, 'eval_f1_macro': 0.5359716749189497, 'eval_f1_weighted': 0.7360204237322276, 'eval_f1_0': 0.345679012345679, 'eval_f1_1': 0.8350515463917526, 'eval_f1_2': 0.42718446601941745, 'eval_runtime': 14.045, 'eval_samples_per_second': 27.269, 'eval_steps_per_second': 0.854, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7997577208568208382\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-7997577208568208382\\checkpoint-191 (score: 0.5401484213414377).\n",
      "\n",
      "100%|██████████| 380/380 [16:00<00:00,  2.53s/it]31, 996.27s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 960.3172, 'train_samples_per_second': 6.365, 'train_steps_per_second': 0.396, 'train_loss': 0.8058414409035131, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.08s/it]\n",
      "Grid Search:  12%|█▎        | 4/32 [1:06:39<7:41:10, 988.23s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:08:33<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1362, 'grad_norm': 5.087227821350098, 'learning_rate': 7.894736842105263e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  12%|█▎        | 4/32 [1:10:27<7:41:10, 988.23s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/30910608043593877\\checkpoint-95\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.050794005393982, 'eval_accuracy': 0.6684073107049608, 'eval_f1_macro': 0.427069927069927, 'eval_f1_weighted': 0.6746205362393352, 'eval_f1_0': 0.1981981981981982, 'eval_f1_1': 0.7972972972972973, 'eval_f1_2': 0.2857142857142857, 'eval_runtime': 14.0729, 'eval_samples_per_second': 27.216, 'eval_steps_per_second': 0.853, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:10:47<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0789, 'grad_norm': 5.93795108795166, 'learning_rate': 1.4993593187904756e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:12:40<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0041, 'grad_norm': 9.832590103149414, 'learning_rate': 1.4237932364189841e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  12%|█▎        | 4/32 [1:14:31<7:41:10, 988.23s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/30910608043593877\\checkpoint-191\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9123504161834717, 'eval_accuracy': 0.618798955613577, 'eval_f1_macro': 0.47397146354725495, 'eval_f1_weighted': 0.6642082462998028, 'eval_f1_0': 0.3050847457627119, 'eval_f1_1': 0.7586206896551724, 'eval_f1_2': 0.3582089552238806, 'eval_runtime': 15.0876, 'eval_samples_per_second': 25.385, 'eval_steps_per_second': 0.795, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:15:02<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9015, 'grad_norm': 8.58504867553711, 'learning_rate': 1.2347244283957055e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:16:57<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8006, 'grad_norm': 14.988029479980469, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  12%|█▎        | 4/32 [1:18:35<7:41:10, 988.23s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/30910608043593877\\checkpoint-286\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8225792646408081, 'eval_accuracy': 0.6866840731070496, 'eval_f1_macro': 0.5395527031088417, 'eval_f1_weighted': 0.7128917387314444, 'eval_f1_0': 0.3655913978494624, 'eval_f1_1': 0.7985212569316081, 'eval_f1_2': 0.45454545454545453, 'eval_runtime': 14.6843, 'eval_samples_per_second': 26.082, 'eval_steps_per_second': 0.817, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:19:16<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7081, 'grad_norm': 14.260416984558105, 'learning_rate': 6.572305265479892e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:21:12<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5788, 'grad_norm': 24.68426513671875, 'learning_rate': 3.6608621318279945e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  12%|█▎        | 4/32 [1:22:40<7:41:10, 988.23s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/30910608043593877\\checkpoint-382\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8158832788467407, 'eval_accuracy': 0.6814621409921671, 'eval_f1_macro': 0.5305350172168026, 'eval_f1_weighted': 0.7059206103951295, 'eval_f1_0': 0.3595505617977528, 'eval_f1_1': 0.7926605504587156, 'eval_f1_2': 0.4393939393939394, 'eval_runtime': 14.4175, 'eval_samples_per_second': 26.565, 'eval_steps_per_second': 0.832, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:23:32<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4822, 'grad_norm': 16.932600021362305, 'learning_rate': 1.396134620473538e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  12%|█▎        | 4/32 [1:25:28<7:41:10, 988.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4588, 'grad_norm': 16.136198043823242, 'learning_rate': 1.596236600448414e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/30910608043593877\\checkpoint-475\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  12%|█▎        | 4/32 [1:26:53<7:41:10, 988.23s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/30910608043593877\\checkpoint-475\n",
      "Configuration saved in ./grid_results/30910608043593877\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8254152536392212, 'eval_accuracy': 0.7023498694516971, 'eval_f1_macro': 0.5391267844167413, 'eval_f1_weighted': 0.7213626338797116, 'eval_f1_0': 0.3614457831325301, 'eval_f1_1': 0.8114901256732495, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 14.7965, 'eval_samples_per_second': 25.885, 'eval_steps_per_second': 0.811, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/30910608043593877\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/30910608043593877\\checkpoint-286 (score: 0.5395527031088417).\n",
      "\n",
      "100%|██████████| 475/475 [20:32<00:00,  2.59s/it]10, 988.23s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1232.3461, 'train_samples_per_second': 6.2, 'train_steps_per_second': 0.385, 'train_loss': 0.7747598346910979, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n",
      "Grid Search:  16%|█▌        | 5/32 [1:27:28<8:06:54, 1082.02s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:29:25<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0618, 'grad_norm': 4.257857799530029, 'learning_rate': 7.894736842105263e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  16%|█▌        | 5/32 [1:31:22<8:06:54, 1082.02s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-95\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9485692977905273, 'eval_accuracy': 0.7754569190600522, 'eval_f1_macro': 0.2911764705882353, 'eval_f1_weighted': 0.6796651819996928, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8735294117647059, 'eval_f1_2': 0.0, 'eval_runtime': 14.0298, 'eval_samples_per_second': 27.299, 'eval_steps_per_second': 0.855, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:31:43<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.991, 'grad_norm': 6.103188514709473, 'learning_rate': 1.4993593187904756e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:33:35<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9338, 'grad_norm': 8.338629722595215, 'learning_rate': 1.4237932364189841e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  16%|█▌        | 5/32 [1:35:28<8:06:54, 1082.02s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-191\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7938119173049927, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.5547607266229858, 'eval_f1_weighted': 0.7578912876001771, 'eval_f1_0': 0.3925233644859813, 'eval_f1_1': 0.8590604026845637, 'eval_f1_2': 0.4126984126984127, 'eval_runtime': 16.4919, 'eval_samples_per_second': 23.223, 'eval_steps_per_second': 0.728, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:36:03<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8153, 'grad_norm': 7.027233123779297, 'learning_rate': 1.2347244283957055e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:38:09<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7359, 'grad_norm': 11.33475399017334, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  16%|█▌        | 5/32 [1:39:59<8:06:54, 1082.02s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-286\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7277739644050598, 'eval_accuracy': 0.7597911227154047, 'eval_f1_macro': 0.5800147987541514, 'eval_f1_weighted': 0.7641424632364091, 'eval_f1_0': 0.4, 'eval_f1_1': 0.8551959114139693, 'eval_f1_2': 0.48484848484848486, 'eval_runtime': 16.6565, 'eval_samples_per_second': 22.994, 'eval_steps_per_second': 0.72, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:40:46<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6613, 'grad_norm': 12.794477462768555, 'learning_rate': 6.572305265479892e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:42:55<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.546, 'grad_norm': 17.719409942626953, 'learning_rate': 3.6608621318279945e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  16%|█▌        | 5/32 [1:44:34<8:06:54, 1082.02s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-382\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7143239378929138, 'eval_accuracy': 0.7597911227154047, 'eval_f1_macro': 0.5763247100802854, 'eval_f1_weighted': 0.7622772468632306, 'eval_f1_0': 0.39473684210526316, 'eval_f1_1': 0.8542372881355932, 'eval_f1_2': 0.48, 'eval_runtime': 16.7009, 'eval_samples_per_second': 22.933, 'eval_steps_per_second': 0.719, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:45:33<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4467, 'grad_norm': 10.087166786193848, 'learning_rate': 1.396134620473538e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  16%|█▌        | 5/32 [1:47:33<8:06:54, 1082.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4292, 'grad_norm': 12.741214752197266, 'learning_rate': 1.596236600448414e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-475\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  16%|█▌        | 5/32 [1:48:59<8:06:54, 1082.02s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3701228453904520582\\checkpoint-475\n",
      "Configuration saved in ./grid_results/3701228453904520582\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7191265821456909, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.5762954193427453, 'eval_f1_weighted': 0.758738393441759, 'eval_f1_0': 0.38461538461538464, 'eval_f1_1': 0.8486956521739131, 'eval_f1_2': 0.49557522123893805, 'eval_runtime': 15.7728, 'eval_samples_per_second': 24.282, 'eval_steps_per_second': 0.761, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3701228453904520582\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/3701228453904520582\\checkpoint-286 (score: 0.5800147987541514).\n",
      "\n",
      "100%|██████████| 475/475 [21:54<00:00,  2.77s/it]54, 1082.02s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1314.266, 'train_samples_per_second': 5.813, 'train_steps_per_second': 0.361, 'train_loss': 0.7194648923371968, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.22s/it]\n",
      "Grid Search:  19%|█▉        | 6/32 [1:49:39<8:25:39, 1166.89s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [1:51:44<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0969, 'grad_norm': 5.579617023468018, 'learning_rate': 7.894736842105263e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  19%|█▉        | 6/32 [1:53:54<8:25:39, 1166.89s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-95\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.050734043121338, 'eval_accuracy': 0.7728459530026109, 'eval_f1_macro': 0.46642675948321627, 'eval_f1_weighted': 0.7357012602597596, 'eval_f1_0': 0.23880597014925373, 'eval_f1_1': 0.8695652173913043, 'eval_f1_2': 0.2909090909090909, 'eval_runtime': 15.4194, 'eval_samples_per_second': 24.839, 'eval_steps_per_second': 0.778, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [1:54:15<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0718, 'grad_norm': 11.665566444396973, 'learning_rate': 1.4993593187904756e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [1:56:17<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0247, 'grad_norm': 10.479418754577637, 'learning_rate': 1.4237932364189841e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  19%|█▉        | 6/32 [1:58:03<8:25:39, 1166.89s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-191\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9098522067070007, 'eval_accuracy': 0.6527415143603134, 'eval_f1_macro': 0.5216052934407365, 'eval_f1_weighted': 0.6895072364922317, 'eval_f1_0': 0.3670886075949367, 'eval_f1_1': 0.7727272727272727, 'eval_f1_2': 0.425, 'eval_runtime': 14.2519, 'eval_samples_per_second': 26.874, 'eval_steps_per_second': 0.842, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [1:58:35<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9066, 'grad_norm': 7.776177406311035, 'learning_rate': 1.2347244283957055e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:00:35<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.828, 'grad_norm': 13.276607513427734, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  19%|█▉        | 6/32 [2:02:20<8:25:39, 1166.89s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-286\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8446640968322754, 'eval_accuracy': 0.6710182767624021, 'eval_f1_macro': 0.5388629135864872, 'eval_f1_weighted': 0.7011495129864506, 'eval_f1_0': 0.40816326530612246, 'eval_f1_1': 0.7819548872180451, 'eval_f1_2': 0.4264705882352941, 'eval_runtime': 15.4035, 'eval_samples_per_second': 24.864, 'eval_steps_per_second': 0.779, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:03:03<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7105, 'grad_norm': 17.141130447387695, 'learning_rate': 6.572305265479892e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:05:05<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.592, 'grad_norm': 19.821670532226562, 'learning_rate': 3.6608621318279945e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  19%|█▉        | 6/32 [2:06:39<8:25:39, 1166.89s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-382\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8355993628501892, 'eval_accuracy': 0.7232375979112271, 'eval_f1_macro': 0.5668712602553821, 'eval_f1_weighted': 0.7406579298383336, 'eval_f1_0': 0.39080459770114945, 'eval_f1_1': 0.8264758497316637, 'eval_f1_2': 0.48333333333333334, 'eval_runtime': 15.586, 'eval_samples_per_second': 24.573, 'eval_steps_per_second': 0.77, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:07:31<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5324, 'grad_norm': 27.09044075012207, 'learning_rate': 1.396134620473538e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:09:33<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4897, 'grad_norm': 16.480667114257812, 'learning_rate': 1.596236600448414e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-475\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  19%|█▉        | 6/32 [2:10:52<8:25:39, 1166.89s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1965007059955465474\\checkpoint-475\n",
      "Configuration saved in ./grid_results/1965007059955465474\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.818594753742218, 'eval_accuracy': 0.7154046997389034, 'eval_f1_macro': 0.5756502581302818, 'eval_f1_weighted': 0.7381513731847579, 'eval_f1_0': 0.40860215053763443, 'eval_f1_1': 0.818348623853211, 'eval_f1_2': 0.5, 'eval_runtime': 14.4473, 'eval_samples_per_second': 26.51, 'eval_steps_per_second': 0.831, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1965007059955465474\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/1965007059955465474\\checkpoint-475 (score: 0.5756502581302818).\n",
      "\n",
      "Grid Search:  19%|█▉        | 6/32 [2:11:00<8:25:39, 1166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1279.0222, 'train_samples_per_second': 5.973, 'train_steps_per_second': 0.371, 'train_loss': 0.7894262113069234, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475/475 [21:19<00:00,  2.69s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n",
      "Grid Search:  22%|██▏       | 7/32 [2:11:15<8:23:42, 1208.91s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:13:11<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0618, 'grad_norm': 4.257888317108154, 'learning_rate': 7.894736842105263e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  22%|██▏       | 7/32 [2:15:09<8:23:42, 1208.91s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9485697746276855, 'eval_accuracy': 0.7754569190600522, 'eval_f1_macro': 0.2911764705882353, 'eval_f1_weighted': 0.6796651819996928, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8735294117647059, 'eval_f1_2': 0.0, 'eval_runtime': 14.3335, 'eval_samples_per_second': 26.721, 'eval_steps_per_second': 0.837, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:15:27<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.991, 'grad_norm': 6.103382587432861, 'learning_rate': 1.4993593187904756e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:17:23<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9338, 'grad_norm': 8.33857536315918, 'learning_rate': 1.4237932364189841e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  22%|██▏       | 7/32 [2:19:12<8:23:42, 1208.91s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.793812096118927, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.5547607266229858, 'eval_f1_weighted': 0.7578912876001771, 'eval_f1_0': 0.3925233644859813, 'eval_f1_1': 0.8590604026845637, 'eval_f1_2': 0.4126984126984127, 'eval_runtime': 14.064, 'eval_samples_per_second': 27.233, 'eval_steps_per_second': 0.853, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:19:40<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8153, 'grad_norm': 7.027476787567139, 'learning_rate': 1.2347244283957055e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:21:34<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7359, 'grad_norm': 11.41559886932373, 'learning_rate': 9.64002168186829e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  22%|██▏       | 7/32 [2:23:11<8:23:42, 1208.91s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7283068895339966, 'eval_accuracy': 0.7597911227154047, 'eval_f1_macro': 0.5800147987541514, 'eval_f1_weighted': 0.7641424632364091, 'eval_f1_0': 0.4, 'eval_f1_1': 0.8551959114139693, 'eval_f1_2': 0.48484848484848486, 'eval_runtime': 14.4145, 'eval_samples_per_second': 26.571, 'eval_steps_per_second': 0.832, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:23:50<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6617, 'grad_norm': 12.753382682800293, 'learning_rate': 6.572305265479892e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:25:44<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5455, 'grad_norm': 17.87042808532715, 'learning_rate': 3.6608621318279945e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  22%|██▏       | 7/32 [2:27:11<8:23:42, 1208.91s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-382\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7156464457511902, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.5740921290770843, 'eval_f1_weighted': 0.7602053116693974, 'eval_f1_0': 0.39473684210526316, 'eval_f1_1': 0.8522920203735145, 'eval_f1_2': 0.4752475247524752, 'eval_runtime': 14.3948, 'eval_samples_per_second': 26.607, 'eval_steps_per_second': 0.834, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:28:00<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4462, 'grad_norm': 10.071828842163086, 'learning_rate': 1.396134620473538e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  22%|██▏       | 7/32 [2:30:02<8:23:42, 1208.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4294, 'grad_norm': 12.814837455749512, 'learning_rate': 1.596236600448414e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  22%|██▏       | 7/32 [2:31:38<8:23:42, 1208.91s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7100026899492633663\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-7100026899492633663\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7207778692245483, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.5734406583626689, 'eval_f1_weighted': 0.7580785855466724, 'eval_f1_0': 0.38461538461538464, 'eval_f1_1': 0.8492201039861352, 'eval_f1_2': 0.4864864864864865, 'eval_runtime': 14.1311, 'eval_samples_per_second': 27.103, 'eval_steps_per_second': 0.849, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7100026899492633663\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-7100026899492633663\\checkpoint-286 (score: 0.5800147987541514).\n",
      "\n",
      "100%|██████████| 475/475 [20:31<00:00,  2.59s/it]42, 1208.91s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1231.58, 'train_samples_per_second': 6.203, 'train_steps_per_second': 0.386, 'train_loss': 0.7194779968261719, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n",
      "Grid Search:  25%|██▌       | 8/32 [2:32:02<8:08:25, 1221.08s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  25%|██▌       | 8/32 [2:35:25<8:08:25, 1221.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8677942715838026781\\checkpoint-48\n",
      "Configuration saved in ./grid_results/8677942715838026781\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0668338537216187, 'eval_accuracy': 0.6919060052219321, 'eval_f1_macro': 0.403902270656294, 'eval_f1_weighted': 0.6794815008917371, 'eval_f1_0': 0.10526315789473684, 'eval_f1_1': 0.8151815181518152, 'eval_f1_2': 0.2912621359223301, 'eval_runtime': 13.982, 'eval_samples_per_second': 27.392, 'eval_steps_per_second': 0.858, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8677942715838026781\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  25%|██▌       | 8/32 [2:35:40<8:08:25, 1221.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0943, 'grad_norm': 4.541919231414795, 'learning_rate': 1.4809503753311253e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  25%|██▌       | 8/32 [2:38:58<8:08:25, 1221.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8677942715838026781\\checkpoint-96\n",
      "Configuration saved in ./grid_results/8677942715838026781\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9884774088859558, 'eval_accuracy': 0.6553524804177546, 'eval_f1_macro': 0.48017311182465444, 'eval_f1_weighted': 0.6883012463599386, 'eval_f1_0': 0.28, 'eval_f1_1': 0.7912885662431942, 'eval_f1_2': 0.36923076923076925, 'eval_runtime': 13.916, 'eval_samples_per_second': 27.522, 'eval_steps_per_second': 0.862, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8677942715838026781\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  25%|██▌       | 8/32 [2:39:24<8:08:25, 1221.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0226, 'grad_norm': 6.989912509918213, 'learning_rate': 9.846900304741159e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  25%|██▌       | 8/32 [2:42:29<8:08:25, 1221.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8677942715838026781\\checkpoint-144\n",
      "Configuration saved in ./grid_results/8677942715838026781\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8869822025299072, 'eval_accuracy': 0.618798955613577, 'eval_f1_macro': 0.5119818089751306, 'eval_f1_weighted': 0.6594498464245466, 'eval_f1_0': 0.4028776978417266, 'eval_f1_1': 0.7330677290836654, 'eval_f1_2': 0.4, 'eval_runtime': 13.8843, 'eval_samples_per_second': 27.585, 'eval_steps_per_second': 0.864, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8677942715838026781\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  25%|██▌       | 8/32 [2:43:00<8:08:25, 1221.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8674, 'grad_norm': 7.707082271575928, 'learning_rate': 2.6203627484681864e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/8677942715838026781\\checkpoint-192\n",
      "Configuration saved in ./grid_results/8677942715838026781\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/8677942715838026781\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  25%|██▌       | 8/32 [2:46:08<8:08:25, 1221.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8677942715838026781\\checkpoint-192\n",
      "Configuration saved in ./grid_results/8677942715838026781\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8805291652679443, 'eval_accuracy': 0.6135770234986945, 'eval_f1_macro': 0.5027781249900588, 'eval_f1_weighted': 0.6553698444250763, 'eval_f1_0': 0.39316239316239315, 'eval_f1_1': 0.731610337972167, 'eval_f1_2': 0.3835616438356164, 'eval_runtime': 14.4826, 'eval_samples_per_second': 26.446, 'eval_steps_per_second': 0.829, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8677942715838026781\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/8677942715838026781\\checkpoint-144 (score: 0.5119818089751306).\n",
      "\n",
      "Grid Search:  25%|██▌       | 8/32 [2:46:16<8:08:25, 1221.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 852.9415, 'train_samples_per_second': 7.166, 'train_steps_per_second': 0.225, 'train_loss': 0.9494467576344808, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [14:13<00:00,  4.44s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  28%|██▊       | 9/32 [2:46:30<7:05:49, 1110.86s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  28%|██▊       | 9/32 [2:49:56<7:05:49, 1110.86s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-110049970556425921\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-110049970556425921\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9432957768440247, 'eval_accuracy': 0.7780678851174935, 'eval_f1_macro': 0.29172785119921685, 'eval_f1_weighted': 0.6809522166373364, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8751835535976505, 'eval_f1_2': 0.0, 'eval_runtime': 13.8039, 'eval_samples_per_second': 27.746, 'eval_steps_per_second': 0.869, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-110049970556425921\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  28%|██▊       | 9/32 [2:50:15<7:05:49, 1110.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0251, 'grad_norm': 4.243741035461426, 'learning_rate': 1.4809503753311253e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  28%|██▊       | 9/32 [2:53:27<7:05:49, 1110.86s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-110049970556425921\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-110049970556425921\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8723844885826111, 'eval_accuracy': 0.6892950391644909, 'eval_f1_macro': 0.49078717175916303, 'eval_f1_weighted': 0.7104921957296513, 'eval_f1_0': 0.3088235294117647, 'eval_f1_1': 0.8197879858657244, 'eval_f1_2': 0.34375, 'eval_runtime': 13.671, 'eval_samples_per_second': 28.016, 'eval_steps_per_second': 0.878, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-110049970556425921\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  28%|██▊       | 9/32 [2:53:54<7:05:49, 1110.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9113, 'grad_norm': 5.6686530113220215, 'learning_rate': 9.846900304741159e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  28%|██▊       | 9/32 [2:57:01<7:05:49, 1110.86s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-110049970556425921\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-110049970556425921\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8052471876144409, 'eval_accuracy': 0.741514360313316, 'eval_f1_macro': 0.5467041549515776, 'eval_f1_weighted': 0.7513146593386694, 'eval_f1_0': 0.3333333333333333, 'eval_f1_1': 0.852233676975945, 'eval_f1_2': 0.45454545454545453, 'eval_runtime': 14.2734, 'eval_samples_per_second': 26.833, 'eval_steps_per_second': 0.841, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-110049970556425921\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  28%|██▊       | 9/32 [2:57:34<7:05:49, 1110.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7839, 'grad_norm': 8.890796661376953, 'learning_rate': 2.6203627484681864e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-110049970556425921\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-110049970556425921\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/-110049970556425921\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  28%|██▊       | 9/32 [3:00:42<7:05:49, 1110.86s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-110049970556425921\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-110049970556425921\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7974128127098083, 'eval_accuracy': 0.7467362924281984, 'eval_f1_macro': 0.5570814586667472, 'eval_f1_weighted': 0.7559414122458936, 'eval_f1_0': 0.3333333333333333, 'eval_f1_1': 0.8537005163511188, 'eval_f1_2': 0.4842105263157895, 'eval_runtime': 14.1258, 'eval_samples_per_second': 27.113, 'eval_steps_per_second': 0.85, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-110049970556425921\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-110049970556425921\\checkpoint-192 (score: 0.5570814586667472).\n",
      "\n",
      "Grid Search:  28%|██▊       | 9/32 [3:00:53<7:05:49, 1110.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 861.0396, 'train_samples_per_second': 7.098, 'train_steps_per_second': 0.223, 'train_loss': 0.8628859917322794, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [14:21<00:00,  4.49s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  31%|███▏      | 10/32 [3:01:07<6:20:48, 1038.55s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  31%|███▏      | 10/32 [3:04:32<6:20:48, 1038.55s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8783539188923247029\\checkpoint-48\n",
      "Configuration saved in ./grid_results/8783539188923247029\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0533950328826904, 'eval_accuracy': 0.5900783289817232, 'eval_f1_macro': 0.4353246209866108, 'eval_f1_weighted': 0.6366408727714205, 'eval_f1_0': 0.24096385542168675, 'eval_f1_1': 0.7362428842504743, 'eval_f1_2': 0.3287671232876712, 'eval_runtime': 13.7598, 'eval_samples_per_second': 27.835, 'eval_steps_per_second': 0.872, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8783539188923247029\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  31%|███▏      | 10/32 [3:04:49<6:20:48, 1038.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0895, 'grad_norm': 4.568072319030762, 'learning_rate': 1.4809503753311253e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  31%|███▏      | 10/32 [3:08:04<6:20:48, 1038.55s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8783539188923247029\\checkpoint-96\n",
      "Configuration saved in ./grid_results/8783539188923247029\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9581890106201172, 'eval_accuracy': 0.5274151436031331, 'eval_f1_macro': 0.4125041142432447, 'eval_f1_weighted': 0.587738397750885, 'eval_f1_0': 0.2692307692307692, 'eval_f1_1': 0.6749482401656315, 'eval_f1_2': 0.29333333333333333, 'eval_runtime': 13.9771, 'eval_samples_per_second': 27.402, 'eval_steps_per_second': 0.859, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8783539188923247029\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  31%|███▏      | 10/32 [3:08:28<6:20:48, 1038.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9816, 'grad_norm': 8.353384017944336, 'learning_rate': 9.846900304741159e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  31%|███▏      | 10/32 [3:11:32<6:20:48, 1038.55s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8783539188923247029\\checkpoint-144\n",
      "Configuration saved in ./grid_results/8783539188923247029\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9006218314170837, 'eval_accuracy': 0.6422976501305483, 'eval_f1_macro': 0.4940432326209776, 'eval_f1_weighted': 0.6773867956051625, 'eval_f1_0': 0.33043478260869563, 'eval_f1_1': 0.768361581920904, 'eval_f1_2': 0.38333333333333336, 'eval_runtime': 13.9213, 'eval_samples_per_second': 27.512, 'eval_steps_per_second': 0.862, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8783539188923247029\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  31%|███▏      | 10/32 [3:12:04<6:20:48, 1038.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8367, 'grad_norm': 9.37945556640625, 'learning_rate': 2.6203627484681864e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/8783539188923247029\\checkpoint-192\n",
      "Configuration saved in ./grid_results/8783539188923247029\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/8783539188923247029\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  31%|███▏      | 10/32 [3:15:13<6:20:48, 1038.55s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/8783539188923247029\\checkpoint-192\n",
      "Configuration saved in ./grid_results/8783539188923247029\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8949688673019409, 'eval_accuracy': 0.639686684073107, 'eval_f1_macro': 0.4965520891942712, 'eval_f1_weighted': 0.6747683011187037, 'eval_f1_0': 0.3177570093457944, 'eval_f1_1': 0.7628083491461101, 'eval_f1_2': 0.4090909090909091, 'eval_runtime': 14.1329, 'eval_samples_per_second': 27.1, 'eval_steps_per_second': 0.849, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/8783539188923247029\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/8783539188923247029\\checkpoint-192 (score: 0.4965520891942712).\n",
      "\n",
      "100%|██████████| 192/192 [14:13<00:00,  4.44s/it]:48, 1038.55s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 853.383, 'train_samples_per_second': 7.162, 'train_steps_per_second': 0.225, 'train_loss': 0.9227468768755595, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  34%|███▍      | 11/32 [3:15:36<5:45:18, 986.61s/it] C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  34%|███▍      | 11/32 [3:19:03<5:45:18, 986.61s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/652980294361032930\\checkpoint-48\n",
      "Configuration saved in ./grid_results/652980294361032930\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9432957172393799, 'eval_accuracy': 0.7780678851174935, 'eval_f1_macro': 0.29172785119921685, 'eval_f1_weighted': 0.6809522166373364, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8751835535976505, 'eval_f1_2': 0.0, 'eval_runtime': 14.4287, 'eval_samples_per_second': 26.544, 'eval_steps_per_second': 0.832, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/652980294361032930\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  34%|███▍      | 11/32 [3:19:20<5:45:18, 986.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0251, 'grad_norm': 4.244510650634766, 'learning_rate': 1.4809503753311253e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  34%|███▍      | 11/32 [3:22:40<5:45:18, 986.61s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/652980294361032930\\checkpoint-96\n",
      "Configuration saved in ./grid_results/652980294361032930\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8688064217567444, 'eval_accuracy': 0.6919060052219321, 'eval_f1_macro': 0.485115945514962, 'eval_f1_weighted': 0.7112383328363819, 'eval_f1_0': 0.3088235294117647, 'eval_f1_1': 0.823943661971831, 'eval_f1_2': 0.3225806451612903, 'eval_runtime': 14.4427, 'eval_samples_per_second': 26.519, 'eval_steps_per_second': 0.831, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/652980294361032930\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  34%|███▍      | 11/32 [3:23:15<5:45:18, 986.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9073, 'grad_norm': 5.69372034072876, 'learning_rate': 9.846900304741159e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  34%|███▍      | 11/32 [3:26:33<5:45:18, 986.61s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/652980294361032930\\checkpoint-144\n",
      "Configuration saved in ./grid_results/652980294361032930\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8054900169372559, 'eval_accuracy': 0.7362924281984334, 'eval_f1_macro': 0.5467790148194696, 'eval_f1_weighted': 0.7476416580628709, 'eval_f1_0': 0.3157894736842105, 'eval_f1_1': 0.846286701208981, 'eval_f1_2': 0.4782608695652174, 'eval_runtime': 13.9896, 'eval_samples_per_second': 27.377, 'eval_steps_per_second': 0.858, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/652980294361032930\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  34%|███▍      | 11/32 [3:27:06<5:45:18, 986.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7807, 'grad_norm': 9.784893035888672, 'learning_rate': 2.6203627484681864e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/652980294361032930\\checkpoint-192\n",
      "Configuration saved in ./grid_results/652980294361032930\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/652980294361032930\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  34%|███▍      | 11/32 [3:30:16<5:45:18, 986.61s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/652980294361032930\\checkpoint-192\n",
      "Configuration saved in ./grid_results/652980294361032930\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7975374460220337, 'eval_accuracy': 0.7441253263707572, 'eval_f1_macro': 0.5515801044468291, 'eval_f1_weighted': 0.7523121760086806, 'eval_f1_0': 0.3146067415730337, 'eval_f1_1': 0.8507718696397941, 'eval_f1_2': 0.48936170212765956, 'eval_runtime': 14.2991, 'eval_samples_per_second': 26.785, 'eval_steps_per_second': 0.839, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/652980294361032930\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/652980294361032930\\checkpoint-192 (score: 0.5515801044468291).\n",
      "\n",
      "100%|██████████| 192/192 [14:48<00:00,  4.63s/it]:18, 986.61s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 888.8054, 'train_samples_per_second': 6.877, 'train_steps_per_second': 0.216, 'train_loss': 0.8592751125494639, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  38%|███▊      | 12/32 [3:30:40<5:20:31, 961.58s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  38%|███▊      | 12/32 [3:34:05<5:20:31, 961.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-48\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0626901388168335, 'eval_accuracy': 0.5300261096605744, 'eval_f1_macro': 0.4039714203380149, 'eval_f1_weighted': 0.5886395899087825, 'eval_f1_0': 0.22950819672131148, 'eval_f1_1': 0.6800804828973843, 'eval_f1_2': 0.3023255813953488, 'eval_runtime': 14.0081, 'eval_samples_per_second': 27.341, 'eval_steps_per_second': 0.857, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  38%|███▊      | 12/32 [3:34:22<5:20:31, 961.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.092, 'grad_norm': 4.5021467208862305, 'learning_rate': 1.4995984406072743e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  38%|███▊      | 12/32 [3:37:38<5:20:31, 961.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-96\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9624460339546204, 'eval_accuracy': 0.5300261096605744, 'eval_f1_macro': 0.42207758665220313, 'eval_f1_weighted': 0.5900163220782206, 'eval_f1_0': 0.28703703703703703, 'eval_f1_1': 0.6736401673640168, 'eval_f1_2': 0.3055555555555556, 'eval_runtime': 14.0305, 'eval_samples_per_second': 27.298, 'eval_steps_per_second': 0.855, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  38%|███▊      | 12/32 [3:38:02<5:20:31, 961.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9911, 'grad_norm': 8.269454956054688, 'learning_rate': 1.2445093613250517e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  38%|███▊      | 12/32 [3:41:07<5:20:31, 961.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-144\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8767108917236328, 'eval_accuracy': 0.6475195822454308, 'eval_f1_macro': 0.5088021331478423, 'eval_f1_weighted': 0.6807052763944014, 'eval_f1_0': 0.35714285714285715, 'eval_f1_1': 0.7660377358490567, 'eval_f1_2': 0.4032258064516129, 'eval_runtime': 13.815, 'eval_samples_per_second': 27.723, 'eval_steps_per_second': 0.869, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  38%|███▊      | 12/32 [3:41:40<5:20:31, 961.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.83, 'grad_norm': 8.966193199157715, 'learning_rate': 6.764871447528295e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  38%|███▊      | 12/32 [3:44:39<5:20:31, 961.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-192\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8702517151832581, 'eval_accuracy': 0.6892950391644909, 'eval_f1_macro': 0.5345333267126803, 'eval_f1_weighted': 0.7140050928034494, 'eval_f1_0': 0.3673469387755102, 'eval_f1_1': 0.8029197080291971, 'eval_f1_2': 0.43333333333333335, 'eval_runtime': 13.8808, 'eval_samples_per_second': 27.592, 'eval_steps_per_second': 0.865, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  38%|███▊      | 12/32 [3:45:19<5:20:31, 961.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6812, 'grad_norm': 10.726954460144043, 'learning_rate': 1.5498499478157372e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-240\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  38%|███▊      | 12/32 [3:48:19<5:20:31, 961.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/3068842257512392887\\checkpoint-240\n",
      "Configuration saved in ./grid_results/3068842257512392887\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8696185350418091, 'eval_accuracy': 0.6527415143603134, 'eval_f1_macro': 0.5141335204204264, 'eval_f1_weighted': 0.6858558483975808, 'eval_f1_0': 0.3711340206185567, 'eval_f1_1': 0.7712665406427222, 'eval_f1_2': 0.4, 'eval_runtime': 14.4673, 'eval_samples_per_second': 26.474, 'eval_steps_per_second': 0.829, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/3068842257512392887\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/3068842257512392887\\checkpoint-192 (score: 0.5345333267126803).\n",
      "\n",
      "100%|██████████| 240/240 [17:46<00:00,  4.44s/it]:31, 961.58s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1066.6932, 'train_samples_per_second': 7.162, 'train_steps_per_second': 0.225, 'train_loss': 0.8517670234044393, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  41%|████      | 13/32 [3:48:42<5:16:03, 998.08s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  41%|████      | 13/32 [3:52:08<5:16:03, 998.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9521226286888123, 'eval_accuracy': 0.7780678851174935, 'eval_f1_macro': 0.29172785119921685, 'eval_f1_weighted': 0.6809522166373364, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8751835535976505, 'eval_f1_2': 0.0, 'eval_runtime': 13.7876, 'eval_samples_per_second': 27.779, 'eval_steps_per_second': 0.87, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  41%|████      | 13/32 [3:52:25<5:16:03, 998.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1707, 'grad_norm': 4.2572221755981445, 'learning_rate': 1.4995984406072743e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  41%|████      | 13/32 [3:55:39<5:16:03, 998.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8792214393615723, 'eval_accuracy': 0.7702349869451697, 'eval_f1_macro': 0.3904004699070675, 'eval_f1_weighted': 0.7121969836930877, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8698315467075038, 'eval_f1_2': 0.3013698630136986, 'eval_runtime': 13.6669, 'eval_samples_per_second': 28.024, 'eval_steps_per_second': 0.878, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  41%|████      | 13/32 [3:56:05<5:16:03, 998.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9374, 'grad_norm': 6.3940510749816895, 'learning_rate': 1.2445093613250517e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  41%|████      | 13/32 [3:59:10<5:16:03, 998.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8576995730400085, 'eval_accuracy': 0.7389033942558747, 'eval_f1_macro': 0.4429742993142661, 'eval_f1_weighted': 0.7172650550346998, 'eval_f1_0': 0.08163265306122448, 'eval_f1_1': 0.851063829787234, 'eval_f1_2': 0.39622641509433965, 'eval_runtime': 13.9142, 'eval_samples_per_second': 27.526, 'eval_steps_per_second': 0.862, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  41%|████      | 13/32 [3:59:43<5:16:03, 998.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8214, 'grad_norm': 7.699423313140869, 'learning_rate': 6.764871447528295e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  41%|████      | 13/32 [4:02:41<5:16:03, 998.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8451902866363525, 'eval_accuracy': 0.7362924281984334, 'eval_f1_macro': 0.488718471584978, 'eval_f1_weighted': 0.7335462836565239, 'eval_f1_0': 0.2318840579710145, 'eval_f1_1': 0.8542713567839196, 'eval_f1_2': 0.38, 'eval_runtime': 13.9154, 'eval_samples_per_second': 27.523, 'eval_steps_per_second': 0.862, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  41%|████      | 13/32 [4:03:21<5:16:03, 998.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7191, 'grad_norm': 9.10242748260498, 'learning_rate': 1.5498499478157372e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-240\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  41%|████      | 13/32 [4:06:20<5:16:03, 998.08s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7756417612876348015\\checkpoint-240\n",
      "Configuration saved in ./grid_results/-7756417612876348015\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8482698798179626, 'eval_accuracy': 0.7023498694516971, 'eval_f1_macro': 0.47718995979865547, 'eval_f1_weighted': 0.7129503863525887, 'eval_f1_0': 0.2028985507246377, 'eval_f1_1': 0.8286713286713286, 'eval_f1_2': 0.4, 'eval_runtime': 14.2188, 'eval_samples_per_second': 26.936, 'eval_steps_per_second': 0.844, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7756417612876348015\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-7756417612876348015\\checkpoint-192 (score: 0.488718471584978).\n",
      "\n",
      "100%|██████████| 240/240 [17:45<00:00,  4.44s/it]:03, 998.08s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1064.9593, 'train_samples_per_second': 7.174, 'train_steps_per_second': 0.225, 'train_loss': 0.8755263566970826, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  44%|████▍     | 14/32 [4:06:43<5:06:52, 1022.94s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  44%|████▍     | 14/32 [4:10:17<5:06:52, 1022.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-48\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.059545636177063, 'eval_accuracy': 0.6997389033942559, 'eval_f1_macro': 0.37880058144241363, 'eval_f1_weighted': 0.6729244719339276, 'eval_f1_0': 0.044444444444444446, 'eval_f1_1': 0.8174474959612278, 'eval_f1_2': 0.27450980392156865, 'eval_runtime': 14.7528, 'eval_samples_per_second': 25.961, 'eval_steps_per_second': 0.813, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  44%|████▍     | 14/32 [4:10:33<5:06:52, 1022.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.16, 'grad_norm': 5.645561218261719, 'learning_rate': 1.4995984406072743e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  44%|████▍     | 14/32 [4:13:48<5:06:52, 1022.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-96\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9777032732963562, 'eval_accuracy': 0.6240208877284595, 'eval_f1_macro': 0.45122826489086204, 'eval_f1_weighted': 0.6622019919845372, 'eval_f1_0': 0.22, 'eval_f1_1': 0.7660377358490567, 'eval_f1_2': 0.36764705882352944, 'eval_runtime': 14.1151, 'eval_samples_per_second': 27.134, 'eval_steps_per_second': 0.85, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  44%|████▍     | 14/32 [4:14:13<5:06:52, 1022.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0123, 'grad_norm': 11.43327522277832, 'learning_rate': 1.2445093613250517e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  44%|████▍     | 14/32 [4:17:20<5:06:52, 1022.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-144\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.930133044719696, 'eval_accuracy': 0.587467362924282, 'eval_f1_macro': 0.4290890992862469, 'eval_f1_weighted': 0.6338682048335833, 'eval_f1_0': 0.19047619047619047, 'eval_f1_1': 0.734375, 'eval_f1_2': 0.3624161073825503, 'eval_runtime': 14.0373, 'eval_samples_per_second': 27.284, 'eval_steps_per_second': 0.855, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  44%|████▍     | 14/32 [4:17:53<5:06:52, 1022.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8741, 'grad_norm': 8.49026870727539, 'learning_rate': 6.764871447528295e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  44%|████▍     | 14/32 [4:20:53<5:06:52, 1022.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-192\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9074387550354004, 'eval_accuracy': 0.639686684073107, 'eval_f1_macro': 0.4661359674218925, 'eval_f1_weighted': 0.6747496069214018, 'eval_f1_0': 0.24489795918367346, 'eval_f1_1': 0.7775700934579439, 'eval_f1_2': 0.37593984962406013, 'eval_runtime': 13.6935, 'eval_samples_per_second': 27.969, 'eval_steps_per_second': 0.876, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  44%|████▍     | 14/32 [4:21:34<5:06:52, 1022.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7511, 'grad_norm': 13.97556209564209, 'learning_rate': 1.5498499478157372e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-240\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  44%|████▍     | 14/32 [4:24:35<5:06:52, 1022.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6463082759312890582\\checkpoint-240\n",
      "Configuration saved in ./grid_results/6463082759312890582\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.911051869392395, 'eval_accuracy': 0.5796344647519582, 'eval_f1_macro': 0.4452319080242539, 'eval_f1_weighted': 0.6260816573003735, 'eval_f1_0': 0.23529411764705882, 'eval_f1_1': 0.714859437751004, 'eval_f1_2': 0.3855421686746988, 'eval_runtime': 14.538, 'eval_samples_per_second': 26.345, 'eval_steps_per_second': 0.825, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6463082759312890582\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/6463082759312890582\\checkpoint-192 (score: 0.4661359674218925).\n",
      "\n",
      "100%|██████████| 240/240 [17:59<00:00,  4.50s/it]:52, 1022.94s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1079.1397, 'train_samples_per_second': 7.08, 'train_steps_per_second': 0.222, 'train_loss': 0.9083930889765421, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n",
      "Grid Search:  47%|████▋     | 15/32 [4:24:57<4:55:57, 1044.57s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  47%|████▋     | 15/32 [4:28:28<4:55:57, 1044.57s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-48\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9521389007568359, 'eval_accuracy': 0.7780678851174935, 'eval_f1_macro': 0.29172785119921685, 'eval_f1_weighted': 0.6809522166373364, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8751835535976505, 'eval_f1_2': 0.0, 'eval_runtime': 14.3382, 'eval_samples_per_second': 26.712, 'eval_steps_per_second': 0.837, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  47%|████▋     | 15/32 [4:28:44<4:55:57, 1044.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1707, 'grad_norm': 4.256056785583496, 'learning_rate': 1.4995984406072743e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  47%|████▋     | 15/32 [4:32:04<4:55:57, 1044.57s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-96\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8790266513824463, 'eval_accuracy': 0.7728459530026109, 'eval_f1_macro': 0.3923717295276929, 'eval_f1_weighted': 0.7140333437133208, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8715596330275229, 'eval_f1_2': 0.3055555555555556, 'eval_runtime': 14.1903, 'eval_samples_per_second': 26.99, 'eval_steps_per_second': 0.846, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  47%|████▋     | 15/32 [4:32:28<4:55:57, 1044.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9368, 'grad_norm': 6.425936698913574, 'learning_rate': 1.2445093613250517e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  47%|████▋     | 15/32 [4:35:36<4:55:57, 1044.57s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-144\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8563001155853271, 'eval_accuracy': 0.7362924281984334, 'eval_f1_macro': 0.45094442794961065, 'eval_f1_weighted': 0.7218029579243617, 'eval_f1_0': 0.11320754716981132, 'eval_f1_1': 0.8543046357615894, 'eval_f1_2': 0.3853211009174312, 'eval_runtime': 13.9071, 'eval_samples_per_second': 27.54, 'eval_steps_per_second': 0.863, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  47%|████▋     | 15/32 [4:36:09<4:55:57, 1044.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.819, 'grad_norm': 7.46780252456665, 'learning_rate': 6.764871447528295e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  47%|████▋     | 15/32 [4:39:08<4:55:57, 1044.57s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-192\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8437289595603943, 'eval_accuracy': 0.7467362924281984, 'eval_f1_macro': 0.4999665241983296, 'eval_f1_weighted': 0.742106000746402, 'eval_f1_0': 0.24242424242424243, 'eval_f1_1': 0.8614357262103506, 'eval_f1_2': 0.39603960396039606, 'eval_runtime': 14.0847, 'eval_samples_per_second': 27.193, 'eval_steps_per_second': 0.852, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  47%|████▋     | 15/32 [4:39:48<4:55:57, 1044.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7189, 'grad_norm': 9.801655769348145, 'learning_rate': 1.5498499478157372e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-240\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  47%|████▋     | 15/32 [4:42:49<4:55:57, 1044.57s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/1699348612223213726\\checkpoint-240\n",
      "Configuration saved in ./grid_results/1699348612223213726\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8461830615997314, 'eval_accuracy': 0.7101827676240209, 'eval_f1_macro': 0.485996601630424, 'eval_f1_weighted': 0.7187115942695226, 'eval_f1_0': 0.20588235294117646, 'eval_f1_1': 0.8327526132404182, 'eval_f1_2': 0.41935483870967744, 'eval_runtime': 14.232, 'eval_samples_per_second': 26.911, 'eval_steps_per_second': 0.843, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/1699348612223213726\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/1699348612223213726\\checkpoint-192 (score: 0.4999665241983296).\n",
      "\n",
      "Grid Search:  47%|████▋     | 15/32 [4:42:58<4:55:57, 1044.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1079.0329, 'train_samples_per_second': 7.08, 'train_steps_per_second': 0.222, 'train_loss': 0.8745547612508138, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [17:59<00:00,  4.50s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  50%|█████     | 16/32 [4:43:12<4:42:33, 1059.58s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:45:05<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1991, 'grad_norm': 6.608591556549072, 'learning_rate': 1.3157894736842108e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  50%|█████     | 16/32 [4:47:00<4:42:33, 1059.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-2543802969456351527\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-2543802969456351527\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0390757322311401, 'eval_accuracy': 0.40992167101827676, 'eval_f1_macro': 0.36675490815627504, 'eval_f1_weighted': 0.4674212522507067, 'eval_f1_0': 0.24528301886792453, 'eval_f1_1': 0.5167464114832536, 'eval_f1_2': 0.3382352941176471, 'eval_runtime': 14.2153, 'eval_samples_per_second': 26.943, 'eval_steps_per_second': 0.844, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-2543802969456351527\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:47:18<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.072, 'grad_norm': 6.84867000579834, 'learning_rate': 1.9694002659393306e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:49:09<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9513, 'grad_norm': 12.420605659484863, 'learning_rate': 1.7215694610530624e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  50%|█████     | 16/32 [4:50:54<4:42:33, 1059.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-2543802969456351527\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-2543802969456351527\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9012377858161926, 'eval_accuracy': 0.5718015665796344, 'eval_f1_macro': 0.4821316341366995, 'eval_f1_weighted': 0.6282908346111579, 'eval_f1_0': 0.3, 'eval_f1_1': 0.6997929606625258, 'eval_f1_2': 0.44660194174757284, 'eval_runtime': 14.1972, 'eval_samples_per_second': 26.977, 'eval_steps_per_second': 0.845, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-2543802969456351527\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:51:21<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8743, 'grad_norm': 9.391298294067383, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:53:13<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7407, 'grad_norm': 10.395607948303223, 'learning_rate': 7.746014439841941e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  50%|█████     | 16/32 [4:54:48<4:42:33, 1059.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-2543802969456351527\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-2543802969456351527\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8386096358299255, 'eval_accuracy': 0.6710182767624021, 'eval_f1_macro': 0.544500319284802, 'eval_f1_weighted': 0.7058158393605634, 'eval_f1_0': 0.3793103448275862, 'eval_f1_1': 0.7854406130268199, 'eval_f1_2': 0.46875, 'eval_runtime': 13.982, 'eval_samples_per_second': 27.392, 'eval_steps_per_second': 0.858, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-2543802969456351527\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:55:26<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6558, 'grad_norm': 20.11691665649414, 'learning_rate': 3.2271842837425917e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  50%|█████     | 16/32 [4:57:17<4:42:33, 1059.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.568, 'grad_norm': 14.675065994262695, 'learning_rate': 4.7674237125185597e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-2543802969456351527\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-2543802969456351527\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-2543802969456351527\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  50%|█████     | 16/32 [4:58:48<4:42:33, 1059.58s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-2543802969456351527\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-2543802969456351527\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8435373902320862, 'eval_accuracy': 0.6971279373368147, 'eval_f1_macro': 0.5614170995588739, 'eval_f1_weighted': 0.7237090890178614, 'eval_f1_0': 0.391304347826087, 'eval_f1_1': 0.8037383177570093, 'eval_f1_2': 0.4892086330935252, 'eval_runtime': 14.3336, 'eval_samples_per_second': 26.72, 'eval_steps_per_second': 0.837, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-2543802969456351527\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-2543802969456351527\\checkpoint-380 (score: 0.5614170995588739).\n",
      "\n",
      "100%|██████████| 380/380 [15:46<00:00,  2.49s/it]:33, 1059.58s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 946.5121, 'train_samples_per_second': 6.457, 'train_steps_per_second': 0.401, 'train_loss': 0.838408919384605, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n",
      "Grid Search:  53%|█████▎    | 17/32 [4:59:14<4:17:35, 1030.38s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:01:08<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.102, 'grad_norm': 5.393991470336914, 'learning_rate': 1.3157894736842108e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  53%|█████▎    | 17/32 [5:03:03<4:17:35, 1030.38s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7692604984116958850\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-7692604984116958850\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9456692934036255, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.33357988165680474, 'eval_f1_weighted': 0.6960715775487818, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8757396449704142, 'eval_f1_2': 0.125, 'eval_runtime': 13.7164, 'eval_samples_per_second': 27.923, 'eval_steps_per_second': 0.875, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7692604984116958850\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:03:22<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9864, 'grad_norm': 5.726266384124756, 'learning_rate': 1.9694002659393306e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:05:17<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8811, 'grad_norm': 11.132863998413086, 'learning_rate': 1.7215694610530624e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  53%|█████▎    | 17/32 [5:07:02<4:17:35, 1030.38s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7692604984116958850\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-7692604984116958850\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7601227760314941, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.543344060692523, 'eval_f1_weighted': 0.7527856671554919, 'eval_f1_0': 0.32941176470588235, 'eval_f1_1': 0.856175972927242, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 13.9229, 'eval_samples_per_second': 27.509, 'eval_steps_per_second': 0.862, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7692604984116958850\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:07:32<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7829, 'grad_norm': 10.066987037658691, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:09:23<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.693, 'grad_norm': 11.139004707336426, 'learning_rate': 7.746014439841941e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  53%|█████▎    | 17/32 [5:10:58<4:17:35, 1030.38s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7692604984116958850\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-7692604984116958850\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7193083763122559, 'eval_accuracy': 0.7232375979112271, 'eval_f1_macro': 0.5597856850077099, 'eval_f1_weighted': 0.7394859730329448, 'eval_f1_0': 0.3516483516483517, 'eval_f1_1': 0.827708703374778, 'eval_f1_2': 0.5, 'eval_runtime': 14.0504, 'eval_samples_per_second': 27.259, 'eval_steps_per_second': 0.854, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7692604984116958850\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:11:38<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6384, 'grad_norm': 15.03713607788086, 'learning_rate': 3.2271842837425917e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:13:29<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.512, 'grad_norm': 13.007887840270996, 'learning_rate': 4.7674237125185597e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-7692604984116958850\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-7692604984116958850\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-7692604984116958850\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  53%|█████▎    | 17/32 [5:15:00<4:17:35, 1030.38s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-7692604984116958850\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-7692604984116958850\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7063664197921753, 'eval_accuracy': 0.7389033942558747, 'eval_f1_macro': 0.5637651995460827, 'eval_f1_weighted': 0.7478626758482877, 'eval_f1_0': 0.379746835443038, 'eval_f1_1': 0.8388214904679376, 'eval_f1_2': 0.4727272727272727, 'eval_runtime': 14.2957, 'eval_samples_per_second': 26.791, 'eval_steps_per_second': 0.839, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-7692604984116958850\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-7692604984116958850\\checkpoint-380 (score: 0.5637651995460827).\n",
      "\n",
      "Grid Search:  53%|█████▎    | 17/32 [5:15:10<4:17:35, 1030.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 952.84, 'train_samples_per_second': 6.415, 'train_steps_per_second': 0.399, 'train_loss': 0.771495455189755, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [15:53<00:00,  2.51s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:15:24<3:56:11, 1012.24s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:17:40<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.127, 'grad_norm': 5.916339874267578, 'learning_rate': 1.3157894736842108e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  56%|█████▋    | 18/32 [5:19:42<3:56:11, 1012.24s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5563746078204649408\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-5563746078204649408\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0383408069610596, 'eval_accuracy': 0.4804177545691906, 'eval_f1_macro': 0.40437207393729135, 'eval_f1_weighted': 0.5501803102098255, 'eval_f1_0': 0.23423423423423423, 'eval_f1_1': 0.6217391304347826, 'eval_f1_2': 0.35714285714285715, 'eval_runtime': 15.517, 'eval_samples_per_second': 24.683, 'eval_steps_per_second': 0.773, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5563746078204649408\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:20:01<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0728, 'grad_norm': 5.827925682067871, 'learning_rate': 1.9694002659393306e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:21:52<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9484, 'grad_norm': 13.015181541442871, 'learning_rate': 1.7215694610530624e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  56%|█████▋    | 18/32 [5:23:36<3:56:11, 1012.24s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5563746078204649408\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-5563746078204649408\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8529317378997803, 'eval_accuracy': 0.6475195822454308, 'eval_f1_macro': 0.5220783673586898, 'eval_f1_weighted': 0.6841407663320616, 'eval_f1_0': 0.3624161073825503, 'eval_f1_1': 0.7642585551330798, 'eval_f1_2': 0.43956043956043955, 'eval_runtime': 14.072, 'eval_samples_per_second': 27.217, 'eval_steps_per_second': 0.853, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5563746078204649408\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:24:06<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8421, 'grad_norm': 8.850244522094727, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:25:56<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7116, 'grad_norm': 15.577313423156738, 'learning_rate': 7.746014439841941e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  56%|█████▋    | 18/32 [5:27:32<3:56:11, 1012.24s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5563746078204649408\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-5563746078204649408\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7775217294692993, 'eval_accuracy': 0.6919060052219321, 'eval_f1_macro': 0.581390278453727, 'eval_f1_weighted': 0.7217304164735632, 'eval_f1_0': 0.4672897196261682, 'eval_f1_1': 0.7915869980879541, 'eval_f1_2': 0.4852941176470588, 'eval_runtime': 14.238, 'eval_samples_per_second': 26.9, 'eval_steps_per_second': 0.843, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5563746078204649408\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:28:15<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6252, 'grad_norm': 19.35627555847168, 'learning_rate': 3.2271842837425917e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:30:06<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5022, 'grad_norm': 19.703887939453125, 'learning_rate': 4.7674237125185597e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-5563746078204649408\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-5563746078204649408\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-5563746078204649408\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  56%|█████▋    | 18/32 [5:31:38<3:56:11, 1012.24s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5563746078204649408\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-5563746078204649408\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7878773212432861, 'eval_accuracy': 0.6919060052219321, 'eval_f1_macro': 0.5603975367419864, 'eval_f1_weighted': 0.7176584523622692, 'eval_f1_0': 0.41237113402061853, 'eval_f1_1': 0.7955390334572491, 'eval_f1_2': 0.4732824427480916, 'eval_runtime': 14.3778, 'eval_samples_per_second': 26.638, 'eval_steps_per_second': 0.835, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5563746078204649408\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-5563746078204649408\\checkpoint-286 (score: 0.581390278453727).\n",
      "\n",
      "Grid Search:  56%|█████▋    | 18/32 [5:31:50<3:56:11, 1012.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 965.3098, 'train_samples_per_second': 6.332, 'train_steps_per_second': 0.394, 'train_loss': 0.8042343616485595, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [16:05<00:00,  2.54s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:32:06<3:38:36, 1008.94s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 380\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:33:57<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.102, 'grad_norm': 5.394041538238525, 'learning_rate': 1.3157894736842108e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  59%|█████▉    | 19/32 [5:35:52<3:38:36, 1008.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1845142459615235796\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-1845142459615235796\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9474587440490723, 'eval_accuracy': 0.7859007832898173, 'eval_f1_macro': 0.34775510204081633, 'eval_f1_weighted': 0.7038823466723504, 'eval_f1_0': 0.0, 'eval_f1_1': 0.88, 'eval_f1_2': 0.16326530612244897, 'eval_runtime': 13.786, 'eval_samples_per_second': 27.782, 'eval_steps_per_second': 0.87, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1845142459615235796\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:36:12<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9884, 'grad_norm': 5.495057582855225, 'learning_rate': 1.9694002659393306e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:38:02<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8873, 'grad_norm': 8.910914421081543, 'learning_rate': 1.7215694610530624e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  59%|█████▉    | 19/32 [5:39:45<3:38:36, 1008.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1845142459615235796\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-1845142459615235796\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7815747857093811, 'eval_accuracy': 0.7597911227154047, 'eval_f1_macro': 0.5648155802901513, 'eval_f1_weighted': 0.7618377926372513, 'eval_f1_0': 0.3617021276595745, 'eval_f1_1': 0.8590604026845637, 'eval_f1_2': 0.47368421052631576, 'eval_runtime': 13.7459, 'eval_samples_per_second': 27.863, 'eval_steps_per_second': 0.873, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1845142459615235796\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:40:15<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7922, 'grad_norm': 7.019629955291748, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:42:07<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.694, 'grad_norm': 13.090804100036621, 'learning_rate': 7.746014439841941e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  59%|█████▉    | 19/32 [5:43:43<3:38:36, 1008.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1845142459615235796\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-1845142459615235796\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7242913842201233, 'eval_accuracy': 0.7258485639686684, 'eval_f1_macro': 0.5668941228263262, 'eval_f1_weighted': 0.7424049508928033, 'eval_f1_0': 0.36363636363636365, 'eval_f1_1': 0.8285714285714286, 'eval_f1_2': 0.5084745762711864, 'eval_runtime': 14.3348, 'eval_samples_per_second': 26.718, 'eval_steps_per_second': 0.837, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1845142459615235796\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:44:23<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6325, 'grad_norm': 12.755248069763184, 'learning_rate': 3.2271842837425917e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  59%|█████▉    | 19/32 [5:46:13<3:38:36, 1008.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.51, 'grad_norm': 11.98836612701416, 'learning_rate': 4.7674237125185597e-07, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-1845142459615235796\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-1845142459615235796\\checkpoint-380\\config.json\n",
      "Model weights saved in ./grid_results/-1845142459615235796\\checkpoint-380\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  59%|█████▉    | 19/32 [5:47:43<3:38:36, 1008.94s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1845142459615235796\\checkpoint-380\n",
      "Configuration saved in ./grid_results/-1845142459615235796\\checkpoint-380\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7127236723899841, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.5861290278960962, 'eval_f1_weighted': 0.7631715019176091, 'eval_f1_0': 0.38961038961038963, 'eval_f1_1': 0.8502581755593803, 'eval_f1_2': 0.5185185185185185, 'eval_runtime': 14.2458, 'eval_samples_per_second': 26.885, 'eval_steps_per_second': 0.842, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1845142459615235796\\checkpoint-380\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-1845142459615235796\\checkpoint-380 (score: 0.5861290278960962).\n",
      "\n",
      "100%|██████████| 380/380 [15:46<00:00,  2.49s/it]:36, 1008.94s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 945.938, 'train_samples_per_second': 6.461, 'train_steps_per_second': 0.402, 'train_loss': 0.7731047027989437, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:48:07<3:18:55, 994.62s/it] C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:50:28<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1308, 'grad_norm': 5.60092830657959, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  62%|██████▎   | 20/32 [5:52:23<3:18:55, 994.62s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-95\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0436780452728271, 'eval_accuracy': 0.5274151436031331, 'eval_f1_macro': 0.42641972809594497, 'eval_f1_weighted': 0.5903409454189028, 'eval_f1_0': 0.233502538071066, 'eval_f1_1': 0.6707566462167689, 'eval_f1_2': 0.375, 'eval_runtime': 14.0714, 'eval_samples_per_second': 27.218, 'eval_steps_per_second': 0.853, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:52:52<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0748, 'grad_norm': 5.776932716369629, 'learning_rate': 1.999145758387301e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:54:45<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9638, 'grad_norm': 14.28627872467041, 'learning_rate': 1.898390981891979e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  62%|██████▎   | 20/32 [5:56:31<3:18:55, 994.62s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-191\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.840461254119873, 'eval_accuracy': 0.6945169712793734, 'eval_f1_macro': 0.5457808358817533, 'eval_f1_weighted': 0.7233047883680265, 'eval_f1_0': 0.368, 'eval_f1_1': 0.8110091743119267, 'eval_f1_2': 0.4583333333333333, 'eval_runtime': 14.1925, 'eval_samples_per_second': 26.986, 'eval_steps_per_second': 0.846, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:57:00<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8575, 'grad_norm': 9.533669471740723, 'learning_rate': 1.646299237860941e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [5:58:51<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7183, 'grad_norm': 14.736576080322266, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  62%|██████▎   | 20/32 [6:00:24<3:18:55, 994.62s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-286\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8298978209495544, 'eval_accuracy': 0.7101827676240209, 'eval_f1_macro': 0.5354466962250265, 'eval_f1_weighted': 0.724892345514014, 'eval_f1_0': 0.3058823529411765, 'eval_f1_1': 0.8176991150442477, 'eval_f1_2': 0.4827586206896552, 'eval_runtime': 13.9779, 'eval_samples_per_second': 27.4, 'eval_steps_per_second': 0.858, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [6:01:02<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6302, 'grad_norm': 17.838565826416016, 'learning_rate': 8.763073687306523e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [6:02:52<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.446, 'grad_norm': 27.671266555786133, 'learning_rate': 4.881149509103993e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  62%|██████▎   | 20/32 [6:04:17<3:18:55, 994.62s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-382\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8631042242050171, 'eval_accuracy': 0.6866840731070496, 'eval_f1_macro': 0.5578001437814523, 'eval_f1_weighted': 0.7123242341709436, 'eval_f1_0': 0.4090909090909091, 'eval_f1_1': 0.788785046728972, 'eval_f1_2': 0.4755244755244755, 'eval_runtime': 14.1658, 'eval_samples_per_second': 27.037, 'eval_steps_per_second': 0.847, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [6:05:06<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3631, 'grad_norm': 9.975651741027832, 'learning_rate': 1.861512827298051e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  62%|██████▎   | 20/32 [6:06:56<3:18:55, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3004, 'grad_norm': 17.881200790405273, 'learning_rate': 2.1283154672645522e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-475\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  62%|██████▎   | 20/32 [6:08:24<3:18:55, 994.62s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/2593948190028998424\\checkpoint-475\n",
      "Configuration saved in ./grid_results/2593948190028998424\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9093779921531677, 'eval_accuracy': 0.7284595300261096, 'eval_f1_macro': 0.5851727086432903, 'eval_f1_weighted': 0.7460307242317497, 'eval_f1_0': 0.4418604651162791, 'eval_f1_1': 0.8258527827648114, 'eval_f1_2': 0.4878048780487805, 'eval_runtime': 14.3563, 'eval_samples_per_second': 26.678, 'eval_steps_per_second': 0.836, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/2593948190028998424\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/2593948190028998424\\checkpoint-475 (score: 0.5851727086432903).\n",
      "\n",
      "100%|██████████| 475/475 [20:06<00:00,  2.54s/it]:55, 994.62s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1206.3613, 'train_samples_per_second': 6.333, 'train_steps_per_second': 0.394, 'train_loss': 0.6980671390734221, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:08:48<3:15:56, 1068.78s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:10:40<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0556, 'grad_norm': 4.595592975616455, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  66%|██████▌   | 21/32 [6:12:35<3:15:56, 1068.78s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9462722539901733, 'eval_accuracy': 0.783289817232376, 'eval_f1_macro': 0.3332830916709372, 'eval_f1_weighted': 0.697063948086379, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8774002954209749, 'eval_f1_2': 0.12244897959183673, 'eval_runtime': 13.8053, 'eval_samples_per_second': 27.743, 'eval_steps_per_second': 0.869, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:12:53<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9843, 'grad_norm': 5.671791076660156, 'learning_rate': 1.999145758387301e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:14:44<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8949, 'grad_norm': 10.889538764953613, 'learning_rate': 1.898390981891979e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  66%|██████▌   | 21/32 [6:16:28<3:15:56, 1068.78s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7630226612091064, 'eval_accuracy': 0.7284595300261096, 'eval_f1_macro': 0.5680249909352076, 'eval_f1_weighted': 0.7445977952182028, 'eval_f1_0': 0.3783783783783784, 'eval_f1_1': 0.8315789473684211, 'eval_f1_2': 0.49411764705882355, 'eval_runtime': 14.0719, 'eval_samples_per_second': 27.217, 'eval_steps_per_second': 0.853, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:16:58<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7875, 'grad_norm': 6.6296820640563965, 'learning_rate': 1.646299237860941e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:18:49<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6755, 'grad_norm': 14.311627388000488, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  66%|██████▌   | 21/32 [6:20:25<3:15:56, 1068.78s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7557043433189392, 'eval_accuracy': 0.7467362924281984, 'eval_f1_macro': 0.5620288297967589, 'eval_f1_weighted': 0.7523085274025202, 'eval_f1_0': 0.35, 'eval_f1_1': 0.8458904109589042, 'eval_f1_2': 0.49019607843137253, 'eval_runtime': 14.1729, 'eval_samples_per_second': 27.023, 'eval_steps_per_second': 0.847, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:21:04<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5834, 'grad_norm': 15.560773849487305, 'learning_rate': 8.763073687306523e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:22:56<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4349, 'grad_norm': 20.380821228027344, 'learning_rate': 4.881149509103993e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  66%|██████▌   | 21/32 [6:24:21<3:15:56, 1068.78s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-382\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7300905585289001, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.5891682491114824, 'eval_f1_weighted': 0.7604812204333466, 'eval_f1_0': 0.367816091954023, 'eval_f1_1': 0.8441330998248686, 'eval_f1_2': 0.5555555555555556, 'eval_runtime': 14.085, 'eval_samples_per_second': 27.192, 'eval_steps_per_second': 0.852, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:25:08<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3407, 'grad_norm': 10.090332984924316, 'learning_rate': 1.861512827298051e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  66%|██████▌   | 21/32 [6:27:00<3:15:56, 1068.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2879, 'grad_norm': 12.799604415893555, 'learning_rate': 2.1283154672645522e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  66%|██████▌   | 21/32 [6:28:17<3:15:56, 1068.78s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-1419047662185878398\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-1419047662185878398\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7537418007850647, 'eval_accuracy': 0.7493472584856397, 'eval_f1_macro': 0.5921066660680916, 'eval_f1_weighted': 0.7599460069315045, 'eval_f1_0': 0.3855421686746988, 'eval_f1_1': 0.8421052631578947, 'eval_f1_2': 0.5486725663716814, 'eval_runtime': 14.1711, 'eval_samples_per_second': 27.027, 'eval_steps_per_second': 0.847, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-1419047662185878398\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-1419047662185878398\\checkpoint-475 (score: 0.5921066660680916).\n",
      "\n",
      "100%|██████████| 475/475 [19:36<00:00,  2.48s/it]:56, 1068.78s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1176.5083, 'train_samples_per_second': 6.494, 'train_steps_per_second': 0.404, 'train_loss': 0.6523320368716591, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:28:40<3:04:17, 1105.76s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:30:54<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0959, 'grad_norm': 5.8472161293029785, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  69%|██████▉   | 22/32 [6:32:49<3:04:17, 1105.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0459898710250854, 'eval_accuracy': 0.6214099216710183, 'eval_f1_macro': 0.4256969673294529, 'eval_f1_weighted': 0.6518107603950342, 'eval_f1_0': 0.2484472049689441, 'eval_f1_1': 0.7644927536231884, 'eval_f1_2': 0.2641509433962264, 'eval_runtime': 14.0256, 'eval_samples_per_second': 27.307, 'eval_steps_per_second': 0.856, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:33:11<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0677, 'grad_norm': 5.9173054695129395, 'learning_rate': 1.999145758387301e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:35:03<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9988, 'grad_norm': 12.060399055480957, 'learning_rate': 1.898390981891979e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  69%|██████▉   | 22/32 [6:36:48<3:04:17, 1105.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8596681952476501, 'eval_accuracy': 0.639686684073107, 'eval_f1_macro': 0.5366988926988489, 'eval_f1_weighted': 0.680610550254876, 'eval_f1_0': 0.3803680981595092, 'eval_f1_1': 0.7514677103718199, 'eval_f1_2': 0.4782608695652174, 'eval_runtime': 14.0176, 'eval_samples_per_second': 27.323, 'eval_steps_per_second': 0.856, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:37:17<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8748, 'grad_norm': 8.205883026123047, 'learning_rate': 1.646299237860941e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:39:09<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7743, 'grad_norm': 13.39167594909668, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  69%|██████▉   | 22/32 [6:40:43<3:04:17, 1105.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8151587843894958, 'eval_accuracy': 0.6840731070496083, 'eval_f1_macro': 0.5438234360114321, 'eval_f1_weighted': 0.70978685263754, 'eval_f1_0': 0.3404255319148936, 'eval_f1_1': 0.7910447761194029, 'eval_f1_2': 0.5, 'eval_runtime': 13.9593, 'eval_samples_per_second': 27.437, 'eval_steps_per_second': 0.86, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:41:21<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6405, 'grad_norm': 14.634238243103027, 'learning_rate': 8.763073687306523e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:43:13<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4855, 'grad_norm': 21.823455810546875, 'learning_rate': 4.881149509103993e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  69%|██████▉   | 22/32 [6:44:38<3:04:17, 1105.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-382\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8079344630241394, 'eval_accuracy': 0.7310704960835509, 'eval_f1_macro': 0.6023960244648318, 'eval_f1_weighted': 0.7520497281241766, 'eval_f1_0': 0.4375, 'eval_f1_1': 0.8256880733944955, 'eval_f1_2': 0.544, 'eval_runtime': 14.3017, 'eval_samples_per_second': 26.78, 'eval_steps_per_second': 0.839, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:45:27<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4096, 'grad_norm': 7.4036993980407715, 'learning_rate': 1.861512827298051e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  69%|██████▉   | 22/32 [6:47:18<3:04:17, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3419, 'grad_norm': 13.258511543273926, 'learning_rate': 2.1283154672645522e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  69%|██████▉   | 22/32 [6:48:36<3:04:17, 1105.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-4162391194489397430\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-4162391194489397430\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8309020400047302, 'eval_accuracy': 0.7389033942558747, 'eval_f1_macro': 0.6002688619847781, 'eval_f1_weighted': 0.7552018580675692, 'eval_f1_0': 0.43478260869565216, 'eval_f1_1': 0.8315412186379928, 'eval_f1_2': 0.5344827586206896, 'eval_runtime': 14.1403, 'eval_samples_per_second': 27.086, 'eval_steps_per_second': 0.849, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-4162391194489397430\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-4162391194489397430\\checkpoint-382 (score: 0.6023960244648318).\n",
      "\n",
      "100%|██████████| 475/475 [19:43<00:00,  2.49s/it]:17, 1105.76s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1183.5617, 'train_samples_per_second': 6.455, 'train_steps_per_second': 0.401, 'train_loss': 0.7236946166189093, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:48:59<2:50:57, 1139.73s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 475\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:50:57<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0556, 'grad_norm': 4.595633506774902, 'learning_rate': 1.0526315789473684e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  72%|███████▏  | 23/32 [6:52:51<2:50:57, 1139.73s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-95\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9462757706642151, 'eval_accuracy': 0.783289817232376, 'eval_f1_macro': 0.3332830916709372, 'eval_f1_weighted': 0.697063948086379, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8774002954209749, 'eval_f1_2': 0.12244897959183673, 'eval_runtime': 13.7958, 'eval_samples_per_second': 27.762, 'eval_steps_per_second': 0.87, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-95\\model.safetensors\n",
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:53:10<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9843, 'grad_norm': 5.671628475189209, 'learning_rate': 1.999145758387301e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:55:02<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8935, 'grad_norm': 10.696532249450684, 'learning_rate': 1.898390981891979e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  72%|███████▏  | 23/32 [6:56:47<2:50:57, 1139.73s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-191\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.766775906085968, 'eval_accuracy': 0.7310704960835509, 'eval_f1_macro': 0.5680451127819549, 'eval_f1_weighted': 0.7467336749183664, 'eval_f1_0': 0.39285714285714285, 'eval_f1_1': 0.8350877192982457, 'eval_f1_2': 0.47619047619047616, 'eval_runtime': 14.1003, 'eval_samples_per_second': 27.162, 'eval_steps_per_second': 0.851, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-191\\model.safetensors\n",
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:57:17<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7881, 'grad_norm': 6.746004104614258, 'learning_rate': 1.646299237860941e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [6:59:08<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6734, 'grad_norm': 13.956596374511719, 'learning_rate': 1.2853362242491054e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  72%|███████▏  | 23/32 [7:00:42<2:50:57, 1139.73s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-286\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7514485716819763, 'eval_accuracy': 0.7571801566579635, 'eval_f1_macro': 0.5710741271319933, 'eval_f1_weighted': 0.7606252060144666, 'eval_f1_0': 0.35443037974683544, 'eval_f1_1': 0.8537414965986394, 'eval_f1_2': 0.5050505050505051, 'eval_runtime': 13.858, 'eval_samples_per_second': 27.637, 'eval_steps_per_second': 0.866, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-286\\model.safetensors\n",
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [7:01:22<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.584, 'grad_norm': 15.703293800354004, 'learning_rate': 8.763073687306523e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [7:03:15<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4354, 'grad_norm': 19.552967071533203, 'learning_rate': 4.881149509103993e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  72%|███████▏  | 23/32 [7:04:40<2:50:57, 1139.73s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-382\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7288745045661926, 'eval_accuracy': 0.7519582245430809, 'eval_f1_macro': 0.5940791554970946, 'eval_f1_weighted': 0.7631904867089573, 'eval_f1_0': 0.367816091954023, 'eval_f1_1': 0.8456140350877193, 'eval_f1_2': 0.5688073394495413, 'eval_runtime': 14.0862, 'eval_samples_per_second': 27.19, 'eval_steps_per_second': 0.852, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-382\\model.safetensors\n",
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [7:05:30<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3446, 'grad_norm': 9.468524932861328, 'learning_rate': 1.861512827298051e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search:  72%|███████▏  | 23/32 [7:07:20<2:50:57, 1139.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2949, 'grad_norm': 11.071236610412598, 'learning_rate': 2.1283154672645522e-07, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-475\\config.json\n",
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-475\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  72%|███████▏  | 23/32 [7:08:38<2:50:57, 1139.73s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-8771810023699535770\\checkpoint-475\n",
      "Configuration saved in ./grid_results/-8771810023699535770\\checkpoint-475\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7526358366012573, 'eval_accuracy': 0.7545691906005222, 'eval_f1_macro': 0.5991759617261158, 'eval_f1_weighted': 0.7647556001745304, 'eval_f1_0': 0.3855421686746988, 'eval_f1_1': 0.8456140350877193, 'eval_f1_2': 0.5663716814159292, 'eval_runtime': 14.5273, 'eval_samples_per_second': 26.364, 'eval_steps_per_second': 0.826, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-8771810023699535770\\checkpoint-475\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-8771810023699535770\\checkpoint-475 (score: 0.5991759617261158).\n",
      "\n",
      "100%|██████████| 475/475 [19:40<00:00,  2.49s/it]:57, 1139.73s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1180.6621, 'train_samples_per_second': 6.471, 'train_steps_per_second': 0.402, 'train_loss': 0.6531217183564839, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.13s/it]\n",
      "Grid Search:  75%|███████▌  | 24/32 [7:09:01<2:34:27, 1158.42s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  75%|███████▌  | 24/32 [7:12:30<2:34:27, 1158.42s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5720019384307295438\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-5720019384307295438\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0561609268188477, 'eval_accuracy': 0.6240208877284595, 'eval_f1_macro': 0.4164227042560875, 'eval_f1_weighted': 0.6446041007833742, 'eval_f1_0': 0.17307692307692307, 'eval_f1_1': 0.7570422535211268, 'eval_f1_2': 0.3191489361702128, 'eval_runtime': 14.0675, 'eval_samples_per_second': 27.226, 'eval_steps_per_second': 0.853, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5720019384307295438\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  75%|███████▌  | 24/32 [7:12:46<2:34:27, 1158.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0932, 'grad_norm': 5.0356879234313965, 'learning_rate': 1.9746005004415004e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  75%|███████▌  | 24/32 [7:16:02<2:34:27, 1158.42s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5720019384307295438\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-5720019384307295438\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9543161988258362, 'eval_accuracy': 0.5117493472584856, 'eval_f1_macro': 0.446985750822068, 'eval_f1_weighted': 0.5788890468209079, 'eval_f1_0': 0.2857142857142857, 'eval_f1_1': 0.6434782608695652, 'eval_f1_2': 0.4117647058823529, 'eval_runtime': 13.9815, 'eval_samples_per_second': 27.393, 'eval_steps_per_second': 0.858, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5720019384307295438\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  75%|███████▌  | 24/32 [7:16:26<2:34:27, 1158.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9905, 'grad_norm': 8.896710395812988, 'learning_rate': 1.3129200406321545e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  75%|███████▌  | 24/32 [7:19:34<2:34:27, 1158.42s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5720019384307295438\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-5720019384307295438\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8297012448310852, 'eval_accuracy': 0.6318537859007833, 'eval_f1_macro': 0.5193063931387741, 'eval_f1_weighted': 0.6698876936486534, 'eval_f1_0': 0.4098360655737705, 'eval_f1_1': 0.7450980392156863, 'eval_f1_2': 0.40298507462686567, 'eval_runtime': 13.8372, 'eval_samples_per_second': 27.679, 'eval_steps_per_second': 0.867, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5720019384307295438\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  75%|███████▌  | 24/32 [7:20:05<2:34:27, 1158.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.792, 'grad_norm': 7.529529094696045, 'learning_rate': 3.493816997957582e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-5720019384307295438\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-5720019384307295438\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/-5720019384307295438\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  75%|███████▌  | 24/32 [7:23:11<2:34:27, 1158.42s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-5720019384307295438\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-5720019384307295438\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8288662433624268, 'eval_accuracy': 0.6292428198433421, 'eval_f1_macro': 0.5201149192035843, 'eval_f1_weighted': 0.6673201786971986, 'eval_f1_0': 0.42990654205607476, 'eval_f1_1': 0.7411764705882353, 'eval_f1_2': 0.38926174496644295, 'eval_runtime': 14.617, 'eval_samples_per_second': 26.202, 'eval_steps_per_second': 0.821, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-5720019384307295438\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-5720019384307295438\\checkpoint-192 (score: 0.5201149192035843).\n",
      "\n",
      "100%|██████████| 192/192 [14:12<00:00,  4.44s/it]:27, 1158.42s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 852.4013, 'train_samples_per_second': 7.17, 'train_steps_per_second': 0.225, 'train_loss': 0.8992614150047302, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n",
      "Grid Search:  78%|███████▊  | 25/32 [7:23:34<2:05:08, 1072.64s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  78%|███████▊  | 25/32 [7:27:02<2:05:08, 1072.64s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-9099542538381727982\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-9099542538381727982\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9360027313232422, 'eval_accuracy': 0.7754569190600522, 'eval_f1_macro': 0.30655599002995587, 'eval_f1_weighted': 0.6842325198088265, 'eval_f1_0': 0.046511627906976744, 'eval_f1_1': 0.8731563421828908, 'eval_f1_2': 0.0, 'eval_runtime': 13.7722, 'eval_samples_per_second': 27.81, 'eval_steps_per_second': 0.871, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-9099542538381727982\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  78%|███████▊  | 25/32 [7:27:20<2:05:08, 1072.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0197, 'grad_norm': 4.927470684051514, 'learning_rate': 1.9746005004415004e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  78%|███████▊  | 25/32 [7:30:36<2:05:08, 1072.64s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-9099542538381727982\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-9099542538381727982\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8200562596321106, 'eval_accuracy': 0.6919060052219321, 'eval_f1_macro': 0.5338619841069321, 'eval_f1_weighted': 0.7223056035929872, 'eval_f1_0': 0.3356643356643357, 'eval_f1_1': 0.8152173913043478, 'eval_f1_2': 0.4507042253521127, 'eval_runtime': 13.8217, 'eval_samples_per_second': 27.71, 'eval_steps_per_second': 0.868, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-9099542538381727982\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  78%|███████▊  | 25/32 [7:31:00<2:05:08, 1072.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8729, 'grad_norm': 6.996899127960205, 'learning_rate': 1.3129200406321545e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  78%|███████▊  | 25/32 [7:34:08<2:05:08, 1072.64s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-9099542538381727982\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-9099542538381727982\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7519314289093018, 'eval_accuracy': 0.741514360313316, 'eval_f1_macro': 0.5966296775120304, 'eval_f1_weighted': 0.7590794502175244, 'eval_f1_0': 0.43137254901960786, 'eval_f1_1': 0.8392857142857143, 'eval_f1_2': 0.5192307692307693, 'eval_runtime': 14.0171, 'eval_samples_per_second': 27.324, 'eval_steps_per_second': 0.856, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-9099542538381727982\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  78%|███████▊  | 25/32 [7:34:40<2:05:08, 1072.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7001, 'grad_norm': 7.691154479980469, 'learning_rate': 3.493816997957582e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-9099542538381727982\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-9099542538381727982\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/-9099542538381727982\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  78%|███████▊  | 25/32 [7:37:49<2:05:08, 1072.64s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-9099542538381727982\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-9099542538381727982\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7514250874519348, 'eval_accuracy': 0.7467362924281984, 'eval_f1_macro': 0.5769475564843302, 'eval_f1_weighted': 0.7567184287744717, 'eval_f1_0': 0.38095238095238093, 'eval_f1_1': 0.8452173913043478, 'eval_f1_2': 0.5046728971962616, 'eval_runtime': 14.2332, 'eval_samples_per_second': 26.909, 'eval_steps_per_second': 0.843, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-9099542538381727982\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-9099542538381727982\\checkpoint-144 (score: 0.5966296775120304).\n",
      "\n",
      "100%|██████████| 192/192 [14:19<00:00,  4.47s/it]:08, 1072.64s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 859.0219, 'train_samples_per_second': 7.115, 'train_steps_per_second': 0.224, 'train_loss': 0.8041774531205496, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n",
      "Grid Search:  81%|████████▏ | 26/32 [7:38:12<1:41:25, 1014.28s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  81%|████████▏ | 26/32 [7:41:48<1:41:25, 1014.28s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-6182173541863012101\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-6182173541863012101\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0489673614501953, 'eval_accuracy': 0.6631853785900783, 'eval_f1_macro': 0.44016430676101437, 'eval_f1_weighted': 0.6770569123168213, 'eval_f1_0': 0.2222222222222222, 'eval_f1_1': 0.7944732297063903, 'eval_f1_2': 0.3037974683544304, 'eval_runtime': 13.8654, 'eval_samples_per_second': 27.623, 'eval_steps_per_second': 0.865, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-6182173541863012101\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  81%|████████▏ | 26/32 [7:42:06<1:41:25, 1014.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0901, 'grad_norm': 4.506767272949219, 'learning_rate': 1.9746005004415004e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  81%|████████▏ | 26/32 [7:45:23<1:41:25, 1014.28s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-6182173541863012101\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-6182173541863012101\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.952618420124054, 'eval_accuracy': 0.4856396866840731, 'eval_f1_macro': 0.429025664319782, 'eval_f1_weighted': 0.5459988808168812, 'eval_f1_0': 0.3025210084033613, 'eval_f1_1': 0.6036036036036037, 'eval_f1_2': 0.38095238095238093, 'eval_runtime': 14.1563, 'eval_samples_per_second': 27.055, 'eval_steps_per_second': 0.848, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-6182173541863012101\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  81%|████████▏ | 26/32 [7:45:48<1:41:25, 1014.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9659, 'grad_norm': 8.50018310546875, 'learning_rate': 1.3129200406321545e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  81%|████████▏ | 26/32 [7:48:57<1:41:25, 1014.28s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-6182173541863012101\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-6182173541863012101\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8515310883522034, 'eval_accuracy': 0.6475195822454308, 'eval_f1_macro': 0.5279206498334191, 'eval_f1_weighted': 0.682595226836901, 'eval_f1_0': 0.38016528925619836, 'eval_f1_1': 0.7591522157996147, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 14.1034, 'eval_samples_per_second': 27.157, 'eval_steps_per_second': 0.851, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-6182173541863012101\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  81%|████████▏ | 26/32 [7:49:31<1:41:25, 1014.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7942, 'grad_norm': 8.549347877502441, 'learning_rate': 3.493816997957582e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-6182173541863012101\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-6182173541863012101\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/-6182173541863012101\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                  \n",
      "Grid Search:  81%|████████▏ | 26/32 [7:52:41<1:41:25, 1014.28s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-6182173541863012101\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-6182173541863012101\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8563085198402405, 'eval_accuracy': 0.639686684073107, 'eval_f1_macro': 0.5131259011562243, 'eval_f1_weighted': 0.6786997181015189, 'eval_f1_0': 0.38095238095238093, 'eval_f1_1': 0.7611650485436893, 'eval_f1_2': 0.3972602739726027, 'eval_runtime': 14.1957, 'eval_samples_per_second': 26.98, 'eval_steps_per_second': 0.845, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-6182173541863012101\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-6182173541863012101\\checkpoint-144 (score: 0.5279206498334191).\n",
      "\n",
      "100%|██████████| 192/192 [14:24<00:00,  4.50s/it]:25, 1014.28s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 864.8766, 'train_samples_per_second': 7.067, 'train_steps_per_second': 0.222, 'train_loss': 0.8917426764965057, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n",
      "Grid Search:  84%|████████▍ | 27/32 [7:53:05<1:21:28, 977.75s/it] C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 192\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  84%|████████▍ | 27/32 [7:56:31<1:21:28, 977.75s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3522952969779627978\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-3522952969779627978\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9352014064788818, 'eval_accuracy': 0.7754569190600522, 'eval_f1_macro': 0.30655599002995587, 'eval_f1_weighted': 0.6842325198088265, 'eval_f1_0': 0.046511627906976744, 'eval_f1_1': 0.8731563421828908, 'eval_f1_2': 0.0, 'eval_runtime': 13.7573, 'eval_samples_per_second': 27.84, 'eval_steps_per_second': 0.872, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3522952969779627978\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  84%|████████▍ | 27/32 [7:56:50<1:21:28, 977.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0197, 'grad_norm': 4.96459436416626, 'learning_rate': 1.9746005004415004e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  84%|████████▍ | 27/32 [8:00:09<1:21:28, 977.75s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3522952969779627978\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-3522952969779627978\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8270466327667236, 'eval_accuracy': 0.6840731070496083, 'eval_f1_macro': 0.5333115468409586, 'eval_f1_weighted': 0.7177443301080223, 'eval_f1_0': 0.3466666666666667, 'eval_f1_1': 0.8088235294117647, 'eval_f1_2': 0.4444444444444444, 'eval_runtime': 13.7427, 'eval_samples_per_second': 27.869, 'eval_steps_per_second': 0.873, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3522952969779627978\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  84%|████████▍ | 27/32 [8:00:34<1:21:28, 977.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8741, 'grad_norm': 7.017254829406738, 'learning_rate': 1.3129200406321545e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  84%|████████▍ | 27/32 [8:03:41<1:21:28, 977.75s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3522952969779627978\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-3522952969779627978\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7526200413703918, 'eval_accuracy': 0.741514360313316, 'eval_f1_macro': 0.5930294734921069, 'eval_f1_weighted': 0.758336722419214, 'eval_f1_0': 0.42, 'eval_f1_1': 0.8398576512455516, 'eval_f1_2': 0.5192307692307693, 'eval_runtime': 13.8482, 'eval_samples_per_second': 27.657, 'eval_steps_per_second': 0.867, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3522952969779627978\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  84%|████████▍ | 27/32 [8:04:14<1:21:28, 977.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7004, 'grad_norm': 7.575967311859131, 'learning_rate': 3.493816997957582e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-3522952969779627978\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-3522952969779627978\\checkpoint-192\\config.json\n",
      "Model weights saved in ./grid_results/-3522952969779627978\\checkpoint-192\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  84%|████████▍ | 27/32 [8:07:27<1:21:28, 977.75s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3522952969779627978\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-3522952969779627978\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7513850927352905, 'eval_accuracy': 0.7519582245430809, 'eval_f1_macro': 0.5849856718337759, 'eval_f1_weighted': 0.7626381153174033, 'eval_f1_0': 0.3953488372093023, 'eval_f1_1': 0.8501742160278746, 'eval_f1_2': 0.5094339622641509, 'eval_runtime': 14.3419, 'eval_samples_per_second': 26.705, 'eval_steps_per_second': 0.837, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3522952969779627978\\checkpoint-192\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-3522952969779627978\\checkpoint-144 (score: 0.5930294734921069).\n",
      "\n",
      "100%|██████████| 192/192 [14:32<00:00,  4.54s/it]:28, 977.75s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 872.4655, 'train_samples_per_second': 7.005, 'train_steps_per_second': 0.22, 'train_loss': 0.8043828805287679, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n",
      "Grid Search:  88%|████████▊ | 28/32 [8:07:52<1:03:23, 950.76s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  88%|████████▊ | 28/32 [8:11:26<1:03:23, 950.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/431816975757196909\\checkpoint-48\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0485551357269287, 'eval_accuracy': 0.6240208877284595, 'eval_f1_macro': 0.44348473224216306, 'eval_f1_weighted': 0.6567170249226868, 'eval_f1_0': 0.23943661971830985, 'eval_f1_1': 0.7622504537205081, 'eval_f1_2': 0.3287671232876712, 'eval_runtime': 14.0566, 'eval_samples_per_second': 27.247, 'eval_steps_per_second': 0.854, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  88%|████████▊ | 28/32 [8:11:43<1:03:23, 950.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0899, 'grad_norm': 4.583962440490723, 'learning_rate': 1.9994645874763657e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  88%|████████▊ | 28/32 [8:14:57<1:03:23, 950.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/431816975757196909\\checkpoint-96\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9386888146400452, 'eval_accuracy': 0.5039164490861618, 'eval_f1_macro': 0.4286657786657786, 'eval_f1_weighted': 0.5652621095179842, 'eval_f1_0': 0.30303030303030304, 'eval_f1_1': 0.6329670329670329, 'eval_f1_2': 0.35, 'eval_runtime': 13.9109, 'eval_samples_per_second': 27.532, 'eval_steps_per_second': 0.863, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  88%|████████▊ | 28/32 [8:15:23<1:03:23, 950.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9628, 'grad_norm': 8.320338249206543, 'learning_rate': 1.659345815100069e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  88%|████████▊ | 28/32 [8:18:31<1:03:23, 950.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/431816975757196909\\checkpoint-144\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8440015316009521, 'eval_accuracy': 0.6475195822454308, 'eval_f1_macro': 0.52219560045647, 'eval_f1_weighted': 0.6840742625735248, 'eval_f1_0': 0.38181818181818183, 'eval_f1_1': 0.7644787644787645, 'eval_f1_2': 0.42028985507246375, 'eval_runtime': 14.1295, 'eval_samples_per_second': 27.106, 'eval_steps_per_second': 0.849, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  88%|████████▊ | 28/32 [8:19:05<1:03:23, 950.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7683, 'grad_norm': 8.692729949951172, 'learning_rate': 9.019828596704394e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  88%|████████▊ | 28/32 [8:22:08<1:03:23, 950.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/431816975757196909\\checkpoint-192\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8802065253257751, 'eval_accuracy': 0.6892950391644909, 'eval_f1_macro': 0.5469326568265682, 'eval_f1_weighted': 0.7143969848641045, 'eval_f1_0': 0.375, 'eval_f1_1': 0.7970479704797048, 'eval_f1_2': 0.46875, 'eval_runtime': 14.2725, 'eval_samples_per_second': 26.835, 'eval_steps_per_second': 0.841, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  88%|████████▊ | 28/32 [8:22:49<1:03:23, 950.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5764, 'grad_norm': 12.43763256072998, 'learning_rate': 2.0664665970876496e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/431816975757196909\\checkpoint-240\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                 \n",
      "Grid Search:  88%|████████▊ | 28/32 [8:25:53<1:03:23, 950.76s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/431816975757196909\\checkpoint-240\n",
      "Configuration saved in ./grid_results/431816975757196909\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8831818103790283, 'eval_accuracy': 0.660574412532637, 'eval_f1_macro': 0.5229304029304029, 'eval_f1_weighted': 0.6930927036410065, 'eval_f1_0': 0.3516483516483517, 'eval_f1_1': 0.7771428571428571, 'eval_f1_2': 0.44, 'eval_runtime': 14.4341, 'eval_samples_per_second': 26.534, 'eval_steps_per_second': 0.831, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/431816975757196909\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/431816975757196909\\checkpoint-192 (score: 0.5469326568265682).\n",
      "\n",
      "100%|██████████| 240/240 [18:02<00:00,  4.51s/it]:23, 950.76s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1082.4903, 'train_samples_per_second': 7.058, 'train_steps_per_second': 0.222, 'train_loss': 0.7903510411580403, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n",
      "Grid Search:  91%|█████████ | 29/32 [8:26:18<49:51, 997.20s/it]  C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:  91%|█████████ | 29/32 [8:29:44<49:51, 997.20s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-48\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9433993697166443, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.3198131760078663, 'eval_f1_weighted': 0.6914612167563946, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8761061946902655, 'eval_f1_2': 0.08333333333333333, 'eval_runtime': 13.8636, 'eval_samples_per_second': 27.626, 'eval_steps_per_second': 0.866, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  91%|█████████ | 29/32 [8:30:02<49:51, 997.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.151, 'grad_norm': 4.683340549468994, 'learning_rate': 1.9994645874763657e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:  91%|█████████ | 29/32 [8:33:19<49:51, 997.20s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-96\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8887830972671509, 'eval_accuracy': 0.7650130548302873, 'eval_f1_macro': 0.40835766820358943, 'eval_f1_weighted': 0.7159650457359429, 'eval_f1_0': 0.046511627906976744, 'eval_f1_1': 0.8668730650154799, 'eval_f1_2': 0.3116883116883117, 'eval_runtime': 14.0913, 'eval_samples_per_second': 27.18, 'eval_steps_per_second': 0.852, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  91%|█████████ | 29/32 [8:33:46<49:51, 997.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9256, 'grad_norm': 5.699304580688477, 'learning_rate': 1.659345815100069e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:  91%|█████████ | 29/32 [8:36:58<49:51, 997.20s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-144\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8468502163887024, 'eval_accuracy': 0.7258485639686684, 'eval_f1_macro': 0.48707641587189454, 'eval_f1_weighted': 0.7249415968118729, 'eval_f1_0': 0.24, 'eval_f1_1': 0.8422818791946308, 'eval_f1_2': 0.37894736842105264, 'eval_runtime': 13.8102, 'eval_samples_per_second': 27.733, 'eval_steps_per_second': 0.869, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  91%|█████████ | 29/32 [8:37:30<49:51, 997.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7788, 'grad_norm': 6.368831634521484, 'learning_rate': 9.019828596704394e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:  91%|█████████ | 29/32 [8:40:29<49:51, 997.20s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-192\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8263468742370605, 'eval_accuracy': 0.7284595300261096, 'eval_f1_macro': 0.49143270124238564, 'eval_f1_weighted': 0.7292918905858128, 'eval_f1_0': 0.24, 'eval_f1_1': 0.8465430016863407, 'eval_f1_2': 0.3877551020408163, 'eval_runtime': 13.6172, 'eval_samples_per_second': 28.126, 'eval_steps_per_second': 0.881, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  91%|█████████ | 29/32 [8:41:10<49:51, 997.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6385, 'grad_norm': 10.271927833557129, 'learning_rate': 2.0664665970876496e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-240\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                               \n",
      "Grid Search:  91%|█████████ | 29/32 [8:44:10<49:51, 997.20s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/-3062482038904118138\\checkpoint-240\n",
      "Configuration saved in ./grid_results/-3062482038904118138\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8269426822662354, 'eval_accuracy': 0.6866840731070496, 'eval_f1_macro': 0.487550816404434, 'eval_f1_weighted': 0.7010336886509971, 'eval_f1_0': 0.23076923076923078, 'eval_f1_1': 0.8056537102473498, 'eval_f1_2': 0.4262295081967213, 'eval_runtime': 14.4051, 'eval_samples_per_second': 26.588, 'eval_steps_per_second': 0.833, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/-3062482038904118138\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/-3062482038904118138\\checkpoint-192 (score: 0.49143270124238564).\n",
      "\n",
      "100%|██████████| 240/240 [17:58<00:00,  4.49s/it]1, 997.20s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1078.4869, 'train_samples_per_second': 7.084, 'train_steps_per_second': 0.223, 'train_loss': 0.8254361947377523, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n",
      "Grid Search:  94%|█████████▍| 30/32 [8:44:32<34:12, 1026.26s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  94%|█████████▍| 30/32 [8:48:15<34:12, 1026.26s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-48\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0568408966064453, 'eval_accuracy': 0.6370757180156658, 'eval_f1_macro': 0.3593985468811827, 'eval_f1_weighted': 0.6363975381622896, 'eval_f1_0': 0.0, 'eval_f1_1': 0.7716262975778547, 'eval_f1_2': 0.30656934306569344, 'eval_runtime': 14.083, 'eval_samples_per_second': 27.196, 'eval_steps_per_second': 0.852, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  94%|█████████▍| 30/32 [8:48:31<34:12, 1026.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1524, 'grad_norm': 4.932790279388428, 'learning_rate': 1.9994645874763657e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  94%|█████████▍| 30/32 [8:51:48<34:12, 1026.26s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-96\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9492852687835693, 'eval_accuracy': 0.5796344647519582, 'eval_f1_macro': 0.464054810929811, 'eval_f1_weighted': 0.6291125821886266, 'eval_f1_0': 0.32167832167832167, 'eval_f1_1': 0.7111111111111111, 'eval_f1_2': 0.359375, 'eval_runtime': 13.8442, 'eval_samples_per_second': 27.665, 'eval_steps_per_second': 0.867, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  94%|█████████▍| 30/32 [8:52:12<34:12, 1026.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.988, 'grad_norm': 10.257997512817383, 'learning_rate': 1.659345815100069e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  94%|█████████▍| 30/32 [8:55:21<34:12, 1026.26s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-144\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8773235082626343, 'eval_accuracy': 0.5926892950391645, 'eval_f1_macro': 0.4710938121553723, 'eval_f1_weighted': 0.6384011426949252, 'eval_f1_0': 0.3064516129032258, 'eval_f1_1': 0.7211155378486056, 'eval_f1_2': 0.38571428571428573, 'eval_runtime': 13.8981, 'eval_samples_per_second': 27.558, 'eval_steps_per_second': 0.863, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  94%|█████████▍| 30/32 [8:55:54<34:12, 1026.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7933, 'grad_norm': 7.734147071838379, 'learning_rate': 9.019828596704394e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  94%|█████████▍| 30/32 [8:58:54<34:12, 1026.26s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-192\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8652822375297546, 'eval_accuracy': 0.6657963446475196, 'eval_f1_macro': 0.5169074894810026, 'eval_f1_weighted': 0.6974433151122349, 'eval_f1_0': 0.3300970873786408, 'eval_f1_1': 0.7865168539325843, 'eval_f1_2': 0.43410852713178294, 'eval_runtime': 13.9626, 'eval_samples_per_second': 27.431, 'eval_steps_per_second': 0.859, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  94%|█████████▍| 30/32 [8:59:35<34:12, 1026.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6333, 'grad_norm': 12.900416374206543, 'learning_rate': 2.0664665970876496e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-240\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  94%|█████████▍| 30/32 [9:02:37<34:12, 1026.26s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/7415174710347636781\\checkpoint-240\n",
      "Configuration saved in ./grid_results/7415174710347636781\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8663033843040466, 'eval_accuracy': 0.6318537859007833, 'eval_f1_macro': 0.5112679228594703, 'eval_f1_weighted': 0.6721267386107487, 'eval_f1_0': 0.36893203883495146, 'eval_f1_1': 0.7519685039370079, 'eval_f1_2': 0.4129032258064516, 'eval_runtime': 14.6135, 'eval_samples_per_second': 26.209, 'eval_steps_per_second': 0.821, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/7415174710347636781\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/7415174710347636781\\checkpoint-192 (score: 0.5169074894810026).\n",
      "\n",
      "100%|██████████| 240/240 [17:56<00:00,  4.48s/it]2, 1026.26s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1076.1115, 'train_samples_per_second': 7.1, 'train_steps_per_second': 0.223, 'train_loss': 0.8348946412404378, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:03:00<17:30, 1050.77s/it]C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\3025239944.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,445,187\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  97%|█████████▋| 31/32 [9:06:25<17:30, 1050.77s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-48\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-48\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9434850215911865, 'eval_accuracy': 0.7806788511749347, 'eval_f1_macro': 0.3198131760078663, 'eval_f1_weighted': 0.6914612167563946, 'eval_f1_0': 0.0, 'eval_f1_1': 0.8761061946902655, 'eval_f1_2': 0.08333333333333333, 'eval_runtime': 13.9533, 'eval_samples_per_second': 27.449, 'eval_steps_per_second': 0.86, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-48\\model.safetensors\n",
      "\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:06:42<17:30, 1050.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.151, 'grad_norm': 4.656449317932129, 'learning_rate': 1.9994645874763657e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  97%|█████████▋| 31/32 [9:09:58<17:30, 1050.77s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-96\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-96\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8873432278633118, 'eval_accuracy': 0.7597911227154047, 'eval_f1_macro': 0.4300679205851619, 'eval_f1_weighted': 0.7214901331674537, 'eval_f1_0': 0.125, 'eval_f1_1': 0.8652037617554859, 'eval_f1_2': 0.3, 'eval_runtime': 13.8543, 'eval_samples_per_second': 27.645, 'eval_steps_per_second': 0.866, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-96\\model.safetensors\n",
      "\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:10:23<17:30, 1050.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9223, 'grad_norm': 5.843655586242676, 'learning_rate': 1.659345815100069e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  97%|█████████▋| 31/32 [9:13:31<17:30, 1050.77s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-144\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-144\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8450310826301575, 'eval_accuracy': 0.7180156657963447, 'eval_f1_macro': 0.4809126974075428, 'eval_f1_weighted': 0.7199147675043345, 'eval_f1_0': 0.23376623376623376, 'eval_f1_1': 0.8378378378378378, 'eval_f1_2': 0.3711340206185567, 'eval_runtime': 14.1993, 'eval_samples_per_second': 26.973, 'eval_steps_per_second': 0.845, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-144\\model.safetensors\n",
      "\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:14:04<17:30, 1050.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7772, 'grad_norm': 6.442226886749268, 'learning_rate': 9.019828596704394e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  97%|█████████▋| 31/32 [9:17:05<17:30, 1050.77s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-192\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-192\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8232595324516296, 'eval_accuracy': 0.7232375979112271, 'eval_f1_macro': 0.48722413807159576, 'eval_f1_weighted': 0.7262547781023684, 'eval_f1_0': 0.23376623376623376, 'eval_f1_1': 0.8440677966101695, 'eval_f1_2': 0.3838383838383838, 'eval_runtime': 14.0098, 'eval_samples_per_second': 27.338, 'eval_steps_per_second': 0.857, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-192\\model.safetensors\n",
      "\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:17:46<17:30, 1050.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6344, 'grad_norm': 9.879302024841309, 'learning_rate': 2.0664665970876496e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-240\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-240\\config.json\n",
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-240\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                                \n",
      "Grid Search:  97%|█████████▋| 31/32 [9:20:46<17:30, 1050.77s/it]\n",
      "\u001b[ASaving model checkpoint to ./grid_results/6922637139503051099\\checkpoint-240\n",
      "Configuration saved in ./grid_results/6922637139503051099\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8256241679191589, 'eval_accuracy': 0.6840731070496083, 'eval_f1_macro': 0.4856910938067856, 'eval_f1_weighted': 0.6989817930321378, 'eval_f1_0': 0.23076923076923078, 'eval_f1_1': 0.8035398230088495, 'eval_f1_2': 0.42276422764227645, 'eval_runtime': 15.3919, 'eval_samples_per_second': 24.883, 'eval_steps_per_second': 0.78, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/6922637139503051099\\checkpoint-240\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/6922637139503051099\\checkpoint-192 (score: 0.48722413807159576).\n",
      "\n",
      "Grid Search:  97%|█████████▋| 31/32 [9:20:55<17:30, 1050.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1073.6182, 'train_samples_per_second': 7.116, 'train_steps_per_second': 0.224, 'train_loss': 0.8229005098342895, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [17:53<00:00,  4.47s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.19s/it]\n",
      "Grid Search: 100%|██████████| 32/32 [9:21:11<00:00, 1052.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to grid_search_results_robust.csv\n"
     ]
    }
   ],
   "source": [
    "# Custom Trainer with enhanced class weighting\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def get_class_weights(labels, floor=1.0):\n",
    "    \"\"\"Robust weight calculation with missing class handling\"\"\"\n",
    "    valid_classes = [0, 1, 2]\n",
    "    label_counts = {cls: max(np.sum(labels == cls), 1) for cls in valid_classes}\n",
    "    total_samples = sum(label_counts.values())\n",
    "    \n",
    "    weights = {\n",
    "        cls: max(total_samples / (len(valid_classes) * count), floor)\n",
    "        for cls, count in label_counts.items()\n",
    "    }\n",
    "    return torch.tensor([weights[cls] for cls in valid_classes], dtype=torch.float32)\n",
    "\n",
    "def train_evaluate_grid(params, tokenized_dataset):\n",
    "    # Get and validate labels\n",
    "    train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
    "    \n",
    "    # Class weight calculation with floor\n",
    "    class_weights = get_class_weights(train_labels, floor=params.get(\"class_weight_floor\", 1.0))\n",
    "\n",
    "    # Model initialization\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"allegro/herbert-base-cased\", \n",
    "        num_labels=3\n",
    "    )\n",
    "\n",
    "    # Training arguments with imbalance optimizations\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./grid_results/{hash(str(params))}\",\n",
    "        num_train_epochs=params[\"epochs\"],\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        warmup_ratio=0.2,  \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        seed=42,\n",
    "        optim=\"adamw_torch\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=params.get(\"grad_accum_steps\", 2),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        lr_scheduler_type=\"cosine\"  \n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train with validation checks\n",
    "    try:\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return {\n",
    "            **params,\n",
    "            \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"macro_f1\": eval_results[\"eval_f1_macro\"],\n",
    "            \"weighted_f1\": eval_results[\"eval_f1_weighted\"],\n",
    "            \"neutral_f1\": eval_results[\"eval_f1_0\"],\n",
    "            \"positive_f1\": eval_results[\"eval_f1_1\"],\n",
    "            \"negative_f1\": eval_results[\"eval_f1_2\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Parameter grid with class-aware parameters\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1.5e-5, 2e-5],     \n",
    "    \"batch_size\": [8, 16],               \n",
    "    \"epochs\": [4, 5],                    \n",
    "    \"weight_decay\": [0.01, 0.001],       \n",
    "    \"class_weight_floor\": [0.5, 1.0]     \n",
    "}\n",
    "\n",
    "# Generate parameter combinations\n",
    "all_params = [dict(zip(param_grid.keys(), values)) \n",
    "             for values in itertools.product(*param_grid.values())]\n",
    "\n",
    "# Run grid search with progress tracking\n",
    "results = []\n",
    "for params in tqdm(all_params, desc=\"Grid Search\"):\n",
    "    try:\n",
    "        result = train_evaluate_grid(params, tokenized_dataset)\n",
    "        if result:  # Only store successful runs\n",
    "            results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping failed params {params}: {str(e)}\")\n",
    "\n",
    "# Save and analyze results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"grid_search_results_robust.csv\", index=False)\n",
    "    print(\"Saved results to grid_search_results_robust.csv\")\n",
    "else:\n",
    "    print(\"No successful runs completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>class_weight_floor</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>neutral_f1</th>\n",
       "      <th>positive_f1</th>\n",
       "      <th>negative_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.754569</td>\n",
       "      <td>0.599176</td>\n",
       "      <td>0.764756</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.845614</td>\n",
       "      <td>0.566372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.759791</td>\n",
       "      <td>0.580015</td>\n",
       "      <td>0.764142</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.855196</td>\n",
       "      <td>0.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.759791</td>\n",
       "      <td>0.580015</td>\n",
       "      <td>0.764142</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.855196</td>\n",
       "      <td>0.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757180</td>\n",
       "      <td>0.586129</td>\n",
       "      <td>0.763172</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.850258</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749347</td>\n",
       "      <td>0.592107</td>\n",
       "      <td>0.759946</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.548673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  batch_size  epochs  weight_decay  class_weight_floor  accuracy  macro_f1  weighted_f1  neutral_f1  positive_f1  negative_f1\n",
       "23       0.000020           8       5         0.001                 1.0  0.754569  0.599176     0.764756    0.385542     0.845614     0.566372\n",
       "7        0.000015           8       5         0.001                 1.0  0.759791  0.580015     0.764142    0.400000     0.855196     0.484848\n",
       "5        0.000015           8       5         0.010                 1.0  0.759791  0.580015     0.764142    0.400000     0.855196     0.484848\n",
       "19       0.000020           8       4         0.001                 1.0  0.757180  0.586129     0.763172    0.389610     0.850258     0.518519\n",
       "21       0.000020           8       5         0.010                 1.0  0.749347  0.592107     0.759946    0.385542     0.842105     0.548673"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by='weighted_f1', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for best parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\1525835478.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
      "loading configuration file config.json from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\szymo\\.cache\\huggingface\\hub\\models--allegro--herbert-base-cased\\snapshots\\50e33e0567be0c0b313832314c586e3df0dc2297\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\Project\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,528\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 570\n",
      "  Number of trainable parameters = 124,445,187\n",
      "  9%|▉         | 50/570 [02:07<21:31,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.046, 'grad_norm': 5.428122520446777, 'learning_rate': 1.3157894736842104e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 95/570 [04:02<19:50,  2.51s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      " 17%|█▋        | 95/570 [04:20<19:50,  2.51s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-95\n",
      "Configuration saved in ./grid_results/best\\checkpoint-95\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9260566830635071, 'eval_accuracy': 0.7702349869451697, 'eval_f1_macro': 0.36978710512545093, 'eval_f1_weighted': 0.7034918947441768, 'eval_f1_0': 0.07692307692307693, 'eval_f1_1': 0.869172932330827, 'eval_f1_2': 0.16326530612244897, 'eval_runtime': 16.8791, 'eval_samples_per_second': 22.691, 'eval_steps_per_second': 0.711, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-95\\model.safetensors\n",
      " 18%|█▊        | 100/570 [04:47<38:28,  4.91s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9745, 'grad_norm': 6.805923938751221, 'learning_rate': 1.474146333670432e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 150/570 [06:55<17:47,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8574, 'grad_norm': 9.339993476867676, 'learning_rate': 1.3816164914906744e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 191/570 [08:42<16:16,  2.58s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      " 34%|███▎      | 191/570 [08:59<16:16,  2.58s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-191\n",
      "Configuration saved in ./grid_results/best\\checkpoint-191\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7768365144729614, 'eval_accuracy': 0.7441253263707572, 'eval_f1_macro': 0.5464107416569216, 'eval_f1_weighted': 0.7481834624134491, 'eval_f1_0': 0.3963963963963964, 'eval_f1_1': 0.8488964346349746, 'eval_f1_2': 0.3939393939393939, 'eval_runtime': 16.6847, 'eval_samples_per_second': 22.955, 'eval_steps_per_second': 0.719, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-191\\model.safetensors\n",
      " 35%|███▌      | 200/570 [09:33<19:04,  3.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7696, 'grad_norm': 9.347685813903809, 'learning_rate': 1.230329239809715e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 250/570 [11:29<12:05,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6649, 'grad_norm': 13.546929359436035, 'learning_rate': 1.0343583830944858e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 286/570 [12:51<10:40,  2.25s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      " 50%|█████     | 286/570 [13:06<10:40,  2.25s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-286\n",
      "Configuration saved in ./grid_results/best\\checkpoint-286\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7559505105018616, 'eval_accuracy': 0.7676240208877284, 'eval_f1_macro': 0.5795190696538944, 'eval_f1_weighted': 0.7694748610965884, 'eval_f1_0': 0.3466666666666667, 'eval_f1_1': 0.8624787775891342, 'eval_f1_2': 0.5294117647058824, 'eval_runtime': 14.3464, 'eval_samples_per_second': 26.697, 'eval_steps_per_second': 0.836, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-286\\model.safetensors\n",
      " 53%|█████▎    | 300/570 [13:47<10:33,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.606, 'grad_norm': 12.884482383728027, 'learning_rate': 8.119345091042494e-06, 'epoch': 3.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 350/570 [15:43<08:14,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4902, 'grad_norm': 26.018383026123047, 'learning_rate': 5.8374905136387105e-06, 'epoch': 3.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 382/570 [16:56<07:10,  2.29s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      " 67%|██████▋   | 382/570 [17:11<07:10,  2.29s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-382\n",
      "Configuration saved in ./grid_results/best\\checkpoint-382\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7514461278915405, 'eval_accuracy': 0.7676240208877284, 'eval_f1_macro': 0.5943131510141819, 'eval_f1_weighted': 0.7720478730680341, 'eval_f1_0': 0.3783783783783784, 'eval_f1_1': 0.8591065292096219, 'eval_f1_2': 0.5454545454545454, 'eval_runtime': 14.3154, 'eval_samples_per_second': 26.754, 'eval_steps_per_second': 0.838, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-382\\model.safetensors\n",
      " 70%|███████   | 400/570 [18:02<06:36,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4003, 'grad_norm': 10.887341499328613, 'learning_rate': 3.7102942645176784e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 450/570 [20:10<05:06,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.36, 'grad_norm': 13.556905746459961, 'learning_rate': 1.9356431037353893e-06, 'epoch': 4.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 477/570 [21:17<03:54,  2.53s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      " 84%|████████▎ | 477/570 [21:35<03:54,  2.53s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-477\n",
      "Configuration saved in ./grid_results/best\\checkpoint-477\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7602640986442566, 'eval_accuracy': 0.7702349869451697, 'eval_f1_macro': 0.5966335196039195, 'eval_f1_weighted': 0.7741584591603416, 'eval_f1_0': 0.3783783783783784, 'eval_f1_1': 0.8610634648370498, 'eval_f1_2': 0.5504587155963303, 'eval_runtime': 16.7294, 'eval_samples_per_second': 22.894, 'eval_steps_per_second': 0.717, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-477\\model.safetensors\n",
      " 88%|████████▊ | 500/570 [22:43<03:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3545, 'grad_norm': 15.006857872009277, 'learning_rate': 6.786275670531453e-07, 'epoch': 5.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 550/570 [24:53<00:54,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3137, 'grad_norm': 20.415361404418945, 'learning_rate': 5.618408287847099e-08, 'epoch': 5.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 570/570 [25:44<00:00,  2.61s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-570\n",
      "Configuration saved in ./grid_results/best\\checkpoint-570\\config.json\n",
      "Model weights saved in ./grid_results/best\\checkpoint-570\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n",
      "\n",
      "100%|██████████| 570/570 [26:12<00:00,  2.61s/it]Saving model checkpoint to ./grid_results/best\\checkpoint-570\n",
      "Configuration saved in ./grid_results/best\\checkpoint-570\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7764945030212402, 'eval_accuracy': 0.7728459530026109, 'eval_f1_macro': 0.5933790424546213, 'eval_f1_weighted': 0.7747674560551521, 'eval_f1_0': 0.36619718309859156, 'eval_f1_1': 0.863481228668942, 'eval_f1_2': 0.5504587155963303, 'eval_runtime': 16.5231, 'eval_samples_per_second': 23.18, 'eval_steps_per_second': 0.726, 'epoch': 5.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./grid_results/best\\checkpoint-570\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./grid_results/best\\checkpoint-477 (score: 0.5966335196039195).\n",
      "100%|██████████| 570/570 [26:44<00:00,  2.82s/it]\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 383\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1604.648, 'train_samples_per_second': 5.713, 'train_steps_per_second': 0.355, 'train_loss': 0.6096646225243284, 'epoch': 5.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Custom Trainer with enhanced class weighting\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def get_class_weights(labels, floor=1.0):\n",
    "    \"\"\"Robust weight calculation with missing class handling\"\"\"\n",
    "    valid_classes = [0, 1, 2]\n",
    "    label_counts = {cls: max(np.sum(labels == cls), 1) for cls in valid_classes}\n",
    "    total_samples = sum(label_counts.values())\n",
    "    \n",
    "    weights = {\n",
    "        cls: max(total_samples / (len(valid_classes) * count), floor)\n",
    "        for cls, count in label_counts.items()\n",
    "    }\n",
    "    return torch.tensor([weights[cls] for cls in valid_classes], dtype=torch.float32)\n",
    "\n",
    "def train_evaluate_grid(params, tokenized_dataset):\n",
    "    # Get and validate labels\n",
    "    train_labels = np.array(tokenized_dataset[\"train\"][\"labels\"])\n",
    "    \n",
    "    # Class weight calculation with floor\n",
    "    class_weights = get_class_weights(train_labels, floor=params.get(\"class_weight_floor\", 1.0))\n",
    "\n",
    "    # Model initialization\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"allegro/herbert-base-cased\", \n",
    "        num_labels=3\n",
    "    )\n",
    "\n",
    "    # Training arguments with imbalance optimizations\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./grid_results/best\",\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=0.000015,\n",
    "        weight_decay=0.001,\n",
    "        warmup_ratio=0.1,  # Increased warmup for stability\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        seed=42,\n",
    "        optim=\"adamw_torch\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=params.get(\"grad_accum_steps\", 2),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        lr_scheduler_type=\"cosine\"  # Better for fine-tuning\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train with validation checks\n",
    "    try:\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return {\n",
    "            **params,\n",
    "            \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"macro_f1\": eval_results[\"eval_f1_macro\"],\n",
    "            \"weighted_f1\": eval_results[\"eval_f1_weighted\"],\n",
    "            \"neutral_f1\": eval_results[\"eval_f1_0\"],\n",
    "            \"positive_f1\": eval_results[\"eval_f1_1\"],\n",
    "            \"negative_f1\": eval_results[\"eval_f1_2\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "result = train_evaluate_grid({}, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7702349869451697,\n",
       " 'macro_f1': 0.5966335196039195,\n",
       " 'weighted_f1': 0.7741584591603416,\n",
       " 'neutral_f1': 0.3783783783783784,\n",
       " 'positive_f1': 0.8610634648370498,\n",
       " 'negative_f1': 0.5504587155963303}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./grid_results/best/checkpoint-570\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file ./grid_results/best/checkpoint-570\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./grid_results/best/checkpoint-570.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 383\n",
      "  Batch size = 8\n",
      "100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.42      0.33      0.37        40\n",
      "     Neutral       0.88      0.85      0.86       298\n",
      "    Positive       0.47      0.67      0.55        45\n",
      "\n",
      "    accuracy                           0.77       383\n",
      "   macro avg       0.59      0.61      0.59       383\n",
      "weighted avg       0.78      0.77      0.77       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"./grid_results/best/checkpoint-570\", num_labels=3)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Create and display confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\2806912349.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(overall_metrics.keys()),\n",
      "C:\\Users\\szymo\\AppData\\Local\\Temp\\ipykernel_424\\2806912349.py:29: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(class_f1.keys()),\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_results(results):\n",
    "  \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "   \n",
    "    overall_metrics = {\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Macro F1': results['macro_f1'],\n",
    "        'Weighted F1': results['weighted_f1']\n",
    "    }\n",
    "    \n",
    "    sns.barplot(x=list(overall_metrics.keys()), \n",
    "                y=list(overall_metrics.values()),\n",
    "                ax=ax1,\n",
    "                palette='viridis')\n",
    "    ax1.set_title('Overall Model Performance')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    class_f1 = {\n",
    "        'Neutral': results['neutral_f1'],\n",
    "        'Positive': results['positive_f1'],\n",
    "        'Negative': results['negative_f1']\n",
    "    }\n",
    "    \n",
    "    sns.barplot(x=list(class_f1.keys()), \n",
    "                y=list(class_f1.values()),\n",
    "                ax=ax2,\n",
    "                palette='muted')\n",
    "    ax2.set_title('Class-wise F1 Scores')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_results(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
